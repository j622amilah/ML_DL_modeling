{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e81e814",
   "metadata": {},
   "source": [
    "# Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34044bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manipulation\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import svm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cluster\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.compat.v1.enable_eager_execution()  # This allows you to use placeholder in version 2.0 or higher\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# LSTM\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Transformer\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "\n",
    "# N-Layer NN\n",
    "from tensorflow.keras.layers import LeakyReLU, Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "# CNN\n",
    "from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521e3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e65ebe",
   "metadata": {},
   "source": [
    "# Subfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, img_dim):\n",
    "    if type(img) == 'numpy.ndarray':\n",
    "        # img is an array, retuns an image object\n",
    "        rgb_image = Image.fromarray(img , 'RGB')\n",
    "    else:\n",
    "        # img is an image object, returns an image object\n",
    "        try:\n",
    "            rgb_image = img.convert('RGB')\n",
    "        except AttributeError:\n",
    "            rgb_image = Image.fromarray(img , 'RGB')\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(img_dim), int(img_dim)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "    return img3\n",
    "\n",
    "def convert_img_a_mat(img, outpt):\n",
    "    mat = np.array(img)  # Convert image to an array\n",
    "    if outpt == 'mat2D':\n",
    "        # Transformer l'image de 3D à 2D\n",
    "        # Convert image back to a 2D array\n",
    "        matout = np.mean(mat, axis=2)\n",
    "    elif outpt == 'img3D': # techniquement c'est un image parce qu'il y a trois RGB channels \n",
    "        matout = mat\n",
    "    return matout\n",
    "\n",
    "def norm_mat(mat2Dor3D, norm):\n",
    "    if norm == 'zero2one':\n",
    "        # Normalizer l'image entre 0 et 1\n",
    "        norout = mat2Dor3D/255\n",
    "    elif norm == 'negone2posone':\n",
    "        # Normalize the images to [-1, 1]\n",
    "        norout = (mat2Dor3D - 127.5) / 127.5\n",
    "    elif norm == 'non':\n",
    "        norout = mat2Dor3D\n",
    "    return norout\n",
    "\n",
    "def threshold_mat(mat2D, thresh):\n",
    "    # Threshold image\n",
    "    val = 255/2\n",
    "    if thresh == 'zero_moins_que_val':\n",
    "        row, col = mat2D.shape\n",
    "        mat_thresh = mat2D\n",
    "        min_val = np.min(mat_thresh)\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if mat_thresh[i,j] < val:\n",
    "                    mat_thresh[i,j] = min_val\n",
    "    elif thresh == 'non':\n",
    "        mat_thresh = mat2D\n",
    "    return mat_thresh\n",
    "\n",
    "def imgORmat_resize_imgORmat_CNN(img_dim, data_in, inpt='img3D', outpt='mat2D', norm='non', thresh='non'):\n",
    "    if inpt == 'img3D' and outpt=='mat2D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='mat2D':\n",
    "        data_in = np.array(data_in)\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        mat2D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(mat2D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='img3D':\n",
    "        data_in = np.array(data_in)\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'img3D' and outpt=='img3D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f06736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_tensorflow(model, X_test, Y_test):\n",
    "\n",
    "    Y_test_1D = [Y_test[i,0:1] for i in range(Y_test.shape[0])]\n",
    "\n",
    "    # First, a baseline metric, defined by scoring,\n",
    "    # Obtenez mean absolute error\n",
    "    y_hat_test = model.predict(X_test, verbose=0)\n",
    "    baseline_mae = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "\n",
    "    vals = {}\n",
    "    # Shuffle each feature columns at a time\n",
    "    for featcol in range(X_test.shape[2]):\n",
    "\n",
    "        # Define a modifiable temporary variable\n",
    "        temp = X_test\n",
    "\n",
    "        # select a column\n",
    "        feat_slice = temp[:,:,featcol]\n",
    "\n",
    "        # Must flatten the matrix because np.random.permutation or \n",
    "        # np.random.shuffle don't work\n",
    "        t = feat_slice.flatten()\n",
    "        t_shuf = np.random.permutation(t)\n",
    "        feat_slice =  np.reshape(t_shuf, (feat_slice.shape))\n",
    "\n",
    "        # put feat_slice back into temp\n",
    "        temp[:,:,featcol] = feat_slice\n",
    "\n",
    "        y_hat_test = model.predict(temp, verbose=0)\n",
    "        mae_per_col = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "        vals[featcol] = mae_per_col\n",
    "\n",
    "    # Sort the columns from largest to smallest mae\n",
    "    laquelle = sort_dict_by_value(vals, reverse = True)\n",
    "    \n",
    "    # Determinez le nombres des columns qui sont plus grande que le baseline_mae\n",
    "    # C'est des marqueurs qui sont importants\n",
    "    feat = list(laquelle.keys())\n",
    "    cnt = [1 for i in range(len(feat)) if feat[i] > baseline_mae]\n",
    "    cnt = np.sum(cnt)\n",
    "    \n",
    "    allout = list(laquelle.items())\n",
    "    nout = [allout[i] for i in range(cnt)]\n",
    "    marquers_important = dict(nout)\n",
    "    \n",
    "    return marquers_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df_test2, ynum):\n",
    "    \n",
    "    # Loop over each model to test\n",
    "    for wm in [3, 1, 4, 0, 2, 5]:\n",
    "        res_permod = []\n",
    "        \n",
    "        for fea in range(4):\n",
    "            print('fea : ', fea)\n",
    "            \n",
    "            # ----------------\n",
    "            # Order of which features to use in a model\n",
    "            if fea == 0:\n",
    "                # 1) All features\n",
    "                X_cols = list(np.arange(1, df_test2.shape[1]-1, 1))\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 1:\n",
    "                # 2) first 3 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(3)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 2:\n",
    "                # 3) first 2 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(2)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 3:\n",
    "                # 4) first feature from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[0][0]]\n",
    "                y_cols = [df_test2.shape[1]-1] \n",
    "            \n",
    "            \n",
    "            # ORIGINAL MODELS:\n",
    "            if wm == 0:\n",
    "                # Sequential : Support Vector Machine\n",
    "                m_name = 'SVC'\n",
    "                batch_size = 24\n",
    "                X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "                model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = svm_batch(X_train, X_test, Y_train, Y_test, info, batch_size)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    # Stack all data over batches\n",
    "                    X_test_batch = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "                    Y_test_1D_batch = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "\n",
    "                    r = permutation_importance(model, X_test_batch, Y_test_1D_batch, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "                    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "                    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 3:\n",
    "                # Spatial (find global trends in the feature) : RandomForest - partitioned subspace\n",
    "                m_name = 'RF'\n",
    "                X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "                \n",
    "                # Stack all data over batch\n",
    "                X_train = np.reshape(X_train, (info['batch_train']*info['timesteps_train'], info['feature_train']))\n",
    "                Y_train_1D = np.reshape(Y_train, (info['batch_train']*info['timesteps_train'], info['n_outputs']))\n",
    "                X_test = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "                Y_test_1D = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "                \n",
    "                model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score, dfs = RandomForest(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "\n",
    "                value_pack_train = evaluation_methods(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_score, dfs)\n",
    "                value_pack_test = evaluation_methods(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_score, dfs)\n",
    "\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    r = permutation_importance(model, X_test, Y_test_1D, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "                    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "                    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "                # ----------------\n",
    "        \n",
    "        elif wm == 5:\n",
    "                # Multilayer perceptron (MLP)/neural network (Deep Learning) : logistic regression NN\n",
    "                m_name = 'MLP'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_multilayer_perceptron(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_multilayer_perceptron_1Dinput(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "        \n",
    "        elif wm == 6:\n",
    "                # Gaussian Naive Bayes\n",
    "                m_name = 'GNB'\n",
    "                if cltype == 'binary':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_gaussian_naive_bayes(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif cltype == 'multi':\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_gaussian_naive_bayes(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "            \n",
    "            # UPDATED MODELS in HAR :\n",
    "            elif wm == 1:\n",
    "                # Sequential : LSTM - changes within a window of points\n",
    "                m_name = 'LSTM'\n",
    "                model, dict_out, X_test, Y_test = run_LSTM(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = [dict_out['delay_train'], dict_out['delay_test']]    \n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 2:\n",
    "                # Sequential : Transformer - changes between windows of points\n",
    "                m_name = 'Trans'\n",
    "                model, dict_out, X_test, Y_test = run_Transformer(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 4:\n",
    "                # Spatial (find global trends in the feature) : CNN\n",
    "                m_name = 'CNN'\n",
    "                model, dict_out, X_test, Y_test = run_CNN(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 5:\n",
    "                # Sequential & Spatial : LSTM-CNN\n",
    "                m_name = 'LSTM-CNN'\n",
    "                model, dict_out, X_test, Y_test = run_LSTM_CNN(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "\n",
    "            # ----------------\n",
    "            # Save all data to array \n",
    "            res_permod.append([ynum, m_name, fea, X_cols, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test, marquers_important, extra])\n",
    "            # ----------------\n",
    "            \n",
    "        # Save data matrices to file per model result :\n",
    "        file_name = \"res_exp_%s_%s_%s_ynum%d_%s.pkl\" % (exp, ax_val, ss_val, ynum, m_name)\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(res_permod, open_file)\n",
    "        open_file.close()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6779b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mettez_dictout_dans_vars(dict_out, q):\n",
    "    if q == 0:\n",
    "        acc = dict_out['acc_train']\n",
    "        prec = dict_out['prec_train']\n",
    "        recall = dict_out['recall_train']\n",
    "        roc_auc = dict_out['roc_auc_train']\n",
    "    elif q == 1:\n",
    "        acc = dict_out['acc_test']\n",
    "        prec = dict_out['prec_test']\n",
    "        recall = dict_out['recall_test']\n",
    "        roc_auc = dict_out['roc_auc_test']\n",
    "    elif q == 2:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_dircalc']\n",
    "    elif q == 3:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_dircalc']\n",
    "            \n",
    "    return acc, prec, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols):\n",
    "\n",
    "    # Ensure that the X matrix size is correct\n",
    "    # df_test2 : (dp_per_sample*n_values, feature)\n",
    "    all_dp, cols = df_test2.shape\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Il faut change df_test2 à : (batch, timesteps, feature)\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    tot = [endd[i]-st[i] for i in range(len(st))]\n",
    "    val = min(tot)\n",
    "\n",
    "    # Ensurez que X est le meme taille\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(tot)):\n",
    "        isamp = endd[i]-st[i]\n",
    "        diff = isamp - val\n",
    "        X.append(df_test2.iloc[st[i]:endd[i]-diff, X_cols].to_numpy())\n",
    "        Y.append(df_test2.iloc[st[i]:endd[i]-diff, y_cols].to_numpy())\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Tensorflow says to use :\n",
    "    batch, timesteps, feature = X.shape\n",
    "\n",
    "    print('batch:' , batch)\n",
    "    print('timesteps:' , timesteps)\n",
    "    print('feature:' , feature)\n",
    "    \n",
    "    print('taille de X:' , X.shape)\n",
    "    # X.shape =  (104570, 20, 1)   # batch, timesteps/sequence length, feature\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    batch, timesteps, n_outputs = Y.shape   # batch, timesteps, 1\n",
    "    print('taille de Y:' , Y.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Split the X an y data into test and train\n",
    "    seed = 0\n",
    "    test_size = 0.25 # default\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = seed, test_size = test_size)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    \n",
    "    print('X_train:' , X_train.shape)\n",
    "    print('Y_train:' , Y_train.shape)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    print('X_test:' , X_test.shape)\n",
    "    print('Y_test:' , Y_test.shape)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Ensure that Y_train and Y_test are integers\n",
    "    Y_test = Y_test.astype(int)\n",
    "    Y_train = Y_train.astype(int)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # batch_train, timesteps_train, feature_train = X_train.shape\n",
    "    # batch_test, timesteps_test, feature_test = X_test.shape\n",
    "    \n",
    "    # OU\n",
    "    \n",
    "    suf = ['train', 'test']\n",
    "    noms = ['batch_', 'timesteps_', 'feature_']\n",
    "    dictkeys = [j+i for i in suf for j in noms]\n",
    "    #print('dictkeys: ', dictkeys)\n",
    "\n",
    "    dictvals = []\n",
    "    dictvals.append(list(X_train.shape))\n",
    "    dictvals.append(list(X_test.shape))\n",
    "    dictvals = np.ravel(dictvals)\n",
    "    \n",
    "    info = dict(zip(dictkeys, dictvals))\n",
    "    info['n_outputs'] = n_outputs\n",
    "    # ----------------\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30eaf05",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_batch(X_train, X_test, Y_train, Y_test, info, batch_size):\n",
    "    \n",
    "    # ----------------------------\n",
    "    \n",
    "    # batch_size = 24\n",
    "    which_mod = 'SVC'  # 'NuSVC', 'SVC'\n",
    "    para = 'adaptive' # 'hyperparmetre', 'adaptive'\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    class_len = len(np.unique(Y_train))\n",
    "    if class_len <= 2:\n",
    "        dfs = 'binary'\n",
    "        dfs_var = 'ovo'\n",
    "    elif class_len > 2:\n",
    "        dfs = 'multi'\n",
    "        dfs_var = 'ovr'\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    testall = 'batch'\n",
    "    if testall == 'batch':\n",
    "        # Test on same batch for all X_train\n",
    "        st = np.random.permutation(int(np.ceil(info['batch_test']/batch_size)))[0]*batch_size\n",
    "        endd = st+batch_size\n",
    "\n",
    "        if endd < info['batch_test']:\n",
    "            X_test_batch = np.reshape(X_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['feature_test']))\n",
    "            Y_test_1D_batch = np.reshape(Y_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['n_outputs']))\n",
    "        else:\n",
    "            batch_mod = info['batch_test']-st\n",
    "            endd = st+batch_mod\n",
    "            X_test_batch = np.reshape(X_test[st:endd,:,:], (batch_mod*info['timesteps_test'], info['feature_test']))\n",
    "            Y_test_1D_batch = np.reshape(Y_test[st:endd,:,:], (batch_mod*info['timesteps_test'], info['n_outputs']))\n",
    "    elif testall == 'toutes_donnes':\n",
    "        # Toutes des donnes\n",
    "        X_test_batch = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "        Y_test_1D_batch = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "\n",
    "    n = int(np.ceil(info['batch_train']/batch_size))\n",
    "    print('n: ', n)\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    C_l = np.linspace(0.9, 2, n)\n",
    "    gamma_l = np.linspace(1/info['feature_train'], 1/2, n)\n",
    "\n",
    "    C = 1  # Defaut\n",
    "    gamma = 1/info['feature_train']\n",
    "    inc_C = (0.9)*(1/10)\n",
    "    inc_gamma = (1/info['feature_train'])*(1/10)\n",
    "    # ----------------------------\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(n):\n",
    "        st = i*batch_size\n",
    "        endd = st+batch_size\n",
    "\n",
    "        # Stack all data over batches\n",
    "        if endd < info['batch_train']:\n",
    "            X_train_batch = np.reshape(X_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['feature_train']))\n",
    "            Y_train_1D_batch = np.reshape(Y_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['n_outputs']))\n",
    "        else:\n",
    "            batch_mod = info['batch_train']-st\n",
    "            endd = st+batch_mod\n",
    "            X_train_batch = np.reshape(X_train[st:endd,:,:], (batch_mod*info['timesteps_train'], info['feature_train']))\n",
    "            Y_train_1D_batch = np.reshape(Y_train[st:endd,:,:], (batch_mod*info['timesteps_train'], info['n_outputs']))\n",
    "\n",
    "        # print('shape of X_train_batch : ', X_train_batch.shape)\n",
    "        # print('shape of Y_train_1D_batch : ', Y_train_1D_batch.shape)\n",
    "\n",
    "        # “one-versus-one” : binary ONLY, Y_train_1D, same implementation as libsvm \n",
    "        # (uses 1/lambda instead of C in cost function)\n",
    "        if para == 'hyperparmetre':\n",
    "            if which_mod == 'NuSVC':\n",
    "                model = svm.NuSVC(decision_function_shape=dfs_var, gamma=gamma_l[i], probability=True, max_iter=-1)\n",
    "            elif which_mod == 'SVC':\n",
    "                model = svm.SVC(decision_function_shape=dfs_var, C=C_l[i], probability=True, max_iter=-1)\n",
    "        elif para == 'adaptive':\n",
    "            if which_mod == 'NuSVC':\n",
    "                model = svm.NuSVC(decision_function_shape=dfs_var, gamma=gamma, probability=True, max_iter=-1)\n",
    "            elif which_mod == 'SVC':\n",
    "                model = svm.SVC(decision_function_shape=dfs_var, C=C, probability=True, max_iter=-1)\n",
    "\n",
    "        model.fit(X_train_batch, Y_train_1D_batch)\n",
    "        # ----------------------------\n",
    "\n",
    "        Y_train_1D_predict = model.predict(X_train_batch)\n",
    "        Y_test_1D_predict = model.predict(X_test_batch)\n",
    "\n",
    "        # The prediction probability of each class : is size [n_samples, n_classes]\n",
    "        Y_train_bin_pp = model.predict_proba(X_train_batch) \n",
    "        Y_test_bin_pp = model.predict_proba(X_test_batch)\n",
    "\n",
    "        Y_train_bin_pp = np.array(Y_train_bin_pp)\n",
    "        # print('shape of Y_train_bin_pp : ', Y_train_bin_pp.shape)\n",
    "        Y_test_bin_pp = np.array(Y_test_bin_pp)\n",
    "        # print('shape of Y_test_bin_pp : ', Y_test_bin_pp.shape)\n",
    "\n",
    "        # How confidently each value predicted for x_test by the classifier is Positive ( large-magnitude Positive value ) or Negative ( large-magnitude Negative value)\n",
    "        Y_train_score = model.decision_function(X_train_batch)  # size is [n_samples, 1]\n",
    "        Y_test_score = model.decision_function(X_test_batch)\n",
    "\n",
    "        Y_train_score = np.array(Y_train_score)\n",
    "        Y_test_score = np.array(Y_test_score)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        value_pack_train = evaluation_methods(model, X_train_batch, Y_train_1D_batch, \n",
    "                                              Y_train_1D_predict, Y_train_bin_pp, Y_train_score, dfs)\n",
    "        value_pack_test = evaluation_methods(model, X_test_batch, Y_test_1D_batch, \n",
    "                                              Y_test_1D_predict, Y_test_bin_pp, Y_test_score, dfs)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        if i > 0:\n",
    "            acc_test_prev = acc_test\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "        acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3) \n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        if which_mod == 'SVC':\n",
    "            if i > 0:\n",
    "                if acc_test_prev > acc_test:\n",
    "                    C = C - inc_C\n",
    "                else:\n",
    "                    C = C + inc_C\n",
    "            results.append([i, C, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test])\n",
    "\n",
    "        elif which_mod == 'NuSVC':\n",
    "            if i > 0:\n",
    "                if acc_test_prev > acc_test:\n",
    "                    gamma = gamma + inc_gamma\n",
    "                else:\n",
    "                    gamma = gamma - inc_gamma\n",
    "            results.append([i, gamma, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test])\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # Save model to file\n",
    "        file_name = \"model_%d.pkl\" % (i)\n",
    "        save_dat_pickle(model, file_name=file_name)\n",
    "\n",
    "        # Delete model\n",
    "        del model\n",
    "        del Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score\n",
    "\n",
    "\n",
    "    # Evaluatez quel modeles est mieux : AUCROC_test > 0.5 et max accuracy\n",
    "    results = np.array(results)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df2 = df[(df.iloc[:,9] > 0.5)]\n",
    "    best = df2.iloc[:,6].idxmax()\n",
    "    print('best : ', best)\n",
    "    print('C ou gamma : ', df.iloc[best,1])\n",
    "\n",
    "    acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = map(float, results[best,2::])\n",
    "\n",
    "    # Load best model only for permutation importance\n",
    "    file_name = \"model_%d.pkl\" % (best)\n",
    "    model = load_dat_pickle(file_name=file_name)\n",
    "\n",
    "    # Delete all .pkl files\n",
    "    # del results\n",
    "    rm_list = [j for j in range(n) if j != best]\n",
    "    for i in rm_list:\n",
    "        os.remove(\"model_%d.pkl\" % (i))\n",
    "    \n",
    "    return model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_Y_bin_pp_2_Y_1D_pp(Y_1D, Y_bin_pp):\n",
    "\n",
    "    # Y_bin_pp is size [n_samples, n_classes=2]\n",
    "    # Take the column of Y_bin_pp for the class of Y_1D, because both vectors need to be [n_samples, 1]\n",
    "    Y_1D_pp = []\n",
    "    for q in range(len(Y_1D)):\n",
    "        desrow = Y_bin_pp[q]\n",
    "        Y_1D_pp.append(desrow[int(Y_1D[q])])\n",
    "    Y_1D_pp = np.ravel(Y_1D_pp)\n",
    "    \n",
    "    Y_1D_pp = np.array(Y_1D_pp)\n",
    "    \n",
    "    return Y_1D_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_methods(model, X, Y_1D, Y_1D_predict, Y_bin_pp, Y_score, dfs):\n",
    "    \n",
    "    average = 'micro'\n",
    "    acc_dircalc = metrics.accuracy_score(Y_1D, Y_1D_predict)\n",
    "    prec_dircalc = metrics.precision_score(Y_1D, Y_1D_predict, average=average)\n",
    "    recall_dircalc = metrics.recall_score(Y_1D, Y_1D_predict, average=average)\n",
    "    f1_dircalc = metrics.f1_score(Y_1D, Y_1D_predict, average=average)\n",
    "    \n",
    "    if dfs == 'binary':\n",
    "        # ----------------------------\n",
    "        Y_1D_pp = transform_Y_bin_pp_2_Y_1D_pp(Y_1D, Y_bin_pp)\n",
    "        Y_1D = np.array(Y_1D)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # prediction probability\n",
    "        rocauc_dircalc = metrics.roc_auc_score(Y_1D, Y_1D_pp, average=average)\n",
    "        \n",
    "    elif dfs == 'multi':\n",
    "    \n",
    "        # ----------------------------\n",
    "        # Need to binarize Y into size [n_samples, n_classes]\n",
    "        Y_bin, unique_classes = binarize_Y1Dvec_2_Ybin(Y_1D)\n",
    "        Y_bin = np.array(Y_bin)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # decision function\n",
    "        rocauc_dircalc = metrics.roc_auc_score(Y_bin, Y_score, average=average)\n",
    "\n",
    "    \n",
    "    value_pack = {}\n",
    "    var_list = ['acc_dircalc', 'prec_dircalc', 'recall_dircalc', 'f1_dircalc', 'rocauc_dircalc']\n",
    "    var_list_num = [acc_dircalc, prec_dircalc, recall_dircalc, f1_dircalc, rocauc_dircalc]\n",
    "    \n",
    "    for q in range(len(var_list)):\n",
    "        value_pack['%s' % (var_list[q])] = var_list_num[q]\n",
    "    \n",
    "    return value_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4c079",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, X_test, Y_train_1D, Y_test_1D):\n",
    "    \n",
    "    # Determine if classes are binary or multiclass:\n",
    "    class_len = len(np.unique(Y_train_1D))\n",
    "    if class_len <= 2:\n",
    "        dfs = 'binary'\n",
    "    elif class_len > 2:\n",
    "        dfs = 'multi'\n",
    "    \n",
    "    forest = RandomForestClassifier(random_state=1, min_samples_leaf=50)  # min_samples_leaf is 100 by default\n",
    "    model = MultiOutputClassifier(forest, n_jobs=-1) #n_jobs=-1 means apply parallel processing\n",
    "    \n",
    "    Y_train_1D = np.reshape(Y_train_1D, (len(Y_train_1D), 1))  # Y needs to have a defined shape ***\n",
    "    model.fit(X_train, Y_train_1D)\n",
    "\n",
    "    # ------------------------------\n",
    "    \n",
    "    Y_train_1D_predict = model.predict(X_train)\n",
    "    Y_test_1D_predict = model.predict(X_test)\n",
    "\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Binary : the prediction probability of each class : is size [n_samples, n_classes]\n",
    "    # Multi-class : the prediction probability of each class : size is [1, n_samples, n_classes]\n",
    "    Y_train_bin_pp = model.predict_proba(X_train) \n",
    "    Y_test_bin_pp = model.predict_proba(X_test)\n",
    "    \n",
    "    if dfs == 'binary':\n",
    "        Y_train_bin_pp = np.reshape(Y_train_bin_pp, (len(Y_train_1D_predict), 2))\n",
    "        Y_test_bin_pp = np.reshape(Y_test_bin_pp, (len(Y_test_1D_predict), 2))\n",
    "    elif dfs == 'multi':\n",
    "        unique_classes = np.unique(Y_train_1D)\n",
    "        Y_train_bin_pp = np.reshape(Y_train_bin_pp, (len(Y_train_1D), len(unique_classes)))\n",
    "        Y_test_bin_pp = np.reshape(Y_test_bin_pp, (len(Y_test_1D), len(unique_classes)))\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # There is NO decision_function\n",
    "    # ------------------------------\n",
    "    if dfs == 'binary':\n",
    "        # size is [n_samples, 1]\n",
    "        Y_train_score = transform_Y_bin_pp_2_Y_1D_pp(Y_train_1D, Y_train_bin_pp)\n",
    "        Y_test_score = transform_Y_bin_pp_2_Y_1D_pp(Y_test_1D, Y_test_bin_pp)\n",
    "        # OR\n",
    "        # How confidently each value predicted for x_test by the classifier is Positive ( large-magnitude Positive value ) or Negative ( large-magnitude Negative value)\n",
    "        #Y_train_1D_score = model.decision_function(X_train)  # size is [n_samples, 1]\n",
    "        #Y_test_1D_score = model.decision_function(X_test)\n",
    "    elif dfs == 'multi':\n",
    "        # size is [n_samples, n_classes]\n",
    "        Y_train_score = Y_train_bin_pp\n",
    "        Y_test_score = Y_test_bin_pp\n",
    "        \n",
    "    Y_train_score = np.array(Y_train_score)\n",
    "    Y_test_score = np.array(Y_test_score)\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    return model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score, dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c46093",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef61e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_CNN(df_test2, X_cols, y_cols, img_dim): \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    # X_train: (batch_train, timesteps_train, feature_train)\n",
    "    # Y_train: (batch_train, timesteps_train, n_outputs)\n",
    "    # X_test: (batch_test, timesteps_test, feature_train)\n",
    "    # Y_test: (batch_test, timesteps_test, n_outputs)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "    X_train_img = Xbtf_2_Xbii3(X_train, img_dim, info['batch_train'])\n",
    "    X_test_img = Xbtf_2_Xbii3(X_test, img_dim, info['batch_test'])\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Y_train_1D =  [Y_train[i,0:1,0] for i in range(info['batch_train'])]\n",
    "    Y_test_1D =  [Y_test[i,0:1,0] for i in range(info['batch_test'])]\n",
    "    Y_train_1D = np.array(Y_train_1D)\n",
    "    Y_test_1D = np.array(Y_test_1D)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # shape of X_train_img :  (batch_train, img_dim, img_dim, 3)\n",
    "    # shape of Y_train_1D :  (batch_train,1)\n",
    "    # shape of X_test_img :  (batch_test, img_dim, img_dim, 3)\n",
    "    # shape of Y_test_1D :  (batch_test,)\n",
    "    \n",
    "    X_train_img = np.asarray(X_train_img, dtype = np.float16, order ='C')  # np.float16, np.float32, np.float64\n",
    "    Y_train_1D = np.asarray(Y_train_1D, dtype = np.float16, order ='C')\n",
    "    X_test_img = np.asarray(X_test_img, dtype = np.float16, order ='C')\n",
    "    Y_test_1D = np.asarray(Y_test_1D, dtype = np.float16, order ='C')\n",
    "    \n",
    "    print('X_train_img:' , X_train_img.shape)\n",
    "    print('Y_train_1D:' , Y_train_1D.shape)\n",
    "    print('X_test_img:' , X_test_img.shape)\n",
    "    print('Y_test_1D:' , Y_test_1D.shape)\n",
    "    \n",
    "    return X_train_img, X_test_img, Y_train_1D, Y_test_1D, info\n",
    "    \n",
    "    \n",
    "# Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "def Xbtf_2_Xbii3(X, img_dim, batch):\n",
    "    X_img = []\n",
    "    for i in range(batch):\n",
    "        X_1D = X[i,:,:].flatten()\n",
    "        \n",
    "        if i == 0:\n",
    "            n = int(np.floor(np.sqrt(len(X_1D))))\n",
    "    \n",
    "        # fold into a square\n",
    "        mat = np.reshape(X_1D[0:n*n], (n, n))\n",
    "    \n",
    "        image = imgORmat_resize_imgORmat_CNN(img_dim, mat, inpt='mat2D', outpt='img3D', norm='non', thresh='non')\n",
    "        \n",
    "        X_img.append(image)\n",
    "    \n",
    "    X_img = np.array(X_img)\n",
    "    \n",
    "    return X_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb06710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Though for the 1D signal classification CNNs are also suitable, like they are implemented in the article \n",
    "# [49] for the seismic signal classification, while dealing with 2D objects CNNs can perform significantly \n",
    "# better results. Thus, firstly, we convert the 1D accelerometer signal into the 2D images via applying \n",
    "# CWT in order to extract signal features and, at the same time, to make it possible to implement 2D CNNs.'\n",
    "\n",
    "def MPCNN_arch(n_outputs, img_dim, rgb_layers, ynum):\n",
    "    \n",
    "    # Typical architecture MPCNN architecture using alternating convolutional and max-pooling layers. \n",
    "    \n",
    "    model = Sequential()  # initialize Sequential model\n",
    "    \n",
    "    mod = 0\n",
    "    if mod == 0:\n",
    "        model.add(Conv2D(32, (5,5), strides=(1,1), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "        model.add(Conv2D(32 * 2, (5,5), strides=(1,1), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    elif mod == 1:\n",
    "        model.add(Conv2D(8,(4,4), strides=(1,1), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D((8,8), strides=(8,8), padding='same'))\n",
    "        model.add(Conv2D(16,(2,2), strides=(1,1), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D((4,4), strides=(4,4), padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderdecoder_arch(n_outputs, img_dim, rgb_layers, ynum):\n",
    "    \n",
    "    base_dimension = 64          \n",
    "    \n",
    "    model = Sequential()\n",
    "    # 1ère valeur (filters) : le nombre de tranches \"(kernel_val,kernel_val)\" qui composent l'image de sortie\n",
    "    # 2eme valeur (kernel_size) : la taille de la carre/filtre que on glisse au dessous l'image \n",
    "    # 3eme valeur (stride): Le plus grande le stride valeur le plus petite l'image sortie : on prends z_dim/stride_num\n",
    "    \n",
    "    # --------\n",
    "    # Entrée = (img_dim, img_dim, 1)\n",
    "    model.add(Conv2D(base_dimension, (5,5), strides=(2,2), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "    # taille_sortie = (28 + 2*p - 5)/2 + 1\n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    # Entrée = \n",
    "    model.add(Conv2D(base_dimension * 2, (5,5), strides=(2,2), padding='same'))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    model.add(Flatten())\n",
    "\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 4096)\n",
    "    # --------\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CNN(df_test2, X_cols, y_cols, ynum):\n",
    "\n",
    "    # ----------------\n",
    "    img_dim = 64\n",
    "    \n",
    "    # Folding data into CNN image format:\n",
    "    X_train_img, X_test_img, Y_train_1D, Y_test_1D, info  = initialize_CNN(df_test2, X_cols, y_cols, img_dim)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            model = MPCNN_arch(n_outputs, img_dim, ynum)\n",
    "        elif i == 1:\n",
    "            den_activation = LeakyReLU(alpha=0.2)\n",
    "            model = dcgan_arch(n_outputs, img_dim, ynum, den_activation)\n",
    "        elif i == 2:\n",
    "            model = encoderdecoder_arch(n_outputs, img_dim, ynum)\n",
    "    \n",
    "        patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "        \n",
    "        if i == 1:\n",
    "            X_train_1D = []\n",
    "            for lay in range(X_train_img.shape[0]):\n",
    "                # Transformez 3D image à 2D matrix\n",
    "                # Train\n",
    "                X_train_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_train_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_train_1D.append(X_train_2D.flatten())\n",
    "            \n",
    "            X_test_1D = []\n",
    "            for lay in range(X_test_img.shape[0]):\n",
    "                # Test\n",
    "                X_test_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_test_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_test_1D.append(X_test_2D.flatten())\n",
    "            \n",
    "            history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        else:\n",
    "            history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "    \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['mod_train'] = mod_type[a]\n",
    "    dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    cnn2D_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return cnn2D_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3affaa",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b449c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    if stateful == True:\n",
    "        # Quand vous definez batch_input_shape, il faut que des entries de modele être le meme taille que le train dataset\n",
    "        model.add(LSTM(n_a, input_shape=(timesteps_train, feature), batch_input_shape=(batch, timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "    elif stateful == False:\n",
    "        model.add(LSTM(n_a, input_shape=(timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "\n",
    "    # Types of W initializer :\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "    # Compile the model for training\n",
    "    # opt = keras.optimizers.Adam()\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    # opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "    # Si vous utilisez softmax activation, la taille de sortie est plus grand que deux donc il faut categorical_crossentropy\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_train = X_Y_a_tfobj(X_train, Y_train, info['batch_train'], return_sequences, stateless)\n",
    "    tf_test = X_Y_a_tfobj(X_test, Y_test, info['batch_test'], return_sequences, stateless)\n",
    "    \n",
    "    # side idea : if I give tf_test info['batch_train'] instead of info['batch_test'], could I run stateful=True\n",
    "    # for all data (train and test)?\n",
    "    # ----------------\n",
    "\n",
    "    return tf_train, tf_test, X_train, Y_train, X_test, Y_test, info\n",
    "\n",
    "\n",
    "\n",
    "def X_Y_a_tfobj(X, Y, batch_size, return_sequences, stateless):\n",
    "    \n",
    "    if return_sequences == False:  # one output per batch of samples\n",
    "        # Y shape : batch_size,      ie: Y.shape =  (104570,)\n",
    "        temp = [Y[i,0:1,:] for i in range(Y.shape[0])]\n",
    "        Y = np.array(temp)\n",
    "        Y = np.reshape(Y, (batch_size,))\n",
    "    elif return_sequences == True: # an output per each batch of samples\n",
    "        # Y shape : batch_size, timesteps, n_output\n",
    "        Y = np.array(Y)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_data = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    \n",
    "    if stateless == True:  # sequence is non-causal\n",
    "        buffer_size = 1000\n",
    "        tf_data = tf_data.cache().shuffle(buffer_size).batch(batch_size)\n",
    "    elif stateless == False:  # sequence is causal\n",
    "        tf_data = tf_data.batch(batch_size)\n",
    "    \n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTM(df_test2, X_cols, y_cols, ynum):\n",
    "    \n",
    "    return_sequences = False # True=return a prediction at every batch sample, (default) False=return one prediction at the end of the batch\n",
    "    stateless = True # True=shuffle the batch samples/slices --samples are non-causal, False=do not shuffle slices---samples are causal\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Folding data into LSTM format:\n",
    "    tf_train, tf_test, X_train, Y_train, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # -------------------------------\n",
    "\n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    return_state = False # True=return a and c, (default) False=do not return hidden state (a) and cell state (c)\n",
    "    stateful = False #  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    liste_de_vals = [20, 40, 60, 70, 80, 100] # number of dimensions for the hidden state of each LSTM cell\n",
    "    \n",
    "    for n_a in liste_de_vals:\n",
    "\n",
    "        model = LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum)\n",
    "        # -------------------------------\n",
    "        \n",
    "        # 50 est trop, 20 est insuffisant \n",
    "        patience = 28 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "\n",
    "        if stateful == False:\n",
    "            history = model.fit(tf_train, epochs=epochs, validation_data=tf_test, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "            # OU\n",
    "            # history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "            # score = model.evaluate(tf_test, verbose=2)\n",
    "            # test_acc.append(score[1])\n",
    "            # test_loss.append(score[0])\n",
    "        elif stateful == True:\n",
    "            # On peut faire tf_train parce que le taille est fixer at batch_input_shape\n",
    "            history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "        \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['delay_train'] = liste_de_vals[a]\n",
    "    dict_out['delay_test'] = liste_de_vals[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    lstm_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return lstm_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a39dc2",
   "metadata": {},
   "source": [
    "# LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTMCNN(df_test2, X_cols, y_cols, ynum):\n",
    "    \n",
    "    return_sequences = False # True=return a prediction at every batch sample, (default) False=return one prediction at the end of the batch\n",
    "    stateless = True # True=shuffle the batch samples/slices --samples are non-causal, False=do not shuffle slices---samples are causal\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Folding data into LSTM format:\n",
    "    tf_train, tf_test, X_train, Y_train, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "\n",
    "    # Model architecture 1\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    return_state = False # True=return a and c, (default) False=do not return hidden state (a) and cell state (c)\n",
    "    stateful = False #  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "    \n",
    "    n_a = 32  # 20, 40\n",
    "    n_outputs = n_a\n",
    "    model = LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum)\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    # 50 est trop, 20 est insuffisant \n",
    "    patience = 28 # Number of epochs with no improvement after which training will be stopped.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    if stateful == False:\n",
    "        history = model.fit(tf_train, epochs=epochs, validation_data=tf_test, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    elif stateful == True:\n",
    "        # On peut faire tf_train parce que le taille est fixer at batch_input_shape\n",
    "        history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "    # -------------------------------\n",
    "\n",
    "    lstm_model_best = model\n",
    "    \n",
    "    X_lstm_train = lstm_model_best.predict(X_train)\n",
    "    X_lstm_test = lstm_model_best.predict(X_test)\n",
    "    # should be (batch, n_a)\n",
    "    \n",
    "    X_lstm_train = np.array(X_lstm_train)\n",
    "    X_lstm_test = np.array(X_lstm_test)\n",
    "    \n",
    "    print('X_lstm_train : ', X_lstm_train.shape)\n",
    "    print('X_lstm_test : ', X_lstm_test.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # CNN\n",
    "    # -------------------------------\n",
    "    img_dim = 64\n",
    "    \n",
    "    # Folding data into CNN image format:\n",
    "    \n",
    "    # Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "    X_train_img = Xbtf_2_Xbii3(X_lstm_train, img_dim, X_lstm_train.shape[0])\n",
    "    X_test_img = Xbtf_2_Xbii3(X_lstm_test, img_dim, X_lstm_test.shape[0])\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Y_train_1D =  [Y_train[i,0:1,0] for i in range(Y_train.shape[0])]\n",
    "    Y_test_1D =  [Y_test[i,0:1,0] for i in range(Y_test.shape[0])]\n",
    "    Y_train_1D = np.array(Y_train_1D)\n",
    "    Y_test_1D = np.array(Y_test_1D)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # shape of X_train_img :  (batch_train, img_dim, img_dim, 3)\n",
    "    # shape of Y_train_1D :  (batch_train,1)\n",
    "    # shape of X_test_img :  (batch_test, img_dim, img_dim, 3)\n",
    "    # shape of Y_test_1D :  (batch_test,)\n",
    "    \n",
    "    X_train_img = np.asarray(X_train_img, dtype = np.float16, order ='C')  # np.float16, np.float32, np.float64\n",
    "    Y_train_1D = np.asarray(Y_train_1D, dtype = np.float16, order ='C')\n",
    "    X_test_img = np.asarray(X_test_img, dtype = np.float16, order ='C')\n",
    "    Y_test_1D = np.asarray(Y_test_1D, dtype = np.float16, order ='C')\n",
    "    \n",
    "    print('X_train_img:' , X_train_img.shape)\n",
    "    print('Y_train_1D:' , Y_train_1D.shape)\n",
    "    print('X_test_img:' , X_test_img.shape)\n",
    "    print('Y_test_1D:' , Y_test_1D.shape)\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Model architecture 2\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            model = MPCNN_arch(n_outputs, img_dim, ynum)\n",
    "        elif i == 1:\n",
    "            model = dcgan_arch(n_outputs, input_shape, ynum)\n",
    "        elif i == 2:\n",
    "            model = encoderdecoder_arch(n_outputs, img_dim, ynum)\n",
    "    \n",
    "        patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "        \n",
    "        if i == 1:\n",
    "            X_train_1D = []\n",
    "            for lay in range(X_train_img.shape[0]):\n",
    "                # Transformez 3D image à 2D matrix\n",
    "                # Train\n",
    "                X_train_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_train_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_train_1D.append(X_train_2D.flatten())\n",
    "            \n",
    "            X_test_1D = []\n",
    "            for lay in range(X_test_img.shape[0]):\n",
    "                # Test\n",
    "                X_test_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_test_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_test_1D.append(X_test_2D.flatten())\n",
    "            \n",
    "            history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        else:\n",
    "            history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "    \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['mod_train'] = mod_type[a]\n",
    "    dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    cnn2D_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return cnn2D_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2b729",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f455fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    # initialize a matrix angle_rads of all the angles\n",
    "    # Get the angles for the positional encoding\n",
    "    # pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "    # i --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]\n",
    "    pos = np.reshape(np.multiply(range(0, positions), 1), (positions,1))\n",
    "    i = np.reshape(np.multiply(range(0, d), 1), (1,d))\n",
    "    angle_rads = pos / (10000 ** ((2*(i//2))/d))\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # END CODE HERE\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Mask\n",
    "# Sequences longer than the maximum length of five will be truncated, and zeros will be added to the \n",
    "# truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum \n",
    "# length, they zeros will also be added for padding. \n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    # If x equals 0 it put a True, and if not puts a False\n",
    "    #out1 = tf.math.equal(x, 0)  \n",
    "    #print('out1 : ' + str(out1))\n",
    "\n",
    "    # Converts the boolean matrix to a float32\n",
    "    #out2 = tf.cast(out1, tf.float32)\n",
    "    #print('out2 : ' + str(out2))\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    # it takes the (3,5) sized seq matrix and makes it a (3, 1, 1, 5) matrix\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b81821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-ahead Mask\n",
    "# In training, you will have access to the complete correct output of your training example. \n",
    "# The look-ahead mask helps your model pretend that it correctly predicted a part of the output and \n",
    "# see if, without looking ahead, it can correctly predict the next output.\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    #Based on the lower and upper matrix diagonal keep values, it keeps \n",
    "    #the matrix entries and sets the other entries to zero\n",
    "    #If lower_mat_dia_keep=0 and upper_mat_dia_keep=1 it keeps the diagonal and one upper\n",
    "    #diagonal section, similarly lower_mat_dia_keep=-1 and upper_mat_dia_keep=0 it keeps\n",
    "    #one lower diagonal section and the diagonal\n",
    "    #out1 = tf.linalg.band_part(tf.ones((3, 3)), lower_mat_dia_keep, upper_mat_dia_keep)\n",
    "    #print('out1 : ' + str(out1))\n",
    "    \n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, embedding_dim) = (batch, timesteps, feature)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim, fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "        # START CODE HERE\n",
    "        # calculate self-attention using mha(~1 line)\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to the self-attention output (~1 line)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer (~1 line)\n",
    "        out1 = self.layernorm1(tf.add(x, attn_output))  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output (~1 line)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer (~1 line)\n",
    "        out2 = self.layernorm2(tf.add(out1, ffn_output))  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        # END CODE HERE\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6, textORnot='timeseries'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.textORnot = textORnot\n",
    "        \n",
    "        if self.textORnot == 'text':\n",
    "            self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        if self.textORnot == 'text':\n",
    "            # Text ONLY : Pass input through the Embedding layer\n",
    "            x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim) = (batch, timesteps, feature)\n",
    "            # print('shape of embedding : ', x.shape)\n",
    "        \n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        # Remember to cast the embedding dimension to data type tf.float32 before computing the square root.\n",
    "        x *= tf.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        #print('shape of x : ' + str(x.shape))\n",
    "        #shape of x : (2, 3, 4) so loop over the first entry\n",
    "        \n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim) = (batch, timesteps, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d46c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeY3D_a_Y1D(Y_bin):\n",
    "    # Y_bin : (batch, timesteps, n_outputs) OU Y_bin : (batch, n_outputs)\n",
    "    \n",
    "    batch = Y_bin.shape[0]\n",
    "    \n",
    "    if len(Y_bin.shape) == 2:\n",
    "        # Y_bin : (batch, n_outputs)\n",
    "        temp = [np.argmax(Y_bin[i,:]) for i in range(batch)]   # (batch,)\n",
    "    \n",
    "    elif len(Y_bin.shape) == 3:\n",
    "        # Y_bin : (batch, timesteps, n_outputs)\n",
    "        temp = [np.argmax(Y_bin[i,0:1,:]) for i in range(batch)]   # (batch,)\n",
    "    \n",
    "    Y = np.array(temp)\n",
    "    Y = np.reshape(Y, (batch,)) # Y : (batch, )\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de233ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Transformer(X_train, X_test, Y_train, Y_test, patience, batch_size, epochs):\n",
    "    \n",
    "    # X_train : (batch_train, timesteps_train, feature)\n",
    "    # X_test : (batch_test, timesteps_test, feature)\n",
    "    \n",
    "    # Y_train : (batch_train, n_outputs)\n",
    "    # Y_test : (batch_test, n_outputs)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    batch_train = X_train.shape[0]\n",
    "    timesteps_train = X_train.shape[1]\n",
    "    feature = X_train.shape[2]\n",
    "    \n",
    "    batch_test = X_test.shape[0]\n",
    "    timesteps_test = X_test.shape[1]\n",
    "    \n",
    "    n_outputs = Y_train.shape[1]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Model architecture\n",
    "    \n",
    "    # Encoder (shuffling and transformation of the data)\n",
    "    tf.random.set_seed(1)  #10\n",
    "    \n",
    "    # unique words, or unique data points to a certain precision\n",
    "    precision = 0.1\n",
    "    input_vocab_size=len(np.arange(0,1,precision))\n",
    "    \n",
    "    # the maximum number of words in each sentence, OR timesteps\n",
    "    maximum_position_encoding = timesteps_train\n",
    "    \n",
    "    encoderq = Encoder(num_layers=6, embedding_dim=feature, num_heads=1, fully_connected_dim=2*feature, \n",
    "                       input_vocab_size=input_vocab_size,\n",
    "                       maximum_position_encoding=maximum_position_encoding, dropout_rate=0.1, \n",
    "                       layernorm_eps=1e-6, textORnot='timeseries')\n",
    "    \n",
    "    training = True  # training for Dropout layers\n",
    "    mask = None #create_padding_mask(X_train)  # create_look_ahead_mask(x.shape[1]) # None\n",
    "    encoder_X_train = encoderq(X_train, training, mask)   # output is shape=(batch_train, timesteps_train, feature)\n",
    "    \n",
    "    mask = None #create_padding_mask(X_test)  # create_look_ahead_mask(x.shape[1]) # None\n",
    "    encoder_X_test = encoderq(X_test, training, mask)   # output is shape=(batch_test, timesteps_test, feature)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Final Fully Connected\n",
    "    model = Sequential()\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    if n_outputs > 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "    # --------\n",
    "\n",
    "    # Compile the model for training\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    if n_outputs > 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "\n",
    "    # -------------------------------\n",
    "    \n",
    "    X_train_1D = np.reshape(encoder_X_train, (batch_train, timesteps_train*feature))\n",
    "    X_test_1D = np.reshape(encoder_X_test, (batch_test, timesteps_test*feature))\n",
    "    \n",
    "    verbose = 2\n",
    "    #history = model.fit(X_train_1D, Y_train, epochs=epochs, \n",
    "    #                    validation_data=(X_test_1D, Y_test), \n",
    "    #                    batch_size=batch_size, callbacks=[early_stopping], verbose=verbose)\n",
    "    history = model.fit(X_train_1D, Y_train, epochs=epochs, \n",
    "                        validation_data=(X_test_1D, Y_test), \n",
    "                        batch_size=batch_size, verbose=verbose)\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return model, history_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
