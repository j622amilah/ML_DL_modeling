{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c481e0d3",
   "metadata": {},
   "source": [
    "# Mouvements de joystick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5676b",
   "metadata": {},
   "source": [
    "## Marquers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab20dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76484add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from scipy import signal\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "from os.path import exists\n",
    "import pickle\n",
    "\n",
    "varr = {}\n",
    "varr['main_path'] = \"/home/oem2/Documents/9_Motor_classification_2018-22/Coding_version3_python_FINAL\"  \n",
    "varr['main_path1'] = \"%s/a_data_standardization\" % (varr['main_path'])\n",
    "varr['main_path2'] = \"%s/b_data_preprocessing\" % (varr['main_path'])\n",
    "varr['main_path3'] = \"%s/c_calculate_metrics\" % (varr['main_path'])\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '%s' % (varr['main_path']))\n",
    "\n",
    "# Personal python functions\n",
    "from c_calculate_metrics.put_timeseries_trialdata_into_pandas import *\n",
    "\n",
    "# from subfunctions.make_a_properlist import *\n",
    "from subfunctions.numderiv import *\n",
    "from subfunctions.freqresp_functions import get_freqresp_mag_phase, select_fc, select_filter\n",
    "from subfunctions.freq_from_sig_timecounting import *\n",
    "from subfunctions.freq_from_sig_freqresp import *\n",
    "from subfunctions.my_dropna_python import *\n",
    "from subfunctions.scale_feature_data import *\n",
    "from subfunctions.normal_distribution_feature_data import *\n",
    "from subfunctions.findall import *\n",
    "from subfunctions.explode_without_colnames2 import *\n",
    "#from subfunctions.scikit_functions_binaryclass import *\n",
    "#from subfunctions.scikit_functions import *\n",
    "\n",
    "from subfunctions.tsig_2_discrete_wavelet_transform import *\n",
    "from subfunctions.tsig_2_spectrogram import *\n",
    "from subfunctions.tsig_2_continuous_wavelet_transform import *\n",
    "\n",
    "df_timeseries_exp = put_timeseries_trialdata_into_pandas(varr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f09d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[rot] :  (86006, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86001</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.453208</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699437</td>\n",
       "      <td>-1.514817</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>-4.719625</td>\n",
       "      <td>0.301392</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-4.927108</td>\n",
       "      <td>0.850650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.927108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86002</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.391906</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.461697</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>-4.719593</td>\n",
       "      <td>0.333524</td>\n",
       "      <td>-0.082369</td>\n",
       "      <td>-4.917930</td>\n",
       "      <td>0.902199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.917930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86003</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>16.500001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.313130</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.382446</td>\n",
       "      <td>0.807940</td>\n",
       "      <td>-4.719292</td>\n",
       "      <td>0.356304</td>\n",
       "      <td>-0.086946</td>\n",
       "      <td>-4.908213</td>\n",
       "      <td>0.953649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86004</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>16.599999</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.221655</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.284506</td>\n",
       "      <td>0.807370</td>\n",
       "      <td>-4.719829</td>\n",
       "      <td>0.362870</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>-4.897957</td>\n",
       "      <td>1.004994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.897957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86005</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.125844</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.179229</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>-4.719719</td>\n",
       "      <td>0.364322</td>\n",
       "      <td>-0.088796</td>\n",
       "      <td>-4.887165</td>\n",
       "      <td>1.056230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.887165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp       time  res_type  SIGCOM_ax0  \\\n",
       "86001     17.0  20.0 -1.0  0.0  163.0  16.300000       7.0   -1.453208   \n",
       "86002     17.0  20.0 -1.0  0.0  164.0  16.400000       7.0   -1.391906   \n",
       "86003     17.0  20.0 -1.0  0.0  165.0  16.500001       7.0   -1.313130   \n",
       "86004     17.0  20.0 -1.0  0.0  166.0  16.599999       7.0   -1.221655   \n",
       "86005     17.0  20.0 -1.0  0.0  167.0  16.700000       7.0   -1.125844   \n",
       "\n",
       "       SIGCOM_ax1  SIGCOM_ax2   SIG_ax0   SIG_ax1   SIG_ax2   JOY_ax0  \\\n",
       "86001    0.804159   -4.699437 -1.514817  0.803347 -4.719625  0.301392   \n",
       "86002    0.804159   -4.699425 -1.461697  0.803352 -4.719593  0.333524   \n",
       "86003    0.804159   -4.699425 -1.382446  0.807940 -4.719292  0.356304   \n",
       "86004    0.804159   -4.699425 -1.284506  0.807370 -4.719829  0.362870   \n",
       "86005    0.804159   -4.699425 -1.179229  0.805081 -4.719719  0.364322   \n",
       "\n",
       "        JOY_ax1   JOY_ax2  NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "86001 -0.082375 -4.927108   0.850650        0.0  -4.927108  \n",
       "86002 -0.082369 -4.917930   0.902199        0.0  -4.917930  \n",
       "86003 -0.086946 -4.908213   0.953649        0.0  -4.908213  \n",
       "86004 -0.089799 -4.897957   1.004994        0.0  -4.897957  \n",
       "86005 -0.088796 -4.887165   1.056230        0.0  -4.887165  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[rot] : ', df_timeseries_exp['rot'].shape)\n",
    "df_timeseries_exp['rot'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c8ee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[trans] :  (43754, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43749</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.3868</td>\n",
       "      <td>51.1975</td>\n",
       "      <td>-1.3760</td>\n",
       "      <td>68.5344</td>\n",
       "      <td>31.0360</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43750</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.7618</td>\n",
       "      <td>51.4284</td>\n",
       "      <td>-1.3559</td>\n",
       "      <td>68.9152</td>\n",
       "      <td>31.1329</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43751</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.1368</td>\n",
       "      <td>51.6561</td>\n",
       "      <td>-1.3360</td>\n",
       "      <td>69.2979</td>\n",
       "      <td>31.2316</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43752</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.5118</td>\n",
       "      <td>51.7815</td>\n",
       "      <td>-1.3128</td>\n",
       "      <td>69.6821</td>\n",
       "      <td>31.3353</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43753</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.7139</td>\n",
       "      <td>51.7700</td>\n",
       "      <td>-1.2916</td>\n",
       "      <td>70.0518</td>\n",
       "      <td>31.4619</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp  time  res_type  SIGCOM_ax0  SIGCOM_ax1  \\\n",
       "43749     13.0  16.0  1.0  1.0  197.0  19.7       7.0         0.0     69.3868   \n",
       "43750     13.0  16.0  1.0  1.0  198.0  19.8       7.0         0.0     69.7618   \n",
       "43751     13.0  16.0  1.0  1.0  199.0  19.9       7.0         0.0     70.1368   \n",
       "43752     13.0  16.0  1.0  1.0  200.0  20.0       7.0         0.0     70.5118   \n",
       "43753     13.0  16.0  1.0  1.0  201.0  20.1       7.0         0.0     70.7139   \n",
       "\n",
       "       SIGCOM_ax2  SIG_ax0  SIG_ax1  SIG_ax2  JOY_ax0  JOY_ax1  JOY_ax2  \\\n",
       "43749     51.1975  -1.3760  68.5344  31.0360  -0.0412  -0.0293      0.0   \n",
       "43750     51.4284  -1.3559  68.9152  31.1329  -0.0412  -0.0294      0.0   \n",
       "43751     51.6561  -1.3360  69.2979  31.2316  -0.0412  -0.0272      0.0   \n",
       "43752     51.7815  -1.3128  69.6821  31.3353  -0.0408  -0.0273      0.0   \n",
       "43753     51.7700  -1.2916  70.0518  31.4619  -0.0408  -0.0273      0.0   \n",
       "\n",
       "       NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "43749       3.75       -0.0        0.0  \n",
       "43750       3.75       -0.0        0.0  \n",
       "43751       3.75       -0.0        0.0  \n",
       "43752       3.75       -0.0        0.0  \n",
       "43753       0.00        0.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[trans] : ', df_timeseries_exp['trans'].shape)\n",
    "df_timeseries_exp['trans'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d651ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(ax_val, ss_val):\n",
    "    \n",
    "    if ax_val == 'all' and ss_val == 'all':\n",
    "        # All the data\n",
    "        df_timeseries_exp[exp].head()\n",
    "        df = df_timeseries_exp[exp]\n",
    "    elif ax_val != 'all' and ss_val == 'all':\n",
    "        # Prediction for each axis\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "    elif ax_val == 'all' and ss_val != 'all':\n",
    "        # Prediction per sup/sub\n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0)]\n",
    "    elif ax_val != 'all' and ss_val != 'all':\n",
    "        # Prediction per axis and sup/sub\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        \n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "\n",
    "    print('Confirmation : exp=', exp, ', ax_val=', ax_val, ', ss_val=', ss_val)\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b3331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the joystick stimulus axis data and put it in a pandas column\n",
    "def indexit(row):\n",
    "    joy_mat = [row.JOY_ax0, row.JOY_ax1, row.JOY_ax2]\n",
    "    return joy_mat[int(row.ax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_and_initial_feature(df):\n",
    "    # IC = 1\n",
    "    # EC = 2, 4, 5\n",
    "    # NC = 3, 6, 7\n",
    "    # NR = 9\n",
    "    # (IC) - sham (do not use) = 8\n",
    "    # (NC) - sham (do not use) = 10\n",
    "\n",
    "    # Just to confirm, what are the unique values of res_type\n",
    "    df.res_type.value_counts(ascending=True)\n",
    "\n",
    "    # Construction of SD_label : How do we define disorientation?\n",
    "\n",
    "    # Way 0 : lenient\n",
    "    # 0 = If participates got the result CORRECT for the trial, they were NOT disoriented. (IC, EC)\n",
    "    # 1 = If participates got the result WRONG or did not respond, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1) | (df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.lenient = ''  # define a new column , rows 8 and 10 will be NaN, need to do dropna for rows\n",
    "    df.loc[idx_NDS, 'lenient'] = 0\n",
    "    df.loc[idx_DS, 'lenient'] = 1\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Way 1 : strict simple\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants were WRONG at any point, they were disoriented. (EC, NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5) | (df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.strict = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'strict'] = 0\n",
    "    df.loc[idx_DS, 'strict'] = 1\n",
    "\n",
    "    # Way 2 : st_complex\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants got the result eventually CORRECT, they were MILDLY disoriented. (EC)\n",
    "    # 2 = If participants were WRONG for the trial, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_MDS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.st_complex = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'st_complex'] = 0\n",
    "    df.loc[idx_MDS, 'st_complex'] = 1\n",
    "    df.loc[idx_DS, 'st_complex'] = 2\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Create features :  (1) position\n",
    "    df['joy_stim'] = df.apply(indexit, axis='columns')  # fill in joy_stim\n",
    "    \n",
    "    # -------------------------------------\n",
    "\n",
    "    # Make DataFrame for trial start-stop index\n",
    "    # Cut the data up per trial across subjects\n",
    "    tr_vec = df.tr.to_numpy()\n",
    "\n",
    "    st = [0]\n",
    "    ender = []\n",
    "    for i in range(len(tr_vec)-1):\n",
    "        if tr_vec[i] != tr_vec[i+1]:\n",
    "            st = st + [i+1]\n",
    "            ender = ender + [i]\n",
    "    ender = ender + [len(tr_vec)-1]\n",
    "\n",
    "    # See start-stop index clearly\n",
    "    e0 = np.reshape(st, (len(st),1))\n",
    "    e1 = np.reshape(ender, (len(st),1))\n",
    "    data = np.ravel(e0), np.ravel(e1)\n",
    "    data = np.transpose(data)\n",
    "    columns = ['stind', 'endind']\n",
    "    temp = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Find the longest trial signal in df_rot['joy_stim']\n",
    "    temp['diff'] = temp.endind - temp.stind\n",
    "    temp['timediff'] = [df.time.iloc[temp.endind[i]] - df.time.iloc[temp.stind[i]] for i in range(len(temp.endind))]\n",
    "    outmin = temp['diff'].min()\n",
    "    outmax = temp['diff'].max()\n",
    "\n",
    "    tomin = temp['timediff'][(temp['diff'] == outmin)]\n",
    "    tomax = temp['timediff'][(temp['diff'] == outmax)]\n",
    "    # print('outmin : ', outmin, 't :', tomin)\n",
    "    # print('outmax : ', outmax, 't :', tomax)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Interpolate : make each trial the same number of data points\n",
    "    from scipy.interpolate import interp1d\n",
    "    feat0 = []\n",
    "    t_feat0 = []\n",
    "    y1_feat0 = []\n",
    "    y2_feat0 = []\n",
    "    y3_feat0 = []\n",
    "    for i in range(len(temp.stind)):\n",
    "        \n",
    "        # X\n",
    "        sSIG = df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        t_sSIG = df['time'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "\n",
    "        # labels\n",
    "        y1 = df['lenient'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y2 = df['strict'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y3 = df['st_complex'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        \n",
    "        # Check if trial data is less than the maximum length\n",
    "        if len(df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]]) < outmax:\n",
    "            \n",
    "            # The trial length is different so interpolate the time-series to make them the same length signal \n",
    "            x = np.linspace(sSIG[0], len(sSIG), num=len(sSIG), endpoint=True)\n",
    "            xnew = np.linspace(sSIG[0], len(sSIG), num=outmax, endpoint=True)\n",
    "\n",
    "            # joystick on stim\n",
    "            f = interp1d(x, sSIG)\n",
    "            sSIGl = f(xnew)\n",
    "\n",
    "            # time\n",
    "            f = interp1d(x, t_sSIG)\n",
    "            t_sSIGl = f(xnew)\n",
    "\n",
    "            # y1\n",
    "            f = interp1d(x, y1)\n",
    "            y1_sSIGl = f(xnew)\n",
    "\n",
    "            # y2\n",
    "            f = interp1d(x, y2)\n",
    "            y2_sSIGl = f(xnew)\n",
    "\n",
    "            # y3\n",
    "            f = interp1d(x, y3)\n",
    "            y3_sSIGl = f(xnew)\n",
    "\n",
    "            # python : you can not create a matrix in real-time in pandas\n",
    "            # you only assign the full matrix at the end\n",
    "            # (0) position\n",
    "            feat0 = feat0 + [sSIGl]\n",
    "            t_feat0 = t_feat0 + [t_sSIGl]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1_sSIGl)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2_sSIGl)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3_sSIGl)]\n",
    "            \n",
    "            del x, f, sSIGl, t_sSIGl, y1_sSIGl, y2_sSIGl, y3_sSIGl\n",
    "        else:\n",
    "            feat0 = feat0 + [sSIG]\n",
    "            t_feat0 = t_feat0 + [t_sSIG]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3)]\n",
    "\n",
    "    # Clean up\n",
    "    del df\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1b43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creation_preprocessing(feat0, t_feat0):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # ----------------\n",
    "    # Make your features\n",
    "    # ----------------\n",
    "    df_feat = pd.DataFrame()\n",
    "    \n",
    "    n = 4   # filter order\n",
    "    fs = 250 # data sampling frequency (Hz)\n",
    "    fcc = 10  # Cut-off frequency of the filter\n",
    "    w = fcc / (fs / 2) # Normalize the frequency\n",
    "    b, a = signal.butter(n, w, 'low')  # 3rd order\n",
    "    \n",
    "    scales = np.arange(1, 128)\n",
    "    \n",
    "    # Need to find when one trial starts and end - take derivative from start-stop periods\n",
    "    for i in range(len(feat0)):\n",
    "\n",
    "        if i == 0:\n",
    "            plotORnot = 1\n",
    "        else:\n",
    "            plotORnot = 0\n",
    "        \n",
    "        # ----------------------------\n",
    "        # (0) position - causale ordre\n",
    "        col0 = scale_feature_data(feat0[i], plotORnot)\n",
    "        \n",
    "        # (1) velocity - causale ordre\n",
    "        vel = numderiv(feat0[i], t_feat0[i])\n",
    "        col1 = scale_feature_data(vel, plotORnot)\n",
    "\n",
    "        # (2) acceleration - causale ordre\n",
    "        acc = numderiv(vel, t_feat0[i])\n",
    "        filtacc = signal.filtfilt(b, a, acc) # the signal is noisy\n",
    "        col2 = scale_feature_data(filtacc, plotORnot)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # ----------------------------\n",
    "        # (4) position - non-causale ordre\n",
    "        col3 = normal_distribution_feature_data(col0, plotORnot)\n",
    "        \n",
    "        # (5) velocity - non-causale ordre\n",
    "        col4 = normal_distribution_feature_data(col1, plotORnot)\n",
    "        \n",
    "        # (6) acceleration - non-causale ordre\n",
    "        col5 = normal_distribution_feature_data(col2, plotORnot)\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ----------------------------\n",
    "        # Frequence marquers : sublevels of frequency pattern\n",
    "        # ----------------------------\n",
    "        # (7-22) une transformation de fréquence (ondelettes)\n",
    "        coeff = tsig_2_discrete_wavelet_transform(col0, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols6 = pd.DataFrame(coeff).T\n",
    "        \n",
    "        coeff = tsig_2_discrete_wavelet_transform(col1, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols7 = pd.DataFrame(coeff).T\n",
    "        \n",
    "        coeff = tsig_2_discrete_wavelet_transform(col2, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols8 = pd.DataFrame(coeff).T\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        # ----------------------------\n",
    "        # Hybrid marquers : temporalle et frequence information\n",
    "        # ----------------------------\n",
    "        # (8) spectrogram flatten - periodogram (fft)\n",
    "        col9 = tsig_2_spectrogram(col0, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        \n",
    "        col10 = tsig_2_spectrogram(col1, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        \n",
    "        col11 = tsig_2_spectrogram(col2, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # ----------------------------\n",
    "        # (9) continuous wavelet transform flatten\n",
    "        # continuous_wavelets = ['mexh', 'morl', 'cgau5', 'gaus5']\n",
    "        col12 = tsig_2_continuous_wavelet_transform(t_feat0[i], col0, scales, waveletname='mexh', plotORnot=0)\n",
    "        \n",
    "        col13 = tsig_2_continuous_wavelet_transform(t_feat0[i], col1, scales, waveletname='mexh', plotORnot=0)\n",
    "        \n",
    "        col14 = tsig_2_continuous_wavelet_transform(t_feat0[i], col2, scales, waveletname='mexh', plotORnot=0)\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        # Peut-être faire des non-causale frequence marquers pour des mieux frequence et hybrid \n",
    "        \n",
    "        tr = pd.Series(i*np.ones(len(col0)))\n",
    "        c0 = pd.Series(col0)\n",
    "        c1 = pd.Series(col1)\n",
    "        c2 = pd.Series(col2)\n",
    "        c3 = pd.Series(col3)\n",
    "        c4 = pd.Series(col4)\n",
    "        c5 = pd.Series(col5)\n",
    "        cs6 = cols6\n",
    "        cs7 = cols7\n",
    "        cs8 = cols8\n",
    "        c9 = pd.Series(col9)\n",
    "        c10 = pd.Series(col10)\n",
    "        c11 = pd.Series(col11)\n",
    "        \n",
    "        c12 = pd.Series(col12)\n",
    "        c13 = pd.Series(col13)\n",
    "        c14 = pd.Series(col14)\n",
    "        \n",
    "        # Clean up\n",
    "        del vel, acc, filtacc, coeff\n",
    "        del col0, col1, col2, col3, col4, col5, cols6, cols7, cols8, col9, col10, col11, col12, col13, col14\n",
    "        \n",
    "        temp = pd.concat([tr, c0, c1, c2, c3, c4, c5, cs6, cs7, cs8, c9, c10, c11, c12, c13, c14], axis=1)\n",
    "        \n",
    "        # Clean up\n",
    "        del tr, c0, c1, c2, c3, c4, c5, cs6, cs7, cs8, c9, c10, c11, c12, c13, c14\n",
    "        \n",
    "        df_feat = pd.concat([df_feat, temp], axis=0)\n",
    "        \n",
    "        del temp\n",
    "        \n",
    "    # ----------------\n",
    "\n",
    "    end = time.time()\n",
    "    print('Elasped time for feature processing : ', end - start)\n",
    "\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8143131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rename_columns(df, col_list):\n",
    "    \n",
    "    way = 1\n",
    "    \n",
    "    if way == 0:\n",
    "        # Façon difficile\n",
    "        # Rename columns\n",
    "        onames = df.columns.to_numpy()\n",
    "        dictout = {}\n",
    "        for nf in range(len(col_list)):\n",
    "            dictout[onames[nf]] = '%s' % (col_list[nf])\n",
    "            \n",
    "        # Determinez quels columns de df repeter\n",
    "        uq = Counter(onames).most_common()\n",
    "        d = {}\n",
    "        for i in range(len(uq)):\n",
    "            temp = []\n",
    "            for ind, val in enumerate(onames):\n",
    "                if uq[i][0] == val:\n",
    "                    temp.append(col_list[ind])\n",
    "            d[i] = temp    \n",
    "        \n",
    "        # if the column name is a key of d pop the names in the list, else return the column name\n",
    "        df.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    \n",
    "    elif way == 1:\n",
    "        # Façon facile\n",
    "        df.columns = col_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd76362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data_2makeclasses_equivalent(df_feat):\n",
    "    # Remove nan value per row\n",
    "    # df_test_noNan = df_feat.dropna(axis=0)\n",
    "    # OR\n",
    "    df_test_noNan = my_dropna_python(df_feat)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Confirm that there are no nan values\n",
    "    out = df_test_noNan.isnull().values.any()\n",
    "    print('Are there nan valeus in the data : ', out)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Check class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test_noNan)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    print('shape of dataframe before padding : ', df_test_noNan.shape)\n",
    "\n",
    "    # ----------------\n",
    "    # Pad the DataFrame\n",
    "    n_classes = len(count_index)\n",
    "    n_samples = len(st)\n",
    "\n",
    "    df_2add_on = pd.DataFrame()\n",
    "    \n",
    "    # Le derniere sample dans df_test_noNan\n",
    "    df_coupe_proche = df_test_noNan.iloc[st[-1]:endd[-1], :]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        #print('i : ', i)\n",
    "        # Pad short length classes\n",
    "        for j in range(needed_samps_class[i]):\n",
    "            #print('j : ', j) \n",
    "            flag = 0\n",
    "            while flag == 0:\n",
    "                permvec = np.random.permutation(n_samples)\n",
    "                index = permvec[0]  #random choosen index\n",
    "                \n",
    "                # look for each class : on veut le classe être le meme\n",
    "                if i == int(df_test_noNan.y.iloc[st[index]]):\n",
    "                    #print('Class match was found : i = ', i, ', data index = ', int(df_test_noNan.y_scalar.iloc[index]), ', index = ', index)\n",
    "                    \n",
    "                    # Append the data with padded data entry\n",
    "                    df_coupe = df_test_noNan.iloc[st[index]:endd[index], :]\n",
    "                    \n",
    "                    # Le derriere sample ne sont pas le meme que le sample actuelle\n",
    "                    if int(df_coupe.iloc[0,0] - df_coupe_proche.iloc[0,0]) != 0:\n",
    "                        df_coupe_proche = df_coupe\n",
    "                        df_2add_on = pd.concat([df_2add_on, df_coupe], axis=0)\n",
    "                        flag = 1 # to brake while\n",
    "                        \n",
    "    # ----------------\n",
    "\n",
    "    # DataFrame a besoin les noms de columns d'avoir le meme noms que df_test_noNan\n",
    "    df_2add_on = df_2add_on.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    col_list = df_test_noNan.columns\n",
    "    df_2add_on = pandas_rename_columns(df_2add_on, col_list)\n",
    "    df_2add_on\n",
    "\n",
    "    print('shape of dataframe to add to original dataframe: ', df_2add_on.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # want to arrange the dataframe with respect to rows (stack on top of the other): so axis=0 \n",
    "    # OR think of it as the rows of the df change so you put axis=0 for rows\n",
    "    df_test2 = pd.concat([df_test_noNan, df_2add_on], axis=0)\n",
    "    df_test2 = df_test2.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    print('shape of padded dataframe (original + toadd) : ', df_test2.shape)\n",
    "\n",
    "    del df_test_noNan, df_2add_on\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Final check of class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Enlevez des columns unnecessaires : num, y\n",
    "    # df_test2 = df_test2.drop(['num', 'y'], axis=1)  # 1 is the axis number (0 for rows and 1 for columns)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Rename each feature column a number, and the label column with y \n",
    "    # col_list = list(map(str, np.arange(len(df_test2.columns) - 1)))\n",
    "    # col_list.append('y')\n",
    "    # df_test2 = pandas_rename_columns(df_test2, col_list)\n",
    "    \n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nonconsecutive_values_debut_fin_pt(vec):\n",
    "  \n",
    "    st = [0]\n",
    "    endd = []\n",
    "    \n",
    "    for i in range(len(vec)-1):\n",
    "        if vec[i] != vec[i+1]:\n",
    "            st.append(i+1)\n",
    "            endd.append(i)\n",
    "    \n",
    "    endd.append(len(vec)-1)\n",
    "    \n",
    "    return st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79951844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(df_test_noNan):\n",
    "\n",
    "    # Get start and end index values for each sample\n",
    "    num = list(map(int, df_test_noNan.num.to_numpy()))\n",
    "    st, endd = detect_nonconsecutive_values_debut_fin_pt(num)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    yy = list(map(int, df_test_noNan.y.to_numpy()))\n",
    "    y_short = []\n",
    "    for i in range(len(st)):\n",
    "        y_short.append(yy[st[i]:st[i]+1][0])\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    liste = Counter(y_short).most_common()\n",
    "    count_index, counted_value = list(map(list, zip(*liste)))\n",
    "\n",
    "    print('Before sorting counted_value : ', counted_value)\n",
    "    print('Before sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Sort counted_value by count_index; in ascending order\n",
    "    sind = np.argsort(count_index)\n",
    "    count_index = [count_index[i] for i in sind]\n",
    "    counted_value = [counted_value[i] for i in sind]\n",
    "    print('After sorting counted_value : ', counted_value)\n",
    "    print('After sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Determine how much to pad each class label\n",
    "    needed_samps_class = np.max(counted_value) - counted_value\n",
    "    print('needed_samps_class : ', needed_samps_class)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    return needed_samps_class, counted_value, count_index, st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f549b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dat_pickle(outSIG, file_name=\"outSIG.pkl\"):\n",
    "    # Save data matrices to file\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(outSIG, open_file)\n",
    "    open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d9ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dat_pickle(file_name=\"outSIG.pkl\"):\n",
    "    open_file = open(file_name, \"rb\")\n",
    "    dataout = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return dataout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f95d2",
   "metadata": {},
   "source": [
    "## Verifier des marquers : Plot des marquers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos, label=\"num\")\n",
    "sns.lineplot(data=df_feat.vel, label=\"num\", color='red')\n",
    "sns.lineplot(data=df_feat.acc, label=\"num\", color='green')\n",
    "plt.title(\"position, velocity, acceleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_sl0, label=\"position\")\n",
    "sns.lineplot(data=df_feat.vel_sl0, label=\"velocity\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_sl0, label=\"acceleration\", color='green')\n",
    "plt.title(\"wavelet sublevel : position, velocity, accleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44874b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_spec, label=\"pos_spec\")\n",
    "sns.lineplot(data=df_feat.vel_spec, label=\"vel_spec\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_spec, label=\"acc_spec\", color='green')\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_cwt, label=\"pos_cwt\")\n",
    "sns.lineplot(data=df_feat.vel_cwt, label=\"vel_cwt\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_cwt, label=\"acc_cwt\", color='green')\n",
    "plt.title(\"Continuous Wavelet Transform\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389323f",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b07dfe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(d, reverse = False):\n",
    "    return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))\n",
    "\n",
    "\n",
    "\n",
    "def permutation_importance_tensorflow(model, X_test, Y_test):\n",
    "\n",
    "    Y_test_1D = [Y_test[i,0:1] for i in range(Y_test.shape[0])]\n",
    "\n",
    "    # First, a baseline metric, defined by scoring,\n",
    "    # Obtenez mean absolute error\n",
    "    y_hat_test = model.predict(X_test, verbose=0)\n",
    "    baseline_mae = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "\n",
    "    vals = {}\n",
    "    # Shuffle each feature columns at a time\n",
    "    for featcol in range(X_test.shape[2]):\n",
    "\n",
    "        # Define a modifiable temporary variable\n",
    "        temp = X_test\n",
    "\n",
    "        # select a column\n",
    "        feat_slice = temp[:,:,featcol]\n",
    "\n",
    "        # Must flatten the matrix because np.random.permutation or \n",
    "        # np.random.shuffle don't work\n",
    "        t = feat_slice.flatten()\n",
    "        t_shuf = np.random.permutation(t)\n",
    "        feat_slice =  np.reshape(t_shuf, (feat_slice.shape))\n",
    "\n",
    "        # put feat_slice back into temp\n",
    "        temp[:,:,featcol] = feat_slice\n",
    "\n",
    "        y_hat_test = model.predict(temp, verbose=0)\n",
    "        mae_per_col = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "        vals[featcol] = mae_per_col\n",
    "\n",
    "    # Sort the columns from largest to smallest mae\n",
    "    laquelle = sort_dict_by_value(vals, reverse = True)\n",
    "    \n",
    "    # Determinez le nombres des columns qui sont plus grande que le baseline_mae\n",
    "    # C'est des marqueurs qui sont importants\n",
    "    feat = list(laquelle.keys())\n",
    "    cnt = [1 for i in range(len(feat)) if feat[i] > baseline_mae]\n",
    "    cnt = np.sum(cnt)\n",
    "    \n",
    "    allout = list(laquelle.items())\n",
    "    nout = [allout[i] for i in range(cnt)]\n",
    "    marquers_important = dict(nout)\n",
    "    \n",
    "    return marquers_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073adc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df, ynum):\n",
    "    \n",
    "    # Loop over each model to test\n",
    "    for wm in [1, 0]:\n",
    "        res_permod = []\n",
    "        \n",
    "        for fea in range(4):\n",
    "            print('fea : ', fea)\n",
    "            \n",
    "            # ----------------\n",
    "            # Order of which features to use in a model\n",
    "            if fea == 0:\n",
    "                # 1) All features\n",
    "                X_cols = list(np.arange(1, df_test2.shape[1]-1, 1))\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 1:\n",
    "                # 2) first 3 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(3)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 2:\n",
    "                # 3) first 2 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(2)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 3:\n",
    "                # 4) first feature from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[0][0]]\n",
    "                y_cols = [df_test2.shape[1]-1] \n",
    "            \n",
    "            if wm == 0:\n",
    "                # Sequential : Support Vector Machine (NuSVC)\n",
    "                m_name = 'NuSVC'\n",
    "                X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2)\n",
    "                \n",
    "                # Determine if classes are binary or multiclass:\n",
    "                class_len = len(np.unique(Y_train_1D))\n",
    "                if class_len <= 2:\n",
    "                    model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = binary_svm_NuSVC_batch(X_train, X_test, Y_train_1D, Y_test_1D, batch_size)\n",
    "                elif class_len > 2:\n",
    "                    model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = multiclass_svm_NuSVC_batch(X_train, X_test, Y_train_1D, Y_test_1D, batch_size)\n",
    "                \n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    from sklearn.inspection import permutation_importance\n",
    "                    r = permutation_importance(model, X_test, Y_test_1D, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "                    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "                    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 1:\n",
    "                # Sequential : LSTM - changes within a window of points\n",
    "                m_name = 'LSTM'\n",
    "                model, dict_out, X_test, Y_test = run_LSTM(df, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = [dict_out['delay_train'], dict_out['delay_test']]    \n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 2:\n",
    "                # Sequential : Transformer - changes between windows of points\n",
    "                m_name = 'Trans'\n",
    "                dict_out, X_test, Y_test = run_Transformer(df, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 3:\n",
    "                # Spatial (find global trends in the feature) : RandomForest - partitioned subspace\n",
    "                m_name = 'RF'\n",
    "                X_train, X_test, Y_train_1D, Y_test_1D = preprocess_df_2_XYtraintest(df, X_cols, y_cols)\n",
    "                \n",
    "                # Determine if classes are binary or multiclass:\n",
    "                class_len = len(np.unique(Y_train_1D))\n",
    "                if class_len <= 2:\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score = binary_RandomForest(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_binary_class(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "                    value_pack_test = evaluation_methods_binary_class(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "                elif class_len > 2:\n",
    "                    model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_bin_score, Y_test_bin_score = multiclass_RandomForest_1Dinput(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "                    value_pack_train = evaluation_methods_multi_class_1D(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_bin_score)\n",
    "                    value_pack_test = evaluation_methods_multi_class_1D(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_bin_score)\n",
    "                \n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    col_noms = ['%s' % (i) for i in range(X_test.shape[1])]\n",
    "                    marquers_important = pipeline_permutation_importance(model, X_test, Y_test_1D, col_noms)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 4:\n",
    "                # Spatial (find global trends in the feature) : CNN\n",
    "                m_name = 'CNN'\n",
    "                dict_out, X_test, Y_test, desired_col = run_CNN(df, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 5:\n",
    "                # Sequential & Spatial : LSTM-CNN\n",
    "                m_name = 'LSTM-CNN'\n",
    "                dict_out, X_test, Y_test = run_LSTM_CNN(df, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "\n",
    "            # ----------------\n",
    "            # Save all data to array \n",
    "            res_permod.append([ynum, m_name, fea, X_cols, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test, marquers_important, extra])\n",
    "            # ----------------\n",
    "            \n",
    "        # Save data matrices to file per model result :\n",
    "        file_name = \"res_exp_%s_%s_%s_ynum%d_%s.pkl\" % (exp, ax_val, ss_val, ynum, m_name)\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(res_permod, open_file)\n",
    "        open_file.close()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eb9e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mettez_dictout_dans_vars(dict_out, q):\n",
    "    if q == 0:\n",
    "        acc = dict_out['acc_train']\n",
    "        prec = dict_out['prec_train']\n",
    "        recall = dict_out['recall_train']\n",
    "        roc_auc = dict_out['roc_auc_train']\n",
    "    elif q == 1:\n",
    "        acc = dict_out['acc_test']\n",
    "        prec = dict_out['prec_test']\n",
    "        recall = dict_out['recall_test']\n",
    "        roc_auc = dict_out['roc_auc_test']\n",
    "    elif q == 2:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_pp_dircalc']\n",
    "    elif q == 3:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_pp_dircalc']\n",
    "            \n",
    "    return acc, prec, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45a56d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols):\n",
    "\n",
    "    # Ensure that the X matrix size is correct\n",
    "    # df_test2 : (dp_per_sample*n_values, feature)\n",
    "    all_dp, cols = df_test2.shape\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Il faut change df_test2 à : (batch, timesteps, feature)\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    tot = [endd[i]-st[i] for i in range(len(st))]\n",
    "    val = min(tot)\n",
    "\n",
    "    # Ensurez que X est le meme taille\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(tot)):\n",
    "        isamp = endd[i]-st[i]\n",
    "        diff = isamp - val\n",
    "        X.append(df_test2.iloc[st[i]:endd[i]-diff, X_cols].to_numpy())\n",
    "        Y.append(df_test2.iloc[st[i]:endd[i]-diff, y_cols].to_numpy())\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Tensorflow says to use :\n",
    "    batch, timesteps, feature = X.shape\n",
    "\n",
    "    print('batch:' , batch)\n",
    "    print('timesteps:' , timesteps)\n",
    "    print('feature:' , feature)\n",
    "    \n",
    "    print('taille de X:' , X.shape)\n",
    "    # X.shape =  (104570, 20, 1)   # batch, timesteps/sequence length, feature\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    batch, timesteps, n_outputs = Y.shape   # batch, timesteps, 1\n",
    "    print('taille de Y:' , Y.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Split the X an y data into test and train\n",
    "    seed = 0\n",
    "    test_size = 0.25 # default\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = seed, test_size = test_size)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    \n",
    "    print('X_train:' , X_train.shape)\n",
    "    print('Y_train:' , Y_train.shape)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    print('X_test:' , X_test.shape)\n",
    "    print('Y_test:' , Y_test.shape)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Ensure that Y_train and Y_test are integers\n",
    "    Y_test = Y_test.astype(int)\n",
    "    Y_train = Y_train.astype(int)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # batch_train, timesteps_train, feature_train = X_train.shape\n",
    "    # batch_test, timesteps_test, feature_test = X_test.shape\n",
    "    \n",
    "    # OU\n",
    "    \n",
    "    suf = ['train', 'test']\n",
    "    noms = ['batch_', 'timesteps_', 'feature_']\n",
    "    dictkeys = [j+i for i in suf for j in noms]\n",
    "    #print('dictkeys: ', dictkeys)\n",
    "\n",
    "    dictvals = []\n",
    "    dictvals.append(list(X_train.shape))\n",
    "    dictvals.append(list(X_test.shape))\n",
    "    dictvals = np.ravel(dictvals)\n",
    "    \n",
    "    info = dict(zip(dictkeys, dictvals))\n",
    "    info['n_outputs'] = n_outputs\n",
    "    # ----------------\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a262a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad7b757c",
   "metadata": {},
   "source": [
    "## CNN !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb414331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_CNN(df_test2, X_cols, y_cols, img_dim): \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    # X_train: (batch_train, timesteps_train, feature_train)\n",
    "    # Y_train: (batch_train, timesteps_train, n_outputs)\n",
    "    # X_test: (batch_test, timesteps_test, feature_train)\n",
    "    # Y_test: (batch_test, timesteps_test, n_outputs)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # combinez timesteps et feature dans un image \n",
    "    X_train1 = np.reshape(X_train, (info['batch_train'], info['timesteps_train']*info['feature_train']))\n",
    "    X_test1 = np.reshape(X_test, (info['batch_test'], info['timesteps_test']*info['feature_test']))\n",
    "    # X_train: (batch_train, timesteps_train*feature_train)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    X_train_img = mat2img(X_train1, img_dim, info['batch_train'])\n",
    "    X_test_img = mat2img(X_test1, img_dim, info['batch_test'])\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Y_train_1D =  [Y_train[i,0:1,0] for i in range(info['batch_train'])]\n",
    "    Y_test_1D =  [Y_test[i,0:1,0] for i in range(info['batch_test'])]\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # shape of X_train_img :  (batch_train, img_dim, ndes, 3)\n",
    "    # shape of Y_train_1D :  (batch_train,)\n",
    "    # shape of X_test_img :  (batch_test, img_dim, ndes, 3)\n",
    "    # shape of Y_test_1D :  (batch_test,)\n",
    "    \n",
    "    return X_train_img, X_test_img, Y_train_1D, Y_test_1D, info\n",
    "    \n",
    "    \n",
    "\n",
    "def mat2img(X, img_dim, batch):\n",
    "    X_img = []\n",
    "    for i in range(batch):\n",
    "        mat = X[i,:]\n",
    "        \n",
    "        if i == 0:\n",
    "            n = int(np.floor(np.sqrt(len(mat))))\n",
    "        \n",
    "        # fold into a square\n",
    "        mat = np.reshape(mat, (n, n))\n",
    "        \n",
    "        # resize et convertir matrix au image : []\n",
    "        img = Image.fromarray( mat , 'L')\n",
    "        rgb_image = img.convert('RGB')\n",
    "        \n",
    "        # Resize image into a 64, 64, 3\n",
    "        new_h, new_w = int(img_dim), int(img_dim)\n",
    "        img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "        w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "        \n",
    "        # Convert image to an array\n",
    "        image = np.array(img3)\n",
    "\n",
    "        # Normalize image\n",
    "        image = image/255\n",
    "        \n",
    "        X_img.append(image)\n",
    "        \n",
    "    \n",
    "    return X_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ba5cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Though for the 1D signal classification CNNs are also suitable, like they are implemented in the article \n",
    "# [49] for the seismic signal classification, while dealing with 2D objects CNNs can perform significantly \n",
    "# better results. Thus, firstly, we convert the 1D accelerometer signal into the 2D images via applying \n",
    "# CWT in order to extract signal features and, at the same time, to make it possible to implement 2D CNNs.'\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "def MPCNN_arch(n_outputs, img_dim, ynum):\n",
    "    \n",
    "    # Typical architecture MPCNN architecture using alternating convolutional and max-pooling layers. \n",
    "    \n",
    "    base_dimension = 32\n",
    "    \n",
    "    model = Sequential()  # initialize Sequential model\n",
    "    \n",
    "    # 32 channels, convolutional layer (kernel 5x5)\n",
    "    model.add(Conv2D(base_dimension, (5,5), strides=(1,1), padding='same', input_shape=(img_dim, img_dim, 1)))\n",
    "    \n",
    "    # Max pooling layer, pool size 2x2, strides 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    # 64 channels, convolutional layer (kernel 5x5)\n",
    "    model.add(Conv2D(base_dimension * 2, (5,5), strides=(1,1), padding='same'))\n",
    "\n",
    "    # Max pooling layer, pool size 2x2, strides 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    \n",
    "    # model.add(BatchNormalization(renorm=True))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a18551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "def dcgan_arch(n_outputs, img_dim, ynum):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_dim * 4, activation=LeakyReLU(alpha=0.2), input_shape=img_dim*img_dim))\n",
    "    model.add(Dense(hidden_dim * 2, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dense(hidden_dim, activation=LeakyReLU(alpha=0.2)))\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a65265f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderdecoder_arch(n_outputs, img_dim, ynum):\n",
    "    \n",
    "    mt = 0.9 # 0.15  défaut=0.99\n",
    "    LyR_alpha = 0.2  # défaut=0.3\n",
    "    \n",
    "    base_dimension = 64          \n",
    "    \n",
    "    model = Sequential()\n",
    "    # 1ère valeur (filters) : le nombre de tranches \"(kernel_val,kernel_val)\" qui composent l'image de sortie\n",
    "    # 2eme valeur (kernel_size) : la taille de la carre/filtre que on glisse au dessous l'image \n",
    "    # 3eme valeur (stride): Le plus grande le stride valeur le plus petite l'image sortie : on prends z_dim/stride_num\n",
    "    \n",
    "    # --------\n",
    "    # Entrée = (img_dim, img_dim, 1)\n",
    "    model.add(Conv2D(base_dimension, (5,5), strides=(2,2), padding='same', input_shape=(img_dim, img_dim, 1)))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "    # taille_sortie = (28 + 2*p - 5)/2 + 1\n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    # Entrée = \n",
    "    model.add(Conv2D(base_dimension * 2, (5,5), strides=(2,2), padding='same'))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    model.add(Flatten())\n",
    "\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 4096)\n",
    "    # --------\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d811903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def mat2D_resize_mat2D_CNN(mat, img_dim):\n",
    "    \n",
    "    img = Image.fromarray( mat , 'L')\n",
    "    \n",
    "    # We preprocess your image to fit your algorithm.\n",
    "    # img = Image.open(my_image)         # PIL: img is not in array form, it is a PIL.PngImagePlugin.PngImageFile \n",
    "    rgb_image = img.convert('RGB')\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(img_dim), int(img_dim)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Convert image to an array\n",
    "    image = np.array(img3)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Transformer l'image de 3D à 2D\n",
    "    # Convert image back to a 2D array\n",
    "    mat_resized = np.mean(image, axis=2)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Normalizer l'image entre 0 et 1\n",
    "    mat_resized = mat_resized/255\n",
    "    \n",
    "    # OU\n",
    "    \n",
    "    # Normalize the images to [-1, 1]\n",
    "    # mat_resized = (mat_resized - 127.5) / 127.5\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    useit = 0\n",
    "    if useit == 1:\n",
    "        # Threshold image\n",
    "        thresh = 255/2\n",
    "        row, col = mat_resized.shape\n",
    "        mat_resized_th = np.ones((row, col))\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if mat_resized[i,j] > thresh:\n",
    "                    mat_resized_th[i,j] = 0\n",
    "        mat_resized = mat_resized_th\n",
    "    \n",
    "    return mat_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()  # This allows you to use placeholder in version 2.0 or higher\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def run_CNN(df_test2, X_cols, y_cols, ynum):\n",
    "\n",
    "    # ----------------\n",
    "    img_dim = 64\n",
    "    \n",
    "    # Folding data into CNN image format:\n",
    "    X_train_img, X_test_img, Y_train_1D, Y_test_1D, info  = initialize_CNN(df_test2, X_cols, y_cols, img_dim)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # X_train et X_test pour dcgan\n",
    "    for i in range()\n",
    "    mat2D_resize_mat2D_CNN(mat, img_dim)\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            model = MPCNN_arch(n_outputs, img_dim, ynum)\n",
    "        elif i == 1:\n",
    "            model = dcgan_arch(n_outputs, input_shape, ynum)\n",
    "        elif i == 2:\n",
    "            model = encoderdecoder_arch(n_outputs, img_dim, ynum)\n",
    "    \n",
    "        patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "        \n",
    "        if i == 1:\n",
    "            \n",
    "            # Reshape the training and test examples \n",
    "            train_x_flatten = X_train.reshape(X_train.shape[0], -1)   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "            test_x_flatten = X_test.reshape(X_test.shape[0], -1)\n",
    "            history = model.fit(X_train_, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        else:\n",
    "            history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "    \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['mod_train'] = mod_type[a]\n",
    "    dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    cnn2D_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return cnn2D_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01675a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cb3ed",
   "metadata": {},
   "source": [
    "## LSTM !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47a7e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_train = X_Y_a_tfobj(X_train, Y_train, info['batch_train'], return_sequences, stateless)\n",
    "    tf_test = X_Y_a_tfobj(X_test, Y_test, info['batch_test'], return_sequences, stateless)\n",
    "    \n",
    "    # side idea : if I give tf_test info['batch_train'] instead of info['batch_test'], could I run stateful=True\n",
    "    # for all data (train and test)?\n",
    "    # ----------------\n",
    "\n",
    "    return tf_train, tf_test, X_test, Y_test, info\n",
    "\n",
    "\n",
    "\n",
    "def X_Y_a_tfobj(X, Y, batch_size, return_sequences, stateless):\n",
    "    \n",
    "    if return_sequences == False:  # one output per batch of samples\n",
    "        # Y shape : batch_size,      ie: Y.shape =  (104570,)\n",
    "        temp = [Y[i,0:1,:] for i in range(Y.shape[0])]\n",
    "        Y = np.array(temp)\n",
    "        Y = np.reshape(Y, (batch_size,))\n",
    "    elif return_sequences == True: # an output per each batch of samples\n",
    "        # Y shape : batch_size, timesteps, n_output\n",
    "        Y = np.array(Y)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_data = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    \n",
    "    if stateless == True:  # sequence is non-causal\n",
    "        buffer_size = 1000\n",
    "        tf_data = tf_data.cache().shuffle(buffer_size).batch(batch_size)\n",
    "    elif stateless == False:  # sequence is causal\n",
    "        tf_data = tf_data.batch(batch_size)\n",
    "    \n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ba908ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "def run_LSTM(df_test2, X_cols, y_cols, ynum):\n",
    "\n",
    "    \n",
    "    return_sequences = False # True=return a prediction at every batch sample, (default) False=return one prediction at the end of the batch\n",
    "    stateless = True # True=shuffle the batch samples/slices --samples are non-causal, False=do not shuffle slices---samples are causal\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Folding data into LSTM format:\n",
    "    tf_train, tf_test, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # -------------------------------\n",
    "\n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    return_state = False # True=return a and c, (default) False=do not return hidden state (a) and cell state (c)\n",
    "    stateful = False #  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    liste_de_vals = [20, 40, 60, 70, 80, 100] # number of dimensions for the hidden state of each LSTM cell\n",
    "    \n",
    "    for n_a in liste_de_vals:\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        if stateful == True:\n",
    "            # Quand vous definez batch_input_shape, il faut que des entries de modele être le meme taille que le train dataset\n",
    "            model.add(LSTM(n_a, input_shape=(timesteps_train, feature), batch_input_shape=(batch, timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "        elif stateful == False:\n",
    "            model.add(LSTM(n_a, input_shape=(timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "\n",
    "        # Types of W initializer :\n",
    "        initializer = tf.keras.initializers.HeUniform()\n",
    "        \n",
    "        if ynum == 2:\n",
    "            model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "        else:\n",
    "            model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "        # Compile the model for training\n",
    "        # opt = keras.optimizers.Adam()\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "        # opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "        # Si vous utilisez softmax activation, la taille de sortie est plus grand que deux donc il faut categorical_crossentropy\n",
    "        if ynum == 2:\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "        else:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "\n",
    "        # -------------------------------\n",
    "        \n",
    "        # 50 est trop, 20 est insuffisant \n",
    "        patience = 1 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "\n",
    "        if stateful == False:\n",
    "            history = model.fit(tf_train, epochs=epochs, validation_data=tf_test, batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "            # OU\n",
    "            # history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "            # score = model.evaluate(tf_test, verbose=2)\n",
    "            # test_acc.append(score[1])\n",
    "            # test_loss.append(score[0])\n",
    "        elif stateful == True:\n",
    "            # On peut faire tf_train parce que le taille est fixer at batch_input_shape\n",
    "            history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "    \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "        \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['delay_train'] = liste_de_vals[a]\n",
    "    dict_out['delay_test'] = liste_de_vals[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    lstm_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return lstm_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df83623",
   "metadata": {},
   "source": [
    "# Batch Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2cd5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty(vec):\n",
    "    \n",
    "    #if not any(vec):\n",
    "    \n",
    "    # OR\n",
    "    \n",
    "    #if len(vec) < 1:  # OR\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    if vec.shape[0] == 0:\n",
    "        # print('yes, the array is empty')\n",
    "        out = True\n",
    "    else:\n",
    "        # print('no, the array is not empty')\n",
    "        out = False\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c3a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_properlist(vec):\n",
    "    \n",
    "    out = []\n",
    "    for i in range(len(vec)):\n",
    "        out = out + [np.ravel(vec[i])]\n",
    "        \n",
    "    if is_empty(out) == False:\n",
    "        vecout = np.concatenate(out).ravel().tolist()\n",
    "    else:\n",
    "        vecout = list(np.ravel(out))\n",
    "    \n",
    "    return vecout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86de48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_script(exp, ax_val, ss_val):\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    df = get_df(ax_val, ss_val)\n",
    "\n",
    "    feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "    # Elasped time for feature processing :  715.9615476131439\n",
    "\n",
    "    del df\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    if exists(\"df_feat.pkl\") == False:\n",
    "        print('Creation des marquers')\n",
    "        df_feat = feature_creation_preprocessing(feat0, t_feat0)\n",
    "        del feat0, t_feat0\n",
    "        save_dat_pickle(df_feat, file_name=\"df_feat.pkl\")\n",
    "    else:\n",
    "        print('Load des marquers')\n",
    "        df_feat = load_dat_pickle(file_name=\"df_feat.pkl\")\n",
    "        \n",
    "    df_org = df_feat\n",
    "    \n",
    "    del df_feat\n",
    "    # ----------------\n",
    "\n",
    "    col_list = ['num', 'pos', 'vel', 'acc', 'pos_nc', 'vel_nc', 'acc_nc', 'pos_sl0', 'pos_sl1', 'pos_sl2', 'pos_sl3', 'pos_sl4', 'vel_sl0', 'vel_sl1', 'vel_sl2', 'vel_sl3', 'vel_sl4', 'acc_sl0', 'acc_sl1', 'acc_sl2', 'acc_sl3', 'acc_sl4', 'pos_spec', 'vel_spec', 'acc_spec', 'pos_cwt', 'vel_cwt', 'acc_cwt']\n",
    "    df_org = pandas_rename_columns(df_org, col_list)\n",
    "    df_org = df_org.reset_index(drop=True)  # reset index : delete the old index column\n",
    "    # Gardez df_org\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    y_alllabel = [y1_feat0, y2_feat0, y3_feat0]\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Loop over the different y labels\n",
    "    for ynum in range(1):    #range(len(y_alllabel)):\n",
    "        print('ynum : ', ynum)\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Select y\n",
    "        y_label = y_alllabel[ynum]\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # Ajoutez le column de label à la fin\n",
    "        y_pd = pd.Series(make_a_properlist(y_label))\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        df_feat = pd.concat([df_org, y_pd], axis=1)\n",
    "        df_feat = df_feat.rename({0: 'y'}, axis=1)\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        # Balancez des class/labels: pad\n",
    "        # Pad data 2 Make Classes Equivalent\n",
    "        df_test2 = pad_data_2makeclasses_equivalent(df_feat)\n",
    "        del df_feat\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # classification for 3 partitions of feature data per 8 classifiers\n",
    "        # classify(df_test2, ynum)\n",
    "        \n",
    "        # del df_test2\n",
    "\n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae227ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmation : exp= rot , ax_val= all , ss_val= all\n",
      "Load des marquers\n",
      "ynum :  0\n",
      "Are there nan valeus in the data :  False\n",
      "Before sorting counted_value :  [294, 96]\n",
      "Before sorting count_index :  [0, 1]\n",
      "After sorting counted_value :  [294, 96]\n",
      "After sorting count_index :  [0, 1]\n",
      "needed_samps_class :  [  0 198]\n",
      "shape of dataframe before padding :  (183690, 29)\n",
      "shape of dataframe to add to original dataframe:  (93060, 29)\n",
      "shape of padded dataframe (original + toadd) :  (276750, 29)\n",
      "Before sorting counted_value :  [294, 294]\n",
      "Before sorting count_index :  [1, 0]\n",
      "After sorting counted_value :  [294, 294]\n",
      "After sorting count_index :  [0, 1]\n",
      "needed_samps_class :  [0 0]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "df_test2 = run_main_script(exp, ax_val, ss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56ba82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c737c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = load_dat_pickle(file_name=\"res_exp_rot_all_all_ynum0_LSTM_fea3.pkl\")\n",
    "dff = pd.DataFrame(out)\n",
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69d315",
   "metadata": {},
   "source": [
    "# Individual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d03d85b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sorting counted_value :  [294, 294]\n",
      "Before sorting count_index :  [1, 0]\n",
      "After sorting counted_value :  [294, 294]\n",
      "After sorting count_index :  [0, 1]\n",
      "needed_samps_class :  [0 0]\n",
      "batch: 588\n",
      "timesteps: 469\n",
      "feature: 27\n",
      "taille de X: (588, 469, 27)\n",
      "taille de Y: (588, 469, 1)\n",
      "X_train: (441, 469, 27)\n",
      "Y_train: (441, 469, 1)\n",
      "X_test: (147, 469, 27)\n",
      "Y_test: (147, 469, 1)\n",
      "class_len:  2\n"
     ]
    }
   ],
   "source": [
    "fea = 0\n",
    "\n",
    "if fea == 0:\n",
    "    # 1) All features\n",
    "    X_cols = list(np.arange(1, df_test2.shape[1]-1, 1))\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 1:\n",
    "    # 2) first 3 from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[oo][0] for oo in range(3)]\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 2:\n",
    "    # 3) first 2 from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[oo][0] for oo in range(2)]\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 3:\n",
    "    # 4) first feature from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[0][0]]\n",
    "    y_cols = [df_test2.shape[1]-1] \n",
    "\n",
    "\n",
    "m_name = 'NuSVC'\n",
    "X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2)\n",
    "\n",
    "# Determine if classes are binary or multiclass:\n",
    "class_len = len(np.unique(Y_train))\n",
    "print('class_len: ', class_len)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = binary_svm_NuSVC_batch(X_train, X_test, Y_train_1D, Y_test_1D, batch_size)\n",
    "extra = np.nan\n",
    "# ----------------\n",
    "# Permutation importance of features : probe which features are most predictive\n",
    "if fea == 0:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    r = permutation_importance(model, X_test, Y_test_1D, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52783366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_methods_binary_class(model, X, Y_1D, Y_1D_predict, Y_bin_pp, Y_1D_score):\n",
    "    \n",
    "    # ----------------------------\n",
    "\n",
    "    # 4) Direct calculation of metrics\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "    # labels = array-like, default=None\n",
    "\n",
    "    # pos_label = str or int, default=1 The class to report if average='binary' and the data is binary. \n",
    "    # If the data are multiclass or multilabel, this will be ignored\n",
    "\n",
    "    # average = ['binary', 'micro', 'macro', 'weighted', 'samples', None]\n",
    "    # This parameter is required for multiclass/multilabel targets. \n",
    "    # None : the scores for each class are returned\n",
    "    # 'binary'  : Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "    # 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    # 'macro' : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "    # 'weighted' : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "    # 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "    average = 'micro'\n",
    "\n",
    "    # sample_weight : array-like of shape (n_samples,), default=None\n",
    "    acc_dircalc = metrics.accuracy_score(Y_1D, Y_1D_predict)\n",
    "    prec_dircalc = metrics.precision_score(Y_1D, Y_1D_predict, average=average)\n",
    "    recall_dircalc = metrics.recall_score(Y_1D, Y_1D_predict, average=average)\n",
    "    f1_dircalc = metrics.f1_score(Y_1D, Y_1D_predict, average=average)\n",
    "    \n",
    "    # Y_bin_pp is size [n_samples, n_classes=2]\n",
    "    # Take the column of Y_bin_pp for the class of Y_1D, because both vectors need to be [n_samples, 1]\n",
    "    Y_1D_pp = []\n",
    "    for q in range(len(Y_1D)):\n",
    "        desrow = Y_bin_pp[q]\n",
    "        Y_1D_pp.append(desrow[int(Y_1D[q])])\n",
    "    Y_1D_pp = np.ravel(Y_1D_pp)\n",
    "    \n",
    "    Y_1D_pp = np.array(Y_1D_pp)\n",
    "    Y_1D = np.array(Y_1D)\n",
    "    \n",
    "    rocauc_pp_dircalc = metrics.roc_auc_score(Y_1D, Y_1D_pp, average=average) # prediction probability\n",
    "    \n",
    "    # ----------------------------\n",
    "    \n",
    "    value_pack = {}\n",
    "    \n",
    "    var_list = ['acc_dircalc', 'prec_dircalc', 'recall_dircalc', 'f1_dircalc', 'rocauc_pp_dircalc']\n",
    "    var_list_num = [acc_dircalc, prec_dircalc, recall_dircalc, f1_dircalc, rocauc_pp_dircalc]\n",
    "    \n",
    "    for q in range(len(var_list)):\n",
    "        value_pack['%s' % (var_list[q])] = var_list_num[q]\n",
    "    \n",
    "    return value_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6076822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "def binary_svm_NuSVC_batch(X_train, X_test, Y_train_1D, Y_test_1D, batch_size):\n",
    "\n",
    "    n = int(np.ceil(info['batch_train']/batch_size))\n",
    "    print('n: ', n)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(1):  # range(n)\n",
    "        st = i*batch_size\n",
    "        endd = st+batch_size\n",
    "\n",
    "        # Stack all data over batches\n",
    "        X_train_batch = np.reshape(X_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['feature_train']))\n",
    "        Y_train_1D_batch = np.reshape(Y_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['n_outputs']))\n",
    "        X_test_batch = np.reshape(X_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['feature_test']))\n",
    "        Y_test_1D_batch = np.reshape(Y_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['n_outputs']))\n",
    "\n",
    "        print('shape of X_train_batch : ', X_train_batch.shape)\n",
    "        print('shape of Y_train_1D_batch : ', Y_train_1D_batch.shape)\n",
    "        print('shape of X_test_batch : ', X_test_batch.shape)\n",
    "        print('shape of Y_test_1D_batch : ', Y_test_1D_batch.shape)\n",
    "        \n",
    "        model = svm.NuSVC(decision_function_shape='ovo', probability=True, max_iter=-1)  # “one-versus-one” : binary ONLY, Y_train_1D, same implementation as libsvm (uses 1/lambda instead of C in cost function)\n",
    "        model.fit(X_train_batch, Y_train_1D_batch)\n",
    "        \n",
    "        # OU\n",
    "        \n",
    "        # from sklearn.pipeline import make_pipeline\n",
    "        # from sklearn.preprocessing import StandardScaler\n",
    "        # from sklearn.svm import NuSVC\n",
    "        # model = make_pipeline(StandardScaler(), NuSVC())\n",
    "        # model.fit(X_train_batch, Y_train_1D_batch)\n",
    "        \n",
    "        # ----------------------------\n",
    "\n",
    "        Y_train_1D_predict = model.predict(X_train_batch)\n",
    "        Y_test_1D_predict = model.predict(X_test_batch)\n",
    "\n",
    "        # The prediction probability of each class : is size [n_samples, n_classes]\n",
    "        Y_train_bin_pp = model.predict_proba(X_train_batch) \n",
    "        Y_test_bin_pp = model.predict_proba(X_test_batch)\n",
    "\n",
    "        Y_train_bin_pp = np.array(Y_train_bin_pp)\n",
    "        print('shape of Y_train_bin_pp : ', Y_train_bin_pp.shape)\n",
    "        Y_test_bin_pp = np.array(Y_test_bin_pp)\n",
    "        print('shape of Y_test_bin_pp : ', Y_test_bin_pp.shape)\n",
    "\n",
    "        # How confidently each value predicted for x_test by the classifier is Positive ( large-magnitude Positive value ) or Negative ( large-magnitude Negative value)\n",
    "        Y_train_1D_score = model.decision_function(X_train_batch)  # size is [n_samples, 1]\n",
    "        Y_test_1D_score = model.decision_function(X_test_batch)\n",
    "\n",
    "        Y_train_1D_score = np.array(Y_train_1D_score)\n",
    "        print('shape of Y_train_1D_score : ', Y_train_1D_score.shape)\n",
    "        Y_test_1D_score = np.array(Y_test_1D_score)\n",
    "        print('shape of Y_test_1D_score : ', Y_test_1D_score.shape)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        value_pack_train = evaluation_methods_binary_class(model, X_train_batch, Y_train_1D_batch, Y_train_1D_predict, Y_train_bin_pp, Y_train_1D_score)\n",
    "        value_pack_test = evaluation_methods_binary_class(model, X_test_batch, Y_test_1D_batch, Y_test_1D_predict, Y_test_bin_pp, Y_test_1D_score)\n",
    "\n",
    "        # ----------------------------\n",
    "        \n",
    "        acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "        acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3) \n",
    "        \n",
    "        results.append(i, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test)\n",
    "        \n",
    "        # Save model to file\n",
    "        file_name = \"model_%s.pkl\" % (i)\n",
    "        save_dat_pickle(model, file_name=file_name)\n",
    "        \n",
    "        # Delete model\n",
    "        del model\n",
    "        del Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_1D_score, Y_test_1D_score\n",
    "        \n",
    "    \n",
    "    # Evaluatez quel modeles est mieux\n",
    "    best = np.argmax(results[:,6])\n",
    "    acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = map(float, results[:,6])\n",
    "    \n",
    "    # Load best model only for permutation importance\n",
    "    file_name = \"model_%s.pkl\" % (best)\n",
    "    model = load_dat_pickle(file_name=file_name)\n",
    "    \n",
    "    # Delete all .pkl files\n",
    "    del results\n",
    "    rm_list = [i for i in range(n) if i != best]\n",
    "    for i in rm_list:\n",
    "        os.remove(\"model_%s.pkl\" % (i))\n",
    "    \n",
    "    return model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f71202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f6366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72b13416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6717c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d28431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
