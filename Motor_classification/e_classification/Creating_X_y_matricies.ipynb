{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c481e0d3",
   "metadata": {},
   "source": [
    "# Mouvements de joystick analyses : Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76484add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manipulation\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Signal processing\n",
    "from scipy.io import wavfile\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from scipy.signal import periodogram\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# MFCC\n",
    "import librosa, librosa.display\n",
    "\n",
    "# Wavelets\n",
    "import pywt\n",
    "from pywt import wavedec\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Scaling signal\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# Loading and saving data\n",
    "import pickle\n",
    "\n",
    "PCtype = 'Linux'\n",
    "\n",
    "varr = {}\n",
    "if PCtype == 'Windows':\n",
    "    # Windows\n",
    "    # varr['main_path'] = \"C:\\\\Users\\\\jamilah\\\\Documents\\\\Github_analysis_PROJECTS\\\\Time_series_analysis\\\\Motor_classification\\\\Motor_classification\"  \n",
    "    # varr['main_path1'] = \"%s\\\\a_data_standardization\" % (varr['main_path'])\n",
    "    # varr['main_path2'] = \"%s\\\\b_data_preprocessing\" % (varr['main_path'])\n",
    "    # varr['main_path3'] = \"%s\\\\c_calculate_metrics\" % (varr['main_path'])\n",
    "    filemarker = '\\\\'\n",
    "elif PCtype == 'Linux':\n",
    "    # Linux\n",
    "    # varr['main_path'] = \"/home/oem2/Documents/PROGRAMMING/9_Motor_classification_2018-22/Coding_version3_python_FINAL\"\n",
    "    # varr['main_path'] = \"/home/oem2/Documents/GITarea/Motor_classification\"\n",
    "    # varr['main_path1'] = \"%s/a_data_standardization\" % (varr['main_path'])\n",
    "    # varr['main_path2'] = \"%s/b_data_preprocessing\" % (varr['main_path'])\n",
    "    # varr['main_path3'] = \"%s/c_calculate_metrics\" % (varr['main_path'])\n",
    "    filemarker = '/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b7450",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8404bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_timeseries_trialdata_into_pandas(varr, filemarker):\n",
    "    \n",
    "    df_timeseries_exp = {}\n",
    "\n",
    "    # 1) Load exp : put the experiment that you wish to run\n",
    "    for exp in range(2):  # 0=rotation, 1=translation\n",
    "        # print('exp : ', exp)\n",
    "\n",
    "        if exp == 0:\n",
    "            # Rotational data - 18 participants\n",
    "            varr['which_exp'] = 'rot'\n",
    "            varr['anom'] = 'RO', 'PI', 'YA'\n",
    "            \n",
    "            # Time series data per subject per trial\n",
    "            file_name1 = \"%s%srotdat.pkl\" % (varr['main_path2'], filemarker)\n",
    "\n",
    "            # Load data experimental preprocessed data matrix\n",
    "            file_name2 = \"%s%srot_Xexp.pkl\" % (varr['main_path3'], filemarker)\n",
    "        elif exp == 1:\n",
    "            # Translational data - 14 participants\n",
    "            varr['which_exp'] = 'trans'\n",
    "            varr['anom'] = 'LR', 'FB', 'UD'\n",
    "\n",
    "            # Time series data per subject per trial\n",
    "            file_name1 = \"%s%stransdat.pkl\" % (varr['main_path2'], filemarker)\n",
    "\n",
    "            # Experimental preprocessed : a scalar metric per subject per trial\n",
    "            file_name2 = \"%s%strans_Xexp.pkl\" % (varr['main_path3'], filemarker)\n",
    "\n",
    "        open_file = open(file_name1, \"rb\")\n",
    "        dat = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        open_file = open(file_name2, \"rb\")\n",
    "        X = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "        \n",
    "        num_of_subs = len(X)    # OR dat.shape[0]\n",
    "\n",
    "        Xsub = []\n",
    "        \n",
    "        for s in range(num_of_subs):\n",
    "            # print('s : ', s)\n",
    "            \n",
    "            num_of_tr = len(dat[s][0])  # OR X[0].shape[0]\n",
    "\n",
    "            for tr in range(num_of_tr):\n",
    "                # print('tr : ', tr)\n",
    "                \n",
    "                # time series dataFrame (ensure vectors are a column): \n",
    "                num_dp = len(dat[s][4][tr][:,0])\n",
    "\n",
    "                subject = s*np.ones((num_dp,1))\n",
    "                trial = tr*np.ones((num_dp,1))\n",
    "                ss = dat[s][0][tr]*np.ones((num_dp,1))\n",
    "                ax = dat[s][1][tr]*np.ones((num_dp,1))\n",
    "\n",
    "                dp = np.reshape(list(range(num_dp)), (num_dp,1))\n",
    "                time = np.reshape(dat[s][8][tr], (num_dp,1))\n",
    "\n",
    "                res_type = X[s][:,5][tr]*np.ones((num_dp,1))\n",
    "                \n",
    "                outSIGCOM_ax0 = np.reshape(dat[s][4][tr][:,0], (num_dp,1))\n",
    "                outSIGCOM_ax1 = np.reshape(dat[s][4][tr][:,1], (num_dp,1))\n",
    "                outSIGCOM_ax2 = np.reshape(dat[s][4][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outSIG_ax0 = np.reshape(dat[s][5][tr][:,0], (num_dp,1))\n",
    "                outSIG_ax1 = np.reshape(dat[s][5][tr][:,1], (num_dp,1))\n",
    "                outSIG_ax2 = np.reshape(dat[s][5][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outJOY_ax0 = np.reshape(dat[s][6][tr][:,0], (num_dp,1))\n",
    "                outJOY_ax1 = np.reshape(dat[s][6][tr][:,1], (num_dp,1))\n",
    "                outJOY_ax2 = np.reshape(dat[s][6][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outNOISE_ax0 = np.reshape(dat[s][7][tr][:,0], (num_dp,1))\n",
    "                outNOISE_ax1 = np.reshape(dat[s][7][tr][:,1], (num_dp,1))\n",
    "                outNOISE_ax2 = np.reshape(dat[s][7][tr][:,2], (num_dp,1))\n",
    "                \n",
    "                X_row = np.ravel(subject), np.ravel(trial), np.ravel(ss), np.ravel(ax), np.ravel(dp), np.ravel(time), np.ravel(res_type), np.ravel(outSIGCOM_ax0), np.ravel(outSIGCOM_ax1), np.ravel(outSIGCOM_ax2), np.ravel(outSIG_ax0), np.ravel(outSIG_ax1), np.ravel(outSIG_ax2), np.ravel(outJOY_ax0), np.ravel(outJOY_ax1), np.ravel(outJOY_ax2), np.ravel(outNOISE_ax0), np.ravel(outNOISE_ax1), np.ravel(outNOISE_ax2),\n",
    "                Xtr = np.transpose(X_row)\n",
    "                # print('shape of Xtr : ', Xtr.shape)\n",
    "                \n",
    "                # concatenate accumulated matrix with new\n",
    "                if s == 0 and tr == 0:\n",
    "                    Xsub = Xtr\n",
    "                else:\n",
    "                    Xsub = np.concatenate((Xsub, Xtr), axis=0)\n",
    "                \n",
    "                # print('len of Xsub : ', len(Xsub))\n",
    "                \n",
    "        \n",
    "        columns = ['subject', 'tr', 'ss', 'ax', 'dp', 'time', 'res_type', 'SIGCOM_ax0', 'SIGCOM_ax1', 'SIGCOM_ax2', 'SIG_ax0', 'SIG_ax1', 'SIG_ax2', 'JOY_ax0', 'JOY_ax1', 'JOY_ax2', 'NOISE_ax0', 'NOISE_ax1', 'NOISE_ax2']\n",
    "        out1 = np.reshape(Xsub, (len(Xsub), len(columns)))\n",
    "        \n",
    "        df = pd.DataFrame(out1, columns=columns)\n",
    "\n",
    "        df_timeseries_exp[varr['which_exp']] = df\n",
    "\n",
    "    return df_timeseries_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab92ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nan can not process strings,  use this.\n",
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ac3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dropna_python(df):\n",
    "    # Python\n",
    "    col_names = list(df.columns.values)\n",
    "    # OR\n",
    "    # col_names = list(df.columns)\n",
    "    \n",
    "    df = df.to_numpy()\n",
    "    df = np.array(df)\n",
    "    # print('size of df : ', df.shape)\n",
    "    data = []\n",
    "    num_of_cols = df.shape[1]\n",
    "    for i in range(df.shape[0]):\n",
    "        row_vec = df[i,:]\n",
    "        \n",
    "        out = [isnan(row_vec[i]) for i in range(len(row_vec))]\n",
    "        # OR\n",
    "        # out = []\n",
    "        # for i in range(len(row_vec)):\n",
    "            # #print('row_vec[i]', row_vec[i])\n",
    "            # out.append(isnan(row_vec[i]))\n",
    "        \n",
    "        out = make_a_properlist(out)  # for dataframes with nested arrays\n",
    "        \n",
    "        if any(out) == False:\n",
    "            data.append(df[i,:])\n",
    "    \n",
    "    num_of_rows = len(data)\n",
    "    data0 = np.reshape(data, (num_of_rows, num_of_cols))\n",
    "    \n",
    "    df_new = pd.DataFrame(data=data0, columns=col_names)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f44b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_properlist(vec):\n",
    "    \n",
    "    out = []\n",
    "    for i in range(len(vec)):\n",
    "        out = out + [np.ravel(vec[i])]\n",
    "        \n",
    "    if is_empty(out) == False:\n",
    "        vecout = np.concatenate(out).ravel().tolist()\n",
    "    else:\n",
    "        vecout = list(np.ravel(out))\n",
    "    \n",
    "    return vecout\n",
    "\n",
    "def is_empty(vec):\n",
    "    vec = np.array(vec)\n",
    "    if vec.shape[0] == 0:\n",
    "        out = True\n",
    "    else:\n",
    "        out = False\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "995af972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numderiv(x, t):\n",
    "    # Created by Jamilah Foucher, Février 01, 2021\n",
    "\n",
    "    # Purpose: Numerical derivative\n",
    "    # \n",
    "    # Input VARIABLES:\n",
    "    # (1) x is a vector in which you want the numerical derivative\n",
    "    # \n",
    "    # (2) t is a time vector\n",
    "\n",
    "    # Output VARIABLES:\n",
    "    # (1) dx is a vector of the numerical derivative\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    if len(x) > 4:\n",
    "        dx0 = np.ones(len(x))\n",
    "        for i in range(len(x)-1):\n",
    "            dx0[i] = ( x[i+1] - x[i] ) / ( t[i+1] - t[i] )\n",
    "        dx = dx0[0:len(dx0)-2], dx0[len(dx0)-2], dx0[len(dx0)-2] \n",
    "    dx = make_a_properlist(dx)\n",
    "       \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dce43bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(d, reverse = False):\n",
    "    return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f74d7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation_of_kstest(statistic, pvalue):\n",
    "     # or np.isclose(pvalue, 1, atol=0.1)\n",
    "    if np.isclose(statistic, 0, atol=0.01):    # default atol=1e-08\n",
    "        out = 1 # result is parametric/normal\n",
    "    else:\n",
    "        out = 0  # result is non-parametric/non-normal\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b93cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution_feature_data(feat, plotORnot):\n",
    "    # Make distribution of feature parametric per trial\n",
    "    \n",
    "    # Check for constant vectors\n",
    "    baseline_f = feat - feat[0]\n",
    "    if int(np.mean(baseline_f)) == 0:\n",
    "        return feat\n",
    "    else:\n",
    "        # Check if the feature has a normal distribution\n",
    "\n",
    "        statistic, pvalue = stats.kstest(feat, 'norm')\n",
    "        # print('statistic : ', statistic, ', pvalue : ', pvalue)\n",
    "        result = interpretation_of_kstest(statistic, pvalue)\n",
    "        # print('result : ', result)\n",
    "\n",
    "        # Initialize : use feature if it has normal distribution OR if normal distribution can not be found\n",
    "        norm_feat = feat\n",
    "\n",
    "        if result == 0 and np.isnan(feat).any() == False:\n",
    "            # Does not work for negative values, so shift up a little bit above zero\n",
    "            pos_shift = feat - (np.min(feat)-0.000001)\n",
    "            # print('min val : ', np.min(pos_shift))\n",
    "\n",
    "            # ----------------\n",
    "\n",
    "            normaldist0 = stats.boxcox(pos_shift)\n",
    "            # ----------------\n",
    "            # OR\n",
    "            # ----------------\n",
    "            # Stack two signals because the functions can not process a single signal alone\n",
    "            X = pos_shift, pos_shift\n",
    "            X = np.transpose(X)\n",
    "            # print('shape of X : ', X.shape)\n",
    "            # ----------------\n",
    "\n",
    "            # https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html\n",
    "\n",
    "            # ----------------\n",
    "            bc = PowerTransformer(method='box-cox')\n",
    "            normaldist1 = bc.fit(X).transform(X)\n",
    "            # ----------------\n",
    "\n",
    "            # ----------------\n",
    "            yj = PowerTransformer(method='yeo-johnson')\n",
    "            normaldist2 = yj.fit(X).transform(X)\n",
    "            # ----------------\n",
    "\n",
    "            # ----------------\n",
    "\n",
    "            rng = np.random.RandomState(0)\n",
    "            num_of_samps = len(feat)\n",
    "            qt = QuantileTransformer(n_quantiles=num_of_samps, output_distribution='normal', random_state=rng)\n",
    "            normaldist3 = qt.fit(X).transform(X)\n",
    "            # ----------------\n",
    "\n",
    "            # Some of the distribution transformations do not always work.  Test to see if the test suceeded.\n",
    "            statistic, pvalue = stats.kstest(normaldist0[0], 'norm')\n",
    "            result0 = interpretation_of_kstest(statistic, pvalue)\n",
    "\n",
    "            statistic, pvalue = stats.kstest(normaldist1[:,0], 'norm')\n",
    "            result1 = interpretation_of_kstest(statistic, pvalue)\n",
    "\n",
    "            statistic, pvalue = stats.kstest(normaldist2[:,0], 'norm')\n",
    "            result2 = interpretation_of_kstest(statistic, pvalue)\n",
    "\n",
    "            statistic, pvalue = stats.kstest(normaldist3[:,0], 'norm')\n",
    "            result3 = interpretation_of_kstest(statistic, pvalue)\n",
    "\n",
    "            allres = [result0, result1, result2, result3]\n",
    "            # print('allres : ', allres)\n",
    "\n",
    "            all_dat = normaldist0[0], list(normaldist1[:,0]), list(normaldist2[:,0]), list(normaldist3[:,0])\n",
    "            all_dat = np.transpose(all_dat)\n",
    "\n",
    "            for i in range(len(allres)):\n",
    "                if allres[i] == 1:\n",
    "                    norm_feat = all_dat[:,i]\n",
    "                    break\n",
    "\n",
    "            # if plotORnot == 1:\n",
    "                # # histogram\n",
    "                # print('shape of all_dat : ', all_dat.shape)\n",
    "                # dfout = pd.DataFrame(data=all_dat)\n",
    "\n",
    "\n",
    "                # fig, ax=plt.subplots(4,1)\n",
    "                # sns.distplot(dfout[0], ax=ax[0], bins=30, label=\"stats\") \n",
    "                # sns.distplot(dfout[1], ax=ax[1], bins=30, label='bc')\n",
    "                # sns.distplot(dfout[2], ax=ax[2], bins=30, label='yj')\n",
    "                # sns.distplot(dfout[3], ax=ax[3], bins=30, label='qt')\n",
    "\n",
    "        return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d3392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_intercurrentpt_makeshortSIGlong_interp1d(shortSIG, longSIG):\n",
    "\n",
    "    x = np.linspace(shortSIG[0], len(shortSIG), num=len(shortSIG), endpoint=True)\n",
    "    y = shortSIG\n",
    "    # print('x : ', x)\n",
    "\n",
    "\n",
    "    # -------------\n",
    "    kind = 'linear'\n",
    "    # kind : Specifies the kind of interpolation as a string or as an integer specifying \n",
    "    # the order of the spline interpolator to use. The string has to be one of ‘linear’, ‘nearest’, \n",
    "    # ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’, ‘slinear’, \n",
    "    # ‘quadratic’ and ‘cubic’ refer to a spline interpolation of zeroth, first, second or third order; \n",
    "    # ‘previous’ and ‘next’ simply return the previous or next value of the point; ‘nearest-up’ and \n",
    "    # ‘nearest’ differ when interpolating half-integers (e.g. 0.5, 1.5) in that ‘nearest-up’ rounds \n",
    "    # up and ‘nearest’ rounds down. Default is ‘linear’.\n",
    "\n",
    "    if kind == 'linear':\n",
    "        f = interp1d(x, y)\n",
    "    elif kind == 'cubic':\n",
    "        f = interp1d(x, y, kind='cubic')\n",
    "    # -------------\n",
    "\n",
    "    xnew = np.linspace(shortSIG[0], len(shortSIG), num=len(longSIG), endpoint=True)\n",
    "    # print('xnew : ', xnew)\n",
    "\n",
    "    siglong = f(xnew)\n",
    "\n",
    "    return siglong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "589fbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level : the number of levels to decompose the time signal, le nombre des marquers par signale\n",
    "def tsig_2_discrete_wavelet_transform(sig, waveletname, level, plotORnot):\n",
    "\n",
    "    # On peut calculater dans deux façons: 0) dwt en boucle and then idwt, 1) wavedec et waverec\n",
    "    # Mais le deux ne donnent pas le meme reponses, wavedec et waverec semble plus raisonable.\n",
    "    coeff = wavedec(sig, waveletname, level)\n",
    "\n",
    "    if plotORnot == 1:\n",
    "        fig, axx = plt.subplots(nrows=len(coeff), ncols=1, figsize=(5,5))\n",
    "        axx[0].set_title(\"coef\")  # Pas certain si c'est coef0 ou coef1\n",
    "        for k in range(len(coeff)):\n",
    "            axx[k].plot(coeff[k], 'r') # output of the low pass filter (averaging filter) of the DWT\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f4e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsig_2_spectrogram(sig, fs, nfft, noverlap, img_dim, plotORnot):\n",
    "\n",
    "    # -----------------------------------\n",
    "    fig,ax = plt.subplots(1)\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    # spectrum2D array : Columns are the periodograms of successive segments.\n",
    "    # freqs1-D array : The frequencies corresponding to the rows in spectrum.\n",
    "    # t1-D array : The times corresponding to midpoints of segments (i.e., the columns in spectrum).\n",
    "    # imAxesImage : The image created by imshow containing the spectrogram.\n",
    "    pxx, freqs, bins, img = ax.specgram(sig, nfft, fs, noverlap=noverlap)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    my_image = 'temp.png'\n",
    "    fig.savefig(my_image)\n",
    "    fname = os.path.abspath(os.getcwd()) + \"/\" +  my_image\n",
    "    \n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)         # PIL: img is not in array form, it is a PIL.PngImagePlugin.PngImageFile \n",
    "\n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Resize sectrogram image\n",
    "    image = imgORmat_resize_imgORmat_CNN(img_dim, data_in=img, inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Flatten image into a vector\n",
    "    img_flatten = np.reshape(np.ravel(image), (img_dim*img_dim, ), order='F')\n",
    "    # print('img_flatten.shape: ', img_flatten.shape)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    return img_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d27bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsig_2_continuous_wavelet_transform(sig, fs, scales, waveletname, img_dim, plotORnot):\n",
    "    \n",
    "    dt = 1/fs\n",
    "    # print('dt : ', dt)\n",
    "\n",
    "    coefficients, frequencies = pywt.cwt(sig, scales, waveletname, dt)\n",
    "    coefficients = np.array(coefficients)\n",
    "    ylen, xlen = coefficients.shape\n",
    "\n",
    "    # Time by frequency plot of cwt : then flatten and use as a feature\n",
    "    stop_val = len(sig)/fs\n",
    "    x = np.arange(0, stop_val, dt)  # time\n",
    "    y = frequencies # frequency \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = coefficients\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes() # creates a 3D axis by using the keyword projection='3d'\n",
    "    ax.contourf(X, Y, Z, xlen, cmap=plt.cm.seismic) # contour fill\n",
    "    ax.axis('off')\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    my_image = 'temp.png'\n",
    "    fig.savefig(my_image)\n",
    "    fname = os.path.abspath(os.getcwd()) + \"/\" +  my_image\n",
    "\n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)  \n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Resize sectrogram image\n",
    "    image = imgORmat_resize_imgORmat_CNN(img_dim, data_in=img, inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Flatten image into a vector\n",
    "    img_flatten = np.reshape(np.ravel(image), (img_dim*img_dim, ), order='F')\n",
    "    # print('img_flatten.shape: ', img_flatten.shape)\n",
    "    \n",
    "    return img_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05a729c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, img_dim):\n",
    "    if type(img) == 'numpy.ndarray':\n",
    "        # img is an array, retuns an image object\n",
    "        rgb_image = Image.fromarray(img , 'RGB')\n",
    "    else:\n",
    "        # img is an image object, returns an image object\n",
    "        try:\n",
    "            rgb_image = img.convert('RGB')\n",
    "        except AttributeError:\n",
    "            rgb_image = Image.fromarray(img , 'RGB')\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(img_dim), int(img_dim)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "    return img3\n",
    "\n",
    "def convert_img_a_mat(img, outpt):\n",
    "    mat = np.array(img)  # Convert image to an array\n",
    "    if outpt == 'mat2D':\n",
    "        # Transformer l'image de 3D à 2D\n",
    "        # Convert image back to a 2D array\n",
    "        matout = np.mean(mat, axis=2)\n",
    "    elif outpt == 'img3D': # techniquement c'est un image parce qu'il y a trois RGB channels \n",
    "        matout = mat\n",
    "    return matout\n",
    "\n",
    "def norm_mat(mat2Dor3D, norm):\n",
    "    if norm == 'zero2one':\n",
    "        # Normalizer l'image entre 0 et 1\n",
    "        norout = mat2Dor3D/255\n",
    "    elif norm == 'negone2posone':\n",
    "        # Normalize the images to [-1, 1]\n",
    "        norout = (mat2Dor3D - 127.5) / 127.5\n",
    "    elif norm == 'non':\n",
    "        norout = mat2Dor3D\n",
    "    return norout\n",
    "\n",
    "def threshold_mat(mat2D, thresh):\n",
    "    # Threshold image\n",
    "    val = 255/2\n",
    "    if thresh == 'zero_moins_que_val':\n",
    "        row, col = mat2D.shape\n",
    "        mat_thresh = mat2D\n",
    "        min_val = np.min(mat_thresh)\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if mat_thresh[i,j] < val:\n",
    "                    mat_thresh[i,j] = min_val\n",
    "    elif thresh == 'non':\n",
    "        mat_thresh = mat2D\n",
    "    return mat_thresh\n",
    "\n",
    "def imgORmat_resize_imgORmat_CNN(img_dim, data_in, inpt='img3D', outpt='mat2D', norm='non', thresh='non'):\n",
    "    if inpt == 'img3D' and outpt=='mat2D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='mat2D':\n",
    "        data_in = np.array(data_in)\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        mat2D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(mat2D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='img3D':\n",
    "        data_in = np.array(data_in)\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'img3D' and outpt=='img3D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d651ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(ax_val, ss_val, df_timeseries_exp):\n",
    "    \n",
    "    if ax_val == 'all' and ss_val == 'all':\n",
    "        # All the data\n",
    "        df_timeseries_exp[exp].head()\n",
    "        df = df_timeseries_exp[exp]\n",
    "    elif ax_val != 'all' and ss_val == 'all':\n",
    "        # Prediction for each axis\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "    elif ax_val == 'all' and ss_val != 'all':\n",
    "        # Prediction per sup/sub\n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0)]\n",
    "    elif ax_val != 'all' and ss_val != 'all':\n",
    "        # Prediction per axis and sup/sub\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        \n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "\n",
    "    print('Confirmation : exp=', exp, ', ax_val=', ax_val, ', ss_val=', ss_val)\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b3331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the joystick stimulus axis data and put it in a pandas column\n",
    "def indexit(row):\n",
    "    joy_mat = [row.JOY_ax0, row.JOY_ax1, row.JOY_ax2]\n",
    "    return joy_mat[int(row.ax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb2e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_lab_kmeans_clustering(*arg):\n",
    "    \n",
    "    n_clusters = arg[0]\n",
    "    X = arg[1]\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    if len(X.shape) == 1 or X.shape[1] == 1:\n",
    "        X = np.ravel(X)\n",
    "        out = pd.Series(X)\n",
    "        X = pd.concat([out, out], axis=1).to_numpy()\n",
    "    \n",
    "    from sklearn import cluster\n",
    "    \n",
    "    kmeans = cluster.KMeans(n_clusters=n_clusters, init='k-means++',algorithm='elkan', random_state=2)\n",
    "    # n_clusters : The number of clusters to form as well as the number of centroids to generate. (int, default=8)\n",
    "    \n",
    "    # init : Method for initialization : (default=’k-means++’)\n",
    "    # init='k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "    # init='random': choose n_clusters observations (rows) at random from data for the initial centroids.\n",
    "    \n",
    "    # n_init : Number of time the k-means algorithm will be run with different centroid seeds (int, default=10)\n",
    "    \n",
    "    # max_iter : Maximum number of iterations of the k-means algorithm for a single run. (int, default=300)\n",
    "    \n",
    "    # tol : Relative tolerance with regards to Frobenius norm of the difference in the cluster centers \n",
    "    # of two consecutive iterations to declare convergence. (float, default=1e-4)\n",
    "    \n",
    "    # (extremly important!) random_state : Determines random number generation for centroid initialization\n",
    "    #(int, RandomState instance or None, default=None)\n",
    "    \n",
    "    # algorithm{“auto”, “full”, “elkan”}, default=”auto”\n",
    "    # K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more \n",
    "    # efficient on data with well-defined clusters, by using the triangle inequality. However it’s more \n",
    "    # memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # print('shape of X : ', X.shape)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # ------------------------------\n",
    "\n",
    "    # Get the prediction of each category : predicted label\n",
    "    label = kmeans.labels_\n",
    "    # print('clusters_out : ' + str(clusters_out))\n",
    "    # OR\n",
    "    label = kmeans.predict(X)\n",
    "    # print('clusters_out : ' + str(clusters_out))\n",
    "    # print('length of clusters_out', len(clusters_out))\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # Centroid values for feature space : this is the center cluster value per feature in X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    # print('centroids org : ' + str(centroids))\n",
    "\n",
    "    # ------------------------------\n",
    "\n",
    "    return kmeans, label, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "039a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_and_initial_feature(df):\n",
    "    # IC = 1\n",
    "    # EC = 2, 4, 5\n",
    "    # NC = 3, 6, 7\n",
    "    # NR = 9\n",
    "    # (IC) - sham (do not use) = 8\n",
    "    # (NC) - sham (do not use) = 10\n",
    "\n",
    "    # Just to confirm, what are the unique values of res_type\n",
    "    df.res_type.value_counts(ascending=True)\n",
    "\n",
    "    # Construction of SD_label : How do we define disorientation?\n",
    "\n",
    "    # Way 0 : lenient\n",
    "    # 0 = If participates got the result CORRECT for the trial, they were NOT disoriented. (IC, EC)\n",
    "    # 1 = If participates got the result WRONG or did not respond, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1) | (df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.lenient = ''  # define a new column , rows 8 and 10 will be NaN, need to do dropna for rows\n",
    "    df.loc[idx_NDS, 'lenient'] = 0\n",
    "    df.loc[idx_DS, 'lenient'] = 1\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Way 1 : strict simple\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants were WRONG at any point, they were disoriented. (EC, NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5) | (df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.strict = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'strict'] = 0\n",
    "    df.loc[idx_DS, 'strict'] = 1\n",
    "\n",
    "    # Way 2 : st_complex\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants got the result eventually CORRECT, they were MILDLY disoriented. (EC)\n",
    "    # 2 = If participants were WRONG for the trial, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_MDS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.st_complex = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'st_complex'] = 0\n",
    "    df.loc[idx_MDS, 'st_complex'] = 1\n",
    "    df.loc[idx_DS, 'st_complex'] = 2\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Create features :  (1) position\n",
    "    df['joy_stim'] = df.apply(indexit, axis='columns')  # fill in joy_stim\n",
    "    \n",
    "    # -------------------------------------\n",
    "\n",
    "    # Make DataFrame for trial start-stop index\n",
    "    # Cut the data up per trial across subjects\n",
    "    tr_vec = df.tr.to_numpy()\n",
    "\n",
    "    st = [0]\n",
    "    ender = []\n",
    "    for i in range(len(tr_vec)-1):\n",
    "        if tr_vec[i] != tr_vec[i+1]:\n",
    "            st = st + [i+1]\n",
    "            ender = ender + [i]\n",
    "    ender = ender + [len(tr_vec)-1]\n",
    "\n",
    "    # See start-stop index clearly\n",
    "    e0 = np.reshape(st, (len(st),1))\n",
    "    e1 = np.reshape(ender, (len(st),1))\n",
    "    data = np.ravel(e0), np.ravel(e1)\n",
    "    data = np.transpose(data)\n",
    "    columns = ['stind', 'endind']\n",
    "    temp = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Find the longest trial signal in df_rot['joy_stim']\n",
    "    temp['diff'] = temp.endind - temp.stind\n",
    "    temp['timediff'] = [df.time.iloc[temp.endind[i]] - df.time.iloc[temp.stind[i]] for i in range(len(temp.endind))]\n",
    "    outmin = temp['diff'].min()\n",
    "    outmax = temp['diff'].max()\n",
    "\n",
    "    tomin = temp['timediff'][(temp['diff'] == outmin)]\n",
    "    tomax = temp['timediff'][(temp['diff'] == outmax)]\n",
    "    # print('outmin : ', outmin, 't :', tomin)\n",
    "    # print('outmax : ', outmax, 't :', tomax)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Interpolate : make each trial the same number of data points\n",
    "    feat0 = []\n",
    "    t_feat0 = []\n",
    "    y1_feat0 = []\n",
    "    y2_feat0 = []\n",
    "    y3_feat0 = []\n",
    "    for i in range(len(temp.stind)):\n",
    "        \n",
    "        # X\n",
    "        sSIG = df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        t_sSIG = df['time'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "\n",
    "        # labels\n",
    "        y1 = df['lenient'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y2 = df['strict'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y3 = df['st_complex'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        \n",
    "        # Check if trial data is less than the maximum length\n",
    "        if len(df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]]) < outmax:\n",
    "            \n",
    "            # The trial length is different so interpolate the time-series to make them the same length signal \n",
    "            x = np.linspace(sSIG[0], len(sSIG), num=len(sSIG), endpoint=True)\n",
    "            xnew = np.linspace(sSIG[0], len(sSIG), num=outmax, endpoint=True)\n",
    "\n",
    "            # joystick on stim\n",
    "            f = interp1d(x, sSIG)\n",
    "            sSIGl = f(xnew)\n",
    "\n",
    "            # time\n",
    "            f = interp1d(x, t_sSIG)\n",
    "            t_sSIGl = f(xnew)\n",
    "\n",
    "            # y1\n",
    "            f = interp1d(x, y1)\n",
    "            y1_sSIGl = f(xnew)\n",
    "\n",
    "            # y2\n",
    "            f = interp1d(x, y2)\n",
    "            y2_sSIGl = f(xnew)\n",
    "\n",
    "            # y3\n",
    "            f = interp1d(x, y3)\n",
    "            y3_sSIGl = f(xnew)\n",
    "\n",
    "            # python : you can not create a matrix in real-time in pandas\n",
    "            # you only assign the full matrix at the end\n",
    "            # (0) position\n",
    "            feat0 = feat0 + [sSIGl]\n",
    "            t_feat0 = t_feat0 + [t_sSIGl]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1_sSIGl)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2_sSIGl)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3_sSIGl)]\n",
    "            \n",
    "            del x, f, sSIGl, t_sSIGl, y1_sSIGl, y2_sSIGl, y3_sSIGl\n",
    "        else:\n",
    "            feat0 = feat0 + [sSIG]\n",
    "            t_feat0 = t_feat0 + [t_sSIG]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3)]\n",
    "\n",
    "    # Clean up\n",
    "    del df\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7644c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_feature_data(feat, plotORnot, type_scale):\n",
    "    \n",
    "    columns = ['0']\n",
    "    dat = pd.DataFrame(data=feat, columns=columns)\n",
    "    \n",
    "    # which type of scaling\n",
    "    if type_scale == 'minmax':\n",
    "        # Values from 0 to 1\n",
    "        scaled_data0 = minmax_scaling(dat, columns=columns)\n",
    "        scaled_data = list(scaled_data0.to_numpy().ravel())\n",
    "        \n",
    "    elif type_scale == 'normalization':\n",
    "        # normalization : same as mlxtend - Values from 0 to 1\n",
    "        scaled_data = []\n",
    "        for q in range(len(feat)):\n",
    "            scaled_data.append( (feat[q] - np.min(feat))/(np.max(feat) - np.min(feat)) )\n",
    "    \n",
    "    elif type_scale == 'pos_normalization':\n",
    "        # positive normalization : same as mlxtend - Values from 0 to 1\n",
    "        shift_up = [i - np.min(feat) for i in feat]\n",
    "        scaled_data = [q/np.max(shift_up) for q in shift_up]\n",
    "    \n",
    "    elif type_scale == 'standardization':\n",
    "        # standardization : values are not restricted to a range, but scaled appropreately\n",
    "        scaled_data = [(q - np.mean(feat))/np.std(feat) for q in feat]\n",
    "\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30b04acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_joystick_sig(feat0):\n",
    "    # Realized that we need to check the correctness of the data signal AGAIN\n",
    "    # Some of the signals look like cabin data movement\n",
    "    \n",
    "    # This joystick signals should start around 0 for each trial\n",
    "    marg = 0.1\n",
    "    temp = []\n",
    "    gard = []\n",
    "    for i in range(len(feat0)):\n",
    "        if feat0[i,0] < marg and feat0[i,0] > -marg:\n",
    "            temp.append(feat[i] - feat[i,0]) # baseline shift signal to zero\n",
    "            gard.append(i)\n",
    "    \n",
    "    return temp, gard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07351b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_sig(*arg):\n",
    "    \n",
    "    # ------------------------------------\n",
    "    \n",
    "    sig = arg[0]\n",
    "    timesteps = arg[1]\n",
    "    \n",
    "    if len(arg) > 2:\n",
    "        t = arg[2]\n",
    "    \n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Downsampling\n",
    "    a = int(np.floor(len(sig)/timesteps))\n",
    "    vals = np.arange(0, a*timesteps, a)\n",
    "    v = [sig[i:(i+a)][0] for i in vals] # duh, pick the first point from each window, that is why [0] is there\n",
    "    \n",
    "    # ------------------------------------\n",
    "    \n",
    "    if len(arg) > 2:\n",
    "        # Recalculate new frequency\n",
    "        tt = [t[i:(i+a)][0] for i in vals]\n",
    "        dt = tt[1] - tt[0]\n",
    "        fs = 1/dt\n",
    "        \n",
    "        return v, fs\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d1b43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creation_preprocessing(feat0, t_feat0):\n",
    "\n",
    "    # ----------------\n",
    "    # Make your features\n",
    "    # ----------------\n",
    "    X = pd.DataFrame()\n",
    "\n",
    "    n = 4   # filter order\n",
    "    fs = 250 # data sampling frequency (Hz)\n",
    "    fcc = 10  # Cut-off frequency of the filter\n",
    "    w = fcc / (fs / 2) # Normalize the frequency\n",
    "    b, a = signal.butter(n, w, 'low')  # 3rd order\n",
    "\n",
    "    print('num de samples avant dropna : ', len(feat0))\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Drop nan values from feat0\n",
    "    temp = pd.DataFrame(feat0)\n",
    "    temp0 = my_dropna_python(temp)\n",
    "    feat0 = temp0.to_numpy()\n",
    "    print('num de samples apres dropna : ', len(feat0))\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Make timesteps divisible by 10 so we can vary it :\n",
    "    rmit = len(feat0[0]) % 10\n",
    "    timesteps = len(feat0[0]) - rmit\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Need to find when one trial starts and end - take derivative from start-stop periods\n",
    "    for i in range(len(feat0)):\n",
    "\n",
    "        if i == 0:\n",
    "            plotORnot = 0 #1\n",
    "        else:\n",
    "            plotORnot = 0\n",
    "            \n",
    "        # ----------------------------\n",
    "        # Causale ordre : temporal features : 3 features\n",
    "        # ----------------------------\n",
    "        # (0) position\n",
    "        pos, newfs = downsample_sig(feat0[i], timesteps, t_feat0[i])\n",
    "\n",
    "        # (1) velocity\n",
    "        vel = numderiv(feat0[i], t_feat0[i])\n",
    "        vel, newfs = downsample_sig(vel, timesteps, t_feat0[i])\n",
    "\n",
    "        # (2) acceleration\n",
    "        acc = numderiv(vel, t_feat0[i])\n",
    "        filtacc = signal.filtfilt(b, a, acc) # the signal is noisy\n",
    "        filtacc, newfs = downsample_sig(filtacc, timesteps, t_feat0[i])\n",
    "        # ----------------------------\n",
    "\n",
    "        \n",
    "        # ....................\n",
    "        # Discrete time wavelets : 5 * 3 = 15 features\n",
    "        # ....................\n",
    "        level = 5\n",
    "        \n",
    "        # (0) position\n",
    "        coeff = tsig_2_discrete_wavelet_transform(pos, 'sym5', level, plotORnot)  # Manageable length = 16\n",
    "        # Not sure why it gives more than level+1 coefficents\n",
    "        # Only take level+1\n",
    "        coeff = coeff[0:level+1]\n",
    "        # Ensure that all wavelets are no longer than timesteps\n",
    "        coeff0 = [coeff[i] if len(coeff[i]) <= timesteps else coeff[i][0:timesteps] for i in range(len(coeff))]\n",
    "        \n",
    "        # (1) velocity\n",
    "        coeff = tsig_2_discrete_wavelet_transform(vel, 'sym5', level, plotORnot)  # Manageable length = 16\n",
    "        # Not sure why it gives more than level+1 coefficents\n",
    "        # Only take level+1\n",
    "        coeff = coeff[0:level+1]\n",
    "        # Ensure that all wavelets are no longer than timesteps\n",
    "        coeff1 = [coeff[i] if len(coeff[i]) <= timesteps else coeff[i][0:timesteps] for i in range(len(coeff))]\n",
    "        \n",
    "        # (2) acceleration\n",
    "        coeff = tsig_2_discrete_wavelet_transform(filtacc, 'sym5', level, plotORnot)  # Manageable length = 16\n",
    "        # Not sure why it gives more than level+1 coefficents\n",
    "        # Only take level+1\n",
    "        coeff = coeff[0:level+1]\n",
    "        # Ensure that all wavelets are no longer than timesteps\n",
    "        coeff2 = [coeff[i] if len(coeff[i]) <= timesteps else coeff[i][0:timesteps] for i in range(len(coeff))]\n",
    "        \n",
    "\n",
    "        # ----------------------------\n",
    "        # Hybrid marquers : temporalle et frequence information\n",
    "        # ----------------------------\n",
    "        # (8) spectrogram flatten - periodogram (fft)\n",
    "        img_dim = int(np.sqrt(timesteps))\n",
    "        nfft = 20\n",
    "        noverlap = 0\n",
    "        \n",
    "        # (0) position\n",
    "        stfft0 = tsig_2_spectrogram(pos, newfs, nfft, noverlap, img_dim, plotORnot)\n",
    "        \n",
    "        # (1) velocity\n",
    "        stfft1 = tsig_2_spectrogram(vel, newfs, nfft, noverlap, img_dim, plotORnot)\n",
    "        \n",
    "        # (2) acceleration\n",
    "        stfft2 = tsig_2_spectrogram(filtacc, newfs, nfft, noverlap, img_dim, plotORnot)\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2D continuous wavelet transform flattened\n",
    "        # ----------------------------\n",
    "        # continuous_wavelets = ['mexh', 'morl', 'cgau5', 'gaus5']\n",
    "        scales = np.arange(1, 128)\n",
    "        waveletname = 'mexh'\n",
    "        \n",
    "        # (0) position\n",
    "        ctfft0 = tsig_2_continuous_wavelet_transform(pos, newfs, scales, waveletname, img_dim, plotORnot)\n",
    "        \n",
    "        # (1) velocity\n",
    "        ctfft1 = tsig_2_continuous_wavelet_transform(vel, newfs, scales, waveletname, img_dim, plotORnot)\n",
    "        \n",
    "        # (2) acceleration\n",
    "        ctfft2 = tsig_2_continuous_wavelet_transform(filtacc, newfs, scales, waveletname, img_dim, plotORnot)\n",
    "        \n",
    "        del scales\n",
    "        # ----------------------------\n",
    "        \n",
    "\n",
    "        # ----------------------------\n",
    "        # kmeans\n",
    "        # ----------------------------\n",
    "        n_clusters = 2  # Note: c'est change par rapport ynum!! ynum=0 ou 1\n",
    "        part1 = pd.concat([pd.Series(pos), pd.Series(vel), pd.Series(filtacc)], axis=1)\n",
    "        temp = part1.to_numpy()\n",
    "        kmeans, km_cl2, centroids = unsupervised_lab_kmeans_clustering(n_clusters, temp)\n",
    "\n",
    "        n_clusters = 3  # Note: c'est change par rapport ynum!!  ynum=2\n",
    "        kmeans, km_cl3, centroids = unsupervised_lab_kmeans_clustering(n_clusters, temp)\n",
    "        \n",
    "        del part1, temp, kmeans, centroids\n",
    "        # ----------------------------\n",
    "        \n",
    "        # Time features :\n",
    "        col0 = pd.Series(pos)\n",
    "        col1 = pd.Series(vel)\n",
    "        col2 = pd.Series(filtacc)\n",
    "        \n",
    "        # Frequency features :\n",
    "        cols3 = pd.DataFrame(coeff0).T\n",
    "        cols4 = pd.DataFrame(coeff1).T\n",
    "        cols5 = pd.DataFrame(coeff2).T\n",
    "\n",
    "        # Time-Frequency features :\n",
    "        col6 = pd.Series(stfft0)\n",
    "        col7 = pd.Series(stfft1)\n",
    "        col8 = pd.Series(stfft2)\n",
    "        \n",
    "        col9 = pd.Series(ctfft0)\n",
    "        col10 = pd.Series(ctfft1)\n",
    "        col11 = pd.Series(ctfft2)\n",
    "        \n",
    "        # Categorical features : kmeans\n",
    "        col12 = pd.Series(km_cl2)\n",
    "        col13 = pd.Series(km_cl3)\n",
    "        \n",
    "        df = pd.concat([col0, col1, cols2, cols3, cols4, cols5, col6, col7, col8, col9, col10, col11, col12, col13], axis=1)\n",
    "        \n",
    "        del col0, col1, col2, cols3, cols4, cols5, col6, col7, col8, col9, col10, col11, col12, col13\n",
    "        # ----------------------------\n",
    "\n",
    "        \n",
    "        # Interpolate toutes les vectors\n",
    "        for i in range(len(df.columns)):\n",
    "            \n",
    "            # Scale toutes les vectors\n",
    "            type_scale = 'standardization' \n",
    "            df.iloc[:,i] = scale_feature_data(df.iloc[:,i].to_numpy(), plotORnot, type_scale)\n",
    "\n",
    "        X = pd.concat([X, df], axis=0)\n",
    "        \n",
    "    X = X.to_numpy()\n",
    "    # ----------------\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8143131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rename_columns(df, col_list):\n",
    "    \n",
    "    way = 1\n",
    "    \n",
    "    if way == 0:\n",
    "        # Façon difficile\n",
    "        # Rename columns\n",
    "        onames = df.columns.to_numpy()\n",
    "        dictout = {}\n",
    "        for nf in range(len(col_list)):\n",
    "            dictout[onames[nf]] = '%s' % (col_list[nf])\n",
    "            \n",
    "        # Determinez quels columns de df repeter\n",
    "        uq = Counter(onames).most_common()\n",
    "        d = {}\n",
    "        for i in range(len(uq)):\n",
    "            temp = []\n",
    "            for ind, val in enumerate(onames):\n",
    "                if uq[i][0] == val:\n",
    "                    temp.append(col_list[ind])\n",
    "            d[i] = temp    \n",
    "        \n",
    "        # if the column name is a key of d pop the names in the list, else return the column name\n",
    "        df.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    \n",
    "    elif way == 1:\n",
    "        # Façon facile\n",
    "        df.columns = col_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd76362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data_2makeclasses_equivalent(df_feat):\n",
    "    # Remove nan value per row\n",
    "    # df_test_noNan = df_feat.dropna(axis=0)\n",
    "    # OR\n",
    "    df_test_noNan = my_dropna_python(df_feat)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Confirm that there are no nan values\n",
    "    out = df_test_noNan.isnull().values.any()\n",
    "    print('Are there nan valeus in the data : ', out)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Check class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test_noNan)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    print('shape of dataframe before padding : ', df_test_noNan.shape)\n",
    "\n",
    "    # ----------------\n",
    "    # Pad the DataFrame\n",
    "    n_classes = len(count_index)\n",
    "    n_samples = len(st)\n",
    "\n",
    "    df_2add_on = pd.DataFrame()\n",
    "    \n",
    "    # Le derniere sample dans df_test_noNan\n",
    "    df_coupe_proche = df_test_noNan.iloc[st[-1]:endd[-1], :]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        #print('i : ', i)\n",
    "        # Pad short length classes\n",
    "        for j in range(needed_samps_class[i]):\n",
    "            #print('j : ', j) \n",
    "            flag = 0\n",
    "            while flag == 0:\n",
    "                permvec = np.random.permutation(n_samples)\n",
    "                index = permvec[0]  #random choosen index\n",
    "                \n",
    "                # look for each class : on veut le classe être le meme\n",
    "                if i == int(df_test_noNan.y.iloc[st[index]]):\n",
    "                    #print('Class match was found : i = ', i, ', data index = ', int(df_test_noNan.y_scalar.iloc[index]), ', index = ', index)\n",
    "                    \n",
    "                    # Append the data with padded data entry\n",
    "                    df_coupe = df_test_noNan.iloc[st[index]:endd[index], :]\n",
    "                    \n",
    "                    # Le derriere sample ne sont pas le meme que le sample actuelle\n",
    "                    if int(df_coupe.iloc[0,0] - df_coupe_proche.iloc[0,0]) != 0:\n",
    "                        df_coupe_proche = df_coupe\n",
    "                        df_2add_on = pd.concat([df_2add_on, df_coupe], axis=0)\n",
    "                        flag = 1 # to brake while\n",
    "                        \n",
    "    # ----------------\n",
    "\n",
    "    # DataFrame a besoin les noms de columns d'avoir le meme noms que df_test_noNan\n",
    "    df_2add_on = df_2add_on.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    col_list = df_test_noNan.columns\n",
    "    df_2add_on = pandas_rename_columns(df_2add_on, col_list)\n",
    "    df_2add_on\n",
    "\n",
    "    print('shape of dataframe to add to original dataframe: ', df_2add_on.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # want to arrange the dataframe with respect to rows (stack on top of the other): so axis=0 \n",
    "    # OR think of it as the rows of the df change so you put axis=0 for rows\n",
    "    df_test2 = pd.concat([df_test_noNan, df_2add_on], axis=0)\n",
    "    df_test2 = df_test2.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    print('shape of padded dataframe (original + toadd) : ', df_test2.shape)\n",
    "\n",
    "    del df_test_noNan, df_2add_on\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Final check of class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Enlevez des columns unnecessaires : num, y\n",
    "    # df_test2 = df_test2.drop(['num', 'y'], axis=1)  # 1 is the axis number (0 for rows and 1 for columns)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Rename each feature column a number, and the label column with y \n",
    "    # col_list = list(map(str, np.arange(len(df_test2.columns) - 1)))\n",
    "    # col_list.append('y')\n",
    "    # df_test2 = pandas_rename_columns(df_test2, col_list)\n",
    "    \n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nonconsecutive_values_debut_fin_pt(vec):\n",
    "  \n",
    "    st = [0]\n",
    "    endd = []\n",
    "    \n",
    "    for i in range(len(vec)-1):\n",
    "        if vec[i] != vec[i+1]:\n",
    "            st.append(i+1)\n",
    "            endd.append(i)\n",
    "    \n",
    "    endd.append(len(vec)-1)\n",
    "    \n",
    "    return st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79951844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(df_test_noNan):\n",
    "\n",
    "    # Get start and end index values for each sample\n",
    "    num = list(map(int, df_test_noNan.num.to_numpy()))\n",
    "    st, endd = detect_nonconsecutive_values_debut_fin_pt(num)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    yy = list(map(int, df_test_noNan.y.to_numpy()))\n",
    "    y_short = []\n",
    "    for i in range(len(st)):\n",
    "        y_short.append(yy[st[i]:st[i]+1][0])\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    liste = Counter(y_short).most_common()\n",
    "    count_index, counted_value = list(map(list, zip(*liste)))\n",
    "\n",
    "    print('Before sorting counted_value : ', counted_value)\n",
    "    print('Before sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Sort counted_value by count_index; in ascending order\n",
    "    sind = np.argsort(count_index)\n",
    "    count_index = [count_index[i] for i in sind]\n",
    "    counted_value = [counted_value[i] for i in sind]\n",
    "    print('After sorting counted_value : ', counted_value)\n",
    "    print('After sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Determine how much to pad each class label\n",
    "    needed_samps_class = np.max(counted_value) - counted_value\n",
    "    print('needed_samps_class : ', needed_samps_class)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    return needed_samps_class, counted_value, count_index, st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f549b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dat_pickle(outSIG, file_name=\"outSIG.pkl\"):\n",
    "    # Save data matrices to file\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(outSIG, open_file)\n",
    "    open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37d9ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dat_pickle(file_name=\"outSIG.pkl\"):\n",
    "    open_file = open(file_name, \"rb\")\n",
    "    dataout = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return dataout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3e60e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_Y1Dvec_2_Ybin(Y):\n",
    "    \n",
    "    # Transform a 1D Y vector (n_samples by 1) to a Y_bin (n_samples by n_classes) vector\n",
    "\n",
    "    # Ensure vector is of integers\n",
    "    Y = [int(i) for i in Y]\n",
    "\n",
    "    # Number of samples\n",
    "    m_examples = len(Y)\n",
    "\n",
    "    # Number of classes\n",
    "    temp = np.unique(Y)\n",
    "    unique_classes = [int(i) for i in temp]\n",
    "    # print('unique_classes : ', unique_classes)\n",
    "\n",
    "    whichone = 2\n",
    "    # Binarize the output\n",
    "    if whichone == 0:\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        Y_bin = label_binarize(Y, classes=unique_classes)  # does not work all the time\n",
    "\n",
    "    elif whichone == 1:\n",
    "        from sklearn import preprocessing\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        Y_bin = lb.fit_transform(Y)  # does not work all the time\n",
    "        \n",
    "    elif whichone == 2:  # reliable\n",
    "        # By hand\n",
    "        Y_bin = np.zeros((m_examples, len(unique_classes)))\n",
    "        for i in range(0, m_examples):\n",
    "            if Y[i] == unique_classes[0]:\n",
    "                Y_bin[i,0] = 1\n",
    "            elif Y[i] == unique_classes[1]:\n",
    "                Y_bin[i,1] = 1\n",
    "            elif Y[i] == unique_classes[2]:\n",
    "                Y_bin[i,2] = 1\n",
    "            elif Y[i] == unique_classes[3]:\n",
    "                Y_bin[i,3] = 1\n",
    "            elif Y[i] == unique_classes[4]:\n",
    "                Y_bin[i,4] = 1\n",
    "            elif Y[i] == unique_classes[5]:\n",
    "                Y_bin[i,5] = 1\n",
    "            elif Y[i] == unique_classes[6]:\n",
    "                Y_bin[i,6] = 1\n",
    "            elif Y[i] == unique_classes[7]:\n",
    "                Y_bin[i,7] = 1\n",
    "            elif Y[i] == unique_classes[8]:\n",
    "                Y_bin[i,8] = 1\n",
    "            elif Y[i] == unique_classes[9]:\n",
    "                Y_bin[i,9] = 1\n",
    "            elif Y[i] == unique_classes[10]:\n",
    "                Y_bin[i,10] = 1\n",
    "                \n",
    "    print('shape of Y_bin : ', Y_bin.shape)\n",
    "\n",
    "    return Y_bin, unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd36c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_repeating_signatures(data2, timesteps, plotORnot):\n",
    "    \n",
    "    # ------------------------------------\n",
    "\n",
    "    # choose wind such that sig is timesteps long\n",
    "    size_of_mat = timesteps/2\n",
    "    wind = int(len(data2)/size_of_mat)\n",
    "    sig = binarize_audio_signal(wind, data2)\n",
    "\n",
    "    min_border = np.mean(sig) + 1*np.std(sig)\n",
    "\n",
    "    # On a besoin d'avoir au moins de 2 peaks\n",
    "    peaks = []\n",
    "    while len(peaks) < 2:\n",
    "        # peaks, properties = find_peaks(sig, height=(min_border, np.max(sig)), prominence=100)\n",
    "        peaks, properties = find_peaks(sig, height=(min_border, np.max(sig)))\n",
    "        min_border = min_border - 10\n",
    "    vv = [sig[i] for i in peaks]\n",
    "\n",
    "    if plotORnot == 1:\n",
    "        plt.plot(sig)\n",
    "        plt.plot(peaks, vv, 'r*')\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.xlabel(\"Data points\")\n",
    "        plt.title(\"Bird call : binarized data2\")\n",
    "        plt.show()\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Calculate the difference in peaks and find the difference that repeats the most\n",
    "    # This repeating difference is the ideal window to cut the data for bird call temporal signatures\n",
    "    peakdiff = [peaks[i+1]-peaks[i] for i in range(len(peaks)-1)]\n",
    "    z = sorted(peakdiff)\n",
    "    o = [len(list(str(z[i]))) for i in range(len(z))]\n",
    "\n",
    "    from collections import Counter\n",
    "    c = Counter(o)\n",
    "    mc = c.most_common()[0][0]\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        if len(list(str(z[i]))) == mc:\n",
    "            wind = z[i]\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Cut the data and compare each binarized piece, using a metric\n",
    "    # To see which pieces are similar\n",
    "    a = int(np.floor(len(sig)/wind))\n",
    "    vals = np.arange(0, a*wind, wind)\n",
    "\n",
    "    dic = {}\n",
    "    dic2 = {}\n",
    "    for i in vals:\n",
    "        out = []\n",
    "        out2 = []\n",
    "\n",
    "        A = sig[i:(i+wind)]\n",
    "        A = [int(r) for r in A]\n",
    "        for j in vals:\n",
    "            # import required libraries\n",
    "            B = sig[j:(j+wind)]\n",
    "            B = [int(r) for r in B]\n",
    "\n",
    "            # cosine similarity\n",
    "            #cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "            #out.append(cosine)\n",
    "\n",
    "            # absolute error\n",
    "            err = [np.abs(A[q]-B[q]) for q in range(len(A))]\n",
    "            out.append(err)\n",
    "\n",
    "            # height value\n",
    "            height = np.max(A)\n",
    "            out2.append(height)\n",
    "\n",
    "        dic[i] = out\n",
    "        dic2[i] = out2\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # tot = np.array(tot)\n",
    "    # tot.shape\n",
    "    # import seaborn as sns\n",
    "    # sns.heatmap(data=tot, annot=True)\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Take the mean across similarity measures for each temporal piece\n",
    "    zz = [np.mean(dic[vals[i]]) for i in range(len(dic))]\n",
    "    zz2 = [np.mean(dic2[vals[i]]) for i in range(len(dic2))]\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Find the temporal piece that are most similar to all other pieces\n",
    "    select_crit = 2\n",
    "\n",
    "    if select_crit == 0:\n",
    "        # No height restriction\n",
    "        # ind_piece = np.argmax(zz)  # cosine similarity\n",
    "        ind_piece = np.argmin(zz)  # absolute error\n",
    "    elif select_crit == 1:\n",
    "        # With height restriction\n",
    "        # use height as threshold for zz\n",
    "        marg = 1*np.std(sig)\n",
    "        thresh = np.max(zz2) - marg\n",
    "        print('thresh : ', thresh)\n",
    "        # OU\n",
    "        # thresh = np.min(zz2)\n",
    "        store = [zz for i in range(len(zz)) if zz2[i] > thresh]\n",
    "\n",
    "        # the select index using metric (cosine, absolute error)\n",
    "        # ind_piece = np.argmax(store)  # cosine similarity\n",
    "        ind_piece = np.argmin(store)  # absolute error\n",
    "    elif select_crit == 2:\n",
    "        # Rien a faire avec metrics, choisi par rapport la hauteur\n",
    "        ind_piece = np.argmax(zz2)\n",
    "    \n",
    "    # Plot selected temporal piece, with data\n",
    "    st = vals[ind_piece]\n",
    "    endd = st+wind\n",
    "    sig2 = sig[st:endd]\n",
    "\n",
    "    if plotORnot == 1:\n",
    "        plt.plot(sig)\n",
    "        plt.plot(np.arange(st,endd), sig2)\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.xlabel(\"Data points\")\n",
    "        plt.title(\"Non-downsampled : Bird call and call signature\")\n",
    "        plt.show()\n",
    "\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Downsampling\n",
    "    v = downsample_sig(sig2, timesteps)\n",
    "\n",
    "    # ------------------------------------\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9632b8",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd54e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries_exp = put_timeseries_trialdata_into_pandas(varr, filemarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634bf435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[rot] :  (86006, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86001</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.453208</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699437</td>\n",
       "      <td>-1.514817</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>-4.719625</td>\n",
       "      <td>0.301392</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>0.097727</td>\n",
       "      <td>0.850650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.927108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86002</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.391906</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.461697</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>-4.719593</td>\n",
       "      <td>0.333524</td>\n",
       "      <td>-0.082369</td>\n",
       "      <td>0.097661</td>\n",
       "      <td>0.902199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.917930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86003</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>16.500001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.313130</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.382446</td>\n",
       "      <td>0.807940</td>\n",
       "      <td>-4.719292</td>\n",
       "      <td>0.356304</td>\n",
       "      <td>-0.086946</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.953649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86004</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>16.599999</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.221655</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.284506</td>\n",
       "      <td>0.807370</td>\n",
       "      <td>-4.719829</td>\n",
       "      <td>0.362870</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>0.090162</td>\n",
       "      <td>1.004994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.897957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86005</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.125844</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.179229</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>-4.719719</td>\n",
       "      <td>0.364322</td>\n",
       "      <td>-0.088796</td>\n",
       "      <td>0.089878</td>\n",
       "      <td>1.056230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.887165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp       time  res_type  SIGCOM_ax0  \\\n",
       "86001     17.0  20.0 -1.0  0.0  163.0  16.300000       7.0   -1.453208   \n",
       "86002     17.0  20.0 -1.0  0.0  164.0  16.400000       7.0   -1.391906   \n",
       "86003     17.0  20.0 -1.0  0.0  165.0  16.500001       7.0   -1.313130   \n",
       "86004     17.0  20.0 -1.0  0.0  166.0  16.599999       7.0   -1.221655   \n",
       "86005     17.0  20.0 -1.0  0.0  167.0  16.700000       7.0   -1.125844   \n",
       "\n",
       "       SIGCOM_ax1  SIGCOM_ax2   SIG_ax0   SIG_ax1   SIG_ax2   JOY_ax0  \\\n",
       "86001    0.804159   -4.699437 -1.514817  0.803347 -4.719625  0.301392   \n",
       "86002    0.804159   -4.699425 -1.461697  0.803352 -4.719593  0.333524   \n",
       "86003    0.804159   -4.699425 -1.382446  0.807940 -4.719292  0.356304   \n",
       "86004    0.804159   -4.699425 -1.284506  0.807370 -4.719829  0.362870   \n",
       "86005    0.804159   -4.699425 -1.179229  0.805081 -4.719719  0.364322   \n",
       "\n",
       "        JOY_ax1   JOY_ax2  NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "86001 -0.082375  0.097727   0.850650        0.0  -4.927108  \n",
       "86002 -0.082369  0.097661   0.902199        0.0  -4.917930  \n",
       "86003 -0.086946  0.096315   0.953649        0.0  -4.908213  \n",
       "86004 -0.089799  0.090162   1.004994        0.0  -4.897957  \n",
       "86005 -0.088796  0.089878   1.056230        0.0  -4.887165  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[rot] : ', df_timeseries_exp['rot'].shape)\n",
    "df_timeseries_exp['rot'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec94790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[trans] :  (43754, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43749</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.3868</td>\n",
       "      <td>51.1975</td>\n",
       "      <td>-1.3760</td>\n",
       "      <td>68.5344</td>\n",
       "      <td>31.0360</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0293</td>\n",
       "      <td>-0.1693</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43750</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.7618</td>\n",
       "      <td>51.4284</td>\n",
       "      <td>-1.3559</td>\n",
       "      <td>68.9152</td>\n",
       "      <td>31.1329</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0294</td>\n",
       "      <td>-0.1693</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43751</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.1368</td>\n",
       "      <td>51.6561</td>\n",
       "      <td>-1.3360</td>\n",
       "      <td>69.2979</td>\n",
       "      <td>31.2316</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>-0.1381</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43752</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.5118</td>\n",
       "      <td>51.7815</td>\n",
       "      <td>-1.3128</td>\n",
       "      <td>69.6821</td>\n",
       "      <td>31.3353</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>-0.0897</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43753</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.7139</td>\n",
       "      <td>51.7700</td>\n",
       "      <td>-1.2916</td>\n",
       "      <td>70.0518</td>\n",
       "      <td>31.4619</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>-0.0810</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp  time  res_type  SIGCOM_ax0  SIGCOM_ax1  \\\n",
       "43749     13.0  16.0  1.0  1.0  197.0  19.7       7.0         0.0     69.3868   \n",
       "43750     13.0  16.0  1.0  1.0  198.0  19.8       7.0         0.0     69.7618   \n",
       "43751     13.0  16.0  1.0  1.0  199.0  19.9       7.0         0.0     70.1368   \n",
       "43752     13.0  16.0  1.0  1.0  200.0  20.0       7.0         0.0     70.5118   \n",
       "43753     13.0  16.0  1.0  1.0  201.0  20.1       7.0         0.0     70.7139   \n",
       "\n",
       "       SIGCOM_ax2  SIG_ax0  SIG_ax1  SIG_ax2  JOY_ax0  JOY_ax1  JOY_ax2  \\\n",
       "43749     51.1975  -1.3760  68.5344  31.0360  -0.0412  -0.0293  -0.1693   \n",
       "43750     51.4284  -1.3559  68.9152  31.1329  -0.0412  -0.0294  -0.1693   \n",
       "43751     51.6561  -1.3360  69.2979  31.2316  -0.0412  -0.0272  -0.1381   \n",
       "43752     51.7815  -1.3128  69.6821  31.3353  -0.0408  -0.0273  -0.0897   \n",
       "43753     51.7700  -1.2916  70.0518  31.4619  -0.0408  -0.0273  -0.0810   \n",
       "\n",
       "       NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "43749       3.75       -0.0        0.0  \n",
       "43750       3.75       -0.0        0.0  \n",
       "43751       3.75       -0.0        0.0  \n",
       "43752       3.75       -0.0        0.0  \n",
       "43753       0.00        0.0        0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[trans] : ', df_timeseries_exp['trans'].shape)\n",
    "df_timeseries_exp['trans'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b9639",
   "metadata": {},
   "source": [
    "## Reconfirming joystick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "explist = ['rot', 'trans']\n",
    "for exp in explist:\n",
    "    # Cabin movement : each figure contains all trial data per SUBJECT\n",
    "    sub = df_timeseries_exp['%s' % (exp)].subject.value_counts().index.to_numpy()\n",
    "    for s in sub:\n",
    "        fig, axx = plt.subplots(nrows=1, ncols=1)\n",
    "        tr = df_timeseries_exp['%s' % (exp)].tr[(df_timeseries_exp['%s' % (exp)].subject == s)].value_counts().index.to_numpy()\n",
    "        for i in tr:\n",
    "            joy0 = df_timeseries_exp['%s' % (exp)].JOY_ax0[(df_timeseries_exp['%s' % (exp)].subject == s) & (df_timeseries_exp['%s' % (exp)].tr == i)].to_numpy()\n",
    "            joy1 = df_timeseries_exp['%s' % (exp)].JOY_ax1[(df_timeseries_exp['%s' % (exp)].subject == s) & (df_timeseries_exp['%s' % (exp)].tr == i)].to_numpy()\n",
    "            joy2 = df_timeseries_exp['%s' % (exp)].JOY_ax2[(df_timeseries_exp['%s' % (exp)].subject == s) & (df_timeseries_exp['%s' % (exp)].tr == i)].to_numpy()\n",
    "            plt.plot(joy0, 'r')\n",
    "            plt.plot(joy1, 'g')\n",
    "            plt.plot(joy2, 'b')\n",
    "            axx.set_title(\"%s, Subject %d : Joystick motion\" % (exp, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f95d2",
   "metadata": {},
   "source": [
    "## Verifier des marquers : Plot des marquers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos, label=\"num\")\n",
    "sns.lineplot(data=df_feat.vel, label=\"num\", color='red')\n",
    "sns.lineplot(data=df_feat.acc, label=\"num\", color='green')\n",
    "plt.title(\"position, velocity, acceleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_sl0, label=\"position\")\n",
    "sns.lineplot(data=df_feat.vel_sl0, label=\"velocity\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_sl0, label=\"acceleration\", color='green')\n",
    "plt.title(\"wavelet sublevel : position, velocity, accleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44874b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_spec, label=\"pos_spec\")\n",
    "sns.lineplot(data=df_feat.vel_spec, label=\"vel_spec\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_spec, label=\"acc_spec\", color='green')\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_cwt, label=\"pos_cwt\")\n",
    "sns.lineplot(data=df_feat.vel_cwt, label=\"vel_cwt\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_cwt, label=\"acc_cwt\", color='green')\n",
    "plt.title(\"Continuous Wavelet Transform\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5c328",
   "metadata": {},
   "source": [
    "# Pipeline for creating X and y for each data condition : Save matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "\n",
    "# ----------------\n",
    "\n",
    "df = get_df(ax_val, ss_val)\n",
    "\n",
    "feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "# Elasped time for feature processing :  715.9615476131439\n",
    "\n",
    "del df\n",
    "\n",
    "# ----------------\n",
    "\n",
    "if exists(\"X.pkl\") == False:\n",
    "    print('Creation des marquers')\n",
    "    X = feature_creation_preprocessing(feat0, t_feat0)\n",
    "    del feat0, t_feat0\n",
    "    save_dat_pickle(X, file_name=\"X.pkl\")\n",
    "else:\n",
    "    print('Load des marquers')\n",
    "    X = load_dat_pickle(file_name=\"X.pkl\")\n",
    "\n",
    "df_org = X\n",
    "\n",
    "del X\n",
    "\n",
    "# ----------------\n",
    "\n",
    "\n",
    "\n",
    "# ----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967aa1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f909eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81abc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmation : exp= rot , ax_val= all , ss_val= all\n",
      "Creation des marquers\n",
      "num de samples avant dropna :  442\n",
      "num de samples apres dropna :  442\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cols2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ax_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m ss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m df_test2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_main_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_timeseries_exp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mrun_main_script\u001b[0;34m(exp, ax_val, ss_val, df_timeseries_exp)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_feat.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreation des marquers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     df_feat \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_creation_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_feat0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m feat0, t_feat0\n\u001b[1;32m     18\u001b[0m     save_dat_pickle(df_feat, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_feat.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mfeature_creation_preprocessing\u001b[0;34m(feat0, t_feat0)\u001b[0m\n\u001b[1;32m    160\u001b[0m col12 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(km_cl2)\n\u001b[1;32m    161\u001b[0m col13 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(km_cl3)\n\u001b[0;32m--> 163\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([col0, col1, \u001b[43mcols2\u001b[49m, cols3, cols4, cols5, col6, col7, col8, col9, col10, col11, col12, col13], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m col0, col1, col2, cols3, cols4, cols5, col6, col7, col8, col9, col10, col11, col12, col13\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Interpolate toutes les vectors\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEiUlEQVR4nO3VMQEAIAzAMMC/5+ECjiYK+nXPzAKAivM7AABeMj4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIueF8BVm9xhwpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEiUlEQVR4nO3VMQEAIAzAMMC/5+ECjiYK+nXPzAKAivM7AABeMj4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIueF8BVm9xhwpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEiUlEQVR4nO3VMQEAIAzAMMC/5+ECjiYK+nXPzAKAivM7AABeMj4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIMT4AUowPgBTjAyDF+ABIueF8BVm9xhwpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADP0lEQVR4nO3asQ3DAAgAwWBl/5FNJnAkN+/Cdy0N1YuC2d0PAI3j6QUA3kR0AUKiCxASXYCQ6AKEvv+GM6fXBoCbdo+5mrl0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKzu0/vAPAaLl2AkOgChEQXICS6ACHRBQiJLkDoB4dOC8lIhesIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADP0lEQVR4nO3asQ3DAAgAwWBl/5FNJnAkN+/Cdy0N1YuC2d0PAI3j6QUA3kR0AUKiCxASXYCQ6AKEvv+GM6fXBoCbdo+5mrl0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKzu0/vAPAaLl2AkOgChEQXICS6ACHRBQiJLkDoB4dOC8lIhesIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADP0lEQVR4nO3asQ3DAAgAwWBl/5FNJnAkN+/Cdy0N1YuC2d0PAI3j6QUA3kR0AUKiCxASXYCQ6AKEvv+GM6fXBoCbdo+5mrl0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKzu0/vAPAaLl2AkOgChEQXICS6ACHRBQiJLkDoB4dOC8lIhesIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "df_test2 = run_main_script(exp, ax_val, ss_val, df_timeseries_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa7faa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_script(exp, ax_val, ss_val, df_timeseries_exp):\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    df = get_df(ax_val, ss_val, df_timeseries_exp)\n",
    "\n",
    "    feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "    # Elasped time for feature processing :  715.9615476131439\n",
    "\n",
    "    del df\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    if exists(\"df_feat.pkl\") == False:\n",
    "        print('Creation des marquers')\n",
    "        df_feat = feature_creation_preprocessing(feat0, t_feat0)\n",
    "        del feat0, t_feat0\n",
    "        save_dat_pickle(df_feat, file_name=\"df_feat.pkl\")\n",
    "    else:\n",
    "        print('Load des marquers')\n",
    "        df_feat = load_dat_pickle(file_name=\"df_feat.pkl\")\n",
    "        \n",
    "    df_org = df_feat\n",
    "    \n",
    "    del df_feat\n",
    "    # ----------------\n",
    "\n",
    "    col_list = ['num', 'pos', 'vel', 'acc', 'pos_nc', 'vel_nc', 'acc_nc', 'pos_sl0', 'pos_sl1', 'pos_sl2', \n",
    "                'pos_sl3', 'pos_sl4', 'vel_sl0', 'vel_sl1', 'vel_sl2', 'vel_sl3', 'vel_sl4', 'acc_sl0', \n",
    "                'acc_sl1', 'acc_sl2', 'acc_sl3', 'acc_sl4', 'pos_spec', 'vel_spec', 'acc_spec', 'pos_cwt', \n",
    "                'vel_cwt', 'acc_cwt', 'kmeans']\n",
    "    df_org = pandas_rename_columns(df_org, col_list)\n",
    "    df_org = df_org.reset_index(drop=True)  # reset index : delete the old index column\n",
    "    # Gardez df_org\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    y_alllabel = [y1_feat0, y2_feat0, y3_feat0]\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Loop over the different y labels\n",
    "    for ynum in range(1):    #range(len(y_alllabel)):\n",
    "        print('ynum : ', ynum)\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Select y\n",
    "        y_label = y_alllabel[ynum]\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # Ajoutez le column de label à la fin\n",
    "        y_pd = pd.Series(make_a_properlist(y_label))\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        df_feat = pd.concat([df_org, y_pd], axis=1)\n",
    "        df_feat = df_feat.rename({0: 'y'}, axis=1)\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        # Balancez des class/labels: pad\n",
    "        # Pad data 2 Make Classes Equivalent\n",
    "        df_test2 = pad_data_2makeclasses_equivalent(df_feat)\n",
    "        del df_feat\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # classification for 3 partitions of feature data per 8 classifiers\n",
    "        # classify(df_test2, ynum)\n",
    "        \n",
    "        # del df_test2\n",
    "\n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4e030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31bae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d19198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073adc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9e407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec7c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e0483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69cb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d2db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa1fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaef08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0b13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9dfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81258a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d6c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7e6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba908ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5611a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d67eba11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45aa82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45865137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488f51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d04ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcf3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb2ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623202ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm AGAIN that the joystick signal is correct.\n",
    "# Look at the joystick features\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "for i in range(len(feat0)):\n",
    "    fig.append_trace(go.Scatter(x=t_feat0[i], y=feat0[i],), row=1, col=1)\n",
    "\n",
    "fig.update_layout(height=600, width=600, title_text=\"Stacked Subplots\")\n",
    "fig.show()\n",
    "\n",
    "# We check the signal for Classification because we create the position feature in \n",
    "# the function above by selecting the joystick movement on the axis in which there\n",
    "# was the stimulus.\n",
    "\n",
    "# Explination for why there is not a joystick signal in each field of feat0:\n",
    "# In previous steps, we time-locked all \"start and stop\" indexing with the time\n",
    "# vector in the experiment. \n",
    "# And we confirmed using the movement of the simulator cabin using the cabin \n",
    "# position and the joystick, using both cabin and joystick direction and amplitude.\n",
    "\n",
    "# At the moment no good reason why the data is not correct - we dropped all avnormal \n",
    "# data in step B (s1_removeBADtrials_savedata)\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106592a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"df_feat.pkl\") == False:\n",
    "    print('Creation des marquers')\n",
    "    df_feat, rm_ind = feature_creation_preprocessing(feat0, t_feat0)\n",
    "    del feat0, t_feat0\n",
    "    save_dat_pickle(df_feat, file_name=\"df_feat.pkl\")\n",
    "    save_dat_pickle(rm_ind, file_name=\"rm_ind.pkl\")\n",
    "else:\n",
    "    print('Load des marquers')\n",
    "    df_feat = load_dat_pickle(file_name=\"df_feat.pkl\")\n",
    "    rm_ind = load_dat_pickle(file_name=\"rm_ind.pkl\")\n",
    "\n",
    "df_org = df_feat\n",
    "\n",
    "del df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecf6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad53792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d589d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c4073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281596fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181f620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78573b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e91721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc9f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e91c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fe411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Manual runs\n",
    "# Justification for not doing a loop : Can not do a loop because the computer stops or it stops \n",
    "# for weird reasons.  Have to run each manually.\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "# -------------\n",
    "# DONE\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
