{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c481e0d3",
   "metadata": {},
   "source": [
    "# Mouvements de joystick analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76484add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File manipulation\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# Loading and saving data\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Scaling signal\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# Signal processing\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "# Wavelets\n",
    "import pywt\n",
    "from pywt import wavedec\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import svm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b7450",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765988a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_timeseries_trialdata_into_pandas(varr):\n",
    "    \n",
    "    df_timeseries_exp = {}\n",
    "\n",
    "    # 1) Load exp : put the experiment that you wish to run\n",
    "    for exp in range(2):  # 0=rotation, 1=translation\n",
    "        # print('exp : ', exp)\n",
    "\n",
    "        if exp == 0:\n",
    "            # Rotational data - 18 participants\n",
    "            varr['which_exp'] = 'rot'\n",
    "            varr['anom'] = 'RO', 'PI', 'YA'\n",
    "            \n",
    "            # Time series data per subject per trial\n",
    "            file_name1 = \"%s/rotdat2.pkl\" % (varr['main_path2'])\n",
    "\n",
    "            # Load data experimental preprocessed data matrix\n",
    "            file_name2 = \"%s/rot_Xexp.pkl\" % (varr['main_path3'])\n",
    "        elif exp == 1:\n",
    "            # Translational data - 14 participants\n",
    "            varr['which_exp'] = 'trans'\n",
    "            varr['anom'] = 'LR', 'FB', 'UD'\n",
    "\n",
    "            # Time series data per subject per trial\n",
    "            file_name1 = \"%s/transdat2.pkl\" % (varr['main_path2'])\n",
    "\n",
    "            # Experimental preprocessed : a scalar metric per subject per trial\n",
    "            file_name2 = \"%s/trans_Xexp.pkl\" % (varr['main_path3'])\n",
    "\n",
    "        open_file = open(file_name1, \"rb\")\n",
    "        dat = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        open_file = open(file_name2, \"rb\")\n",
    "        X = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "        \n",
    "        num_of_subs = len(X)    # OR dat.shape[0]\n",
    "\n",
    "        Xsub = []\n",
    "        \n",
    "        for s in range(num_of_subs):\n",
    "            # print('s : ', s)\n",
    "            \n",
    "            num_of_tr = len(dat[s][0])  # OR X[0].shape[0]\n",
    "\n",
    "            for tr in range(num_of_tr):\n",
    "                # print('tr : ', tr)\n",
    "                \n",
    "                # time series dataFrame (ensure vectors are a column): \n",
    "                num_dp = len(dat[s][4][tr][:,0])\n",
    "\n",
    "                subject = s*np.ones((num_dp,1))\n",
    "                trial = tr*np.ones((num_dp,1))\n",
    "                ss = dat[s][0][tr]*np.ones((num_dp,1))\n",
    "                ax = dat[s][1][tr]*np.ones((num_dp,1))\n",
    "\n",
    "                dp = np.reshape(list(range(num_dp)), (num_dp,1))\n",
    "                time = np.reshape(dat[s][9][tr], (num_dp,1))\n",
    "\n",
    "                res_type = X[s][:,5][tr]*np.ones((num_dp,1))\n",
    "                \n",
    "                outSIGCOM_ax0 = np.reshape(dat[s][4][tr][:,0], (num_dp,1))\n",
    "                outSIGCOM_ax1 = np.reshape(dat[s][4][tr][:,1], (num_dp,1))\n",
    "                outSIGCOM_ax2 = np.reshape(dat[s][4][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outSIG_ax0 = np.reshape(dat[s][5][tr][:,0], (num_dp,1))\n",
    "                outSIG_ax1 = np.reshape(dat[s][5][tr][:,1], (num_dp,1))\n",
    "                outSIG_ax2 = np.reshape(dat[s][5][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outJOY_ax0 = np.reshape(dat[s][6][tr][:,0], (num_dp,1))\n",
    "                outJOY_ax1 = np.reshape(dat[s][6][tr][:,1], (num_dp,1))\n",
    "                outJOY_ax2 = np.reshape(dat[s][7][tr][:,2], (num_dp,1))\n",
    "\n",
    "                outNOISE_ax0 = np.reshape(dat[s][7][tr][:,0], (num_dp,1))\n",
    "                outNOISE_ax1 = np.reshape(dat[s][7][tr][:,1], (num_dp,1))\n",
    "                outNOISE_ax2 = np.reshape(dat[s][7][tr][:,2], (num_dp,1))\n",
    "                \n",
    "                X_row = np.ravel(subject), np.ravel(trial), np.ravel(ss), np.ravel(ax), np.ravel(dp), np.ravel(time), np.ravel(res_type), np.ravel(outSIGCOM_ax0), np.ravel(outSIGCOM_ax1), np.ravel(outSIGCOM_ax2), np.ravel(outSIG_ax0), np.ravel(outSIG_ax1), np.ravel(outSIG_ax2), np.ravel(outJOY_ax0), np.ravel(outJOY_ax1), np.ravel(outJOY_ax2), np.ravel(outNOISE_ax0), np.ravel(outNOISE_ax1), np.ravel(outNOISE_ax2),\n",
    "                Xtr = np.transpose(X_row)\n",
    "                # print('shape of Xtr : ', Xtr.shape)\n",
    "                \n",
    "                # concatenate accumulated matrix with new\n",
    "                if s == 0 and tr == 0:\n",
    "                    Xsub = Xtr\n",
    "                else:\n",
    "                    Xsub = np.concatenate((Xsub, Xtr), axis=0)\n",
    "                \n",
    "                # print('len of Xsub : ', len(Xsub))\n",
    "                \n",
    "        \n",
    "        columns = ['subject', 'tr', 'ss', 'ax', 'dp', 'time', 'res_type', 'SIGCOM_ax0', 'SIGCOM_ax1', 'SIGCOM_ax2', 'SIG_ax0', 'SIG_ax1', 'SIG_ax2', 'JOY_ax0', 'JOY_ax1', 'JOY_ax2', 'NOISE_ax0', 'NOISE_ax1', 'NOISE_ax2']\n",
    "        out1 = np.reshape(Xsub, (len(Xsub), len(columns)))\n",
    "        \n",
    "        df = pd.DataFrame(out1, columns=columns)\n",
    "\n",
    "        df_timeseries_exp[varr['which_exp']] = df\n",
    "\n",
    "    return df_timeseries_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab92ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nan can not process strings,  use this.\n",
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ac3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dropna_python(df):\n",
    "    # Python\n",
    "    col_names = list(df.columns.values)\n",
    "    # OR\n",
    "    # col_names = list(df.columns)\n",
    "    \n",
    "    df = df.to_numpy()\n",
    "    df = np.array(df)\n",
    "    # print('size of df : ', df.shape)\n",
    "    data = []\n",
    "    num_of_cols = df.shape[1]\n",
    "    for i in range(df.shape[0]):\n",
    "        row_vec = df[i,:]\n",
    "        \n",
    "        out = [isnan(row_vec[i]) for i in range(len(row_vec))]\n",
    "        # OR\n",
    "        # out = []\n",
    "        # for i in range(len(row_vec)):\n",
    "            # #print('row_vec[i]', row_vec[i])\n",
    "            # out.append(isnan(row_vec[i]))\n",
    "        \n",
    "        out = make_a_properlist(out)  # for dataframes with nested arrays\n",
    "        \n",
    "        if any(out) == False:\n",
    "            data.append(df[i,:])\n",
    "    \n",
    "    num_of_rows = len(data)\n",
    "    data0 = np.reshape(data, (num_of_rows, num_of_cols))\n",
    "    \n",
    "    df_new = pd.DataFrame(data=data0, columns=col_names)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb25354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty(vec):\n",
    "    \n",
    "    #if not any(vec):\n",
    "    \n",
    "    # OR\n",
    "    \n",
    "    #if len(vec) < 1:  # OR\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    if vec.shape[0] == 0:\n",
    "        # print('yes, the array is empty')\n",
    "        out = True\n",
    "    else:\n",
    "        # print('no, the array is not empty')\n",
    "        out = False\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0187ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_properlist(vec):\n",
    "    \n",
    "    out = []\n",
    "    for i in range(len(vec)):\n",
    "        out = out + [np.ravel(vec[i])]\n",
    "        \n",
    "    if is_empty(out) == False:\n",
    "        vecout = np.concatenate(out).ravel().tolist()\n",
    "    else:\n",
    "        vecout = list(np.ravel(out))\n",
    "    \n",
    "    return vecout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d5afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numderiv(x, t):\n",
    "    # Created by Jamilah Foucher, Février 01, 2021\n",
    "\n",
    "    # Purpose: Numerical derivative\n",
    "    # \n",
    "    # Input VARIABLES:\n",
    "    # (1) x is a vector in which you want the numerical derivative\n",
    "    # \n",
    "    # (2) t is a time vector\n",
    "\n",
    "    # Output VARIABLES:\n",
    "    # (1) dx is a vector of the numerical derivative\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    if len(x) > 4:\n",
    "        dx0 = np.ones(len(x))\n",
    "        for i in range(len(x)-1):\n",
    "            dx0[i] = ( x[i+1] - x[i] ) / ( t[i+1] - t[i] )\n",
    "        dx = dx0[0:len(dx0)-2], dx0[len(dx0)-2], dx0[len(dx0)-2] \n",
    "    dx = make_a_properlist(dx)\n",
    "       \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9350c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(d, reverse = False):\n",
    "    return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03474bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation_of_kstest(statistic, pvalue):\n",
    "     # or np.isclose(pvalue, 1, atol=0.1)\n",
    "    if np.isclose(statistic, 0, atol=0.01):    # default atol=1e-08\n",
    "        out = 1 # result is parametric/normal\n",
    "    else:\n",
    "        out = 0  # result is non-parametric/non-normal\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbe27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution_feature_data(feat, plotORnot):\n",
    "    # Make distribution of feature parametric per trial\n",
    "    \n",
    "    \n",
    "    # Check if the feature has a normal distribution\n",
    "    \n",
    "    statistic, pvalue = stats.kstest(feat, 'norm')\n",
    "    # print('statistic : ', statistic, ', pvalue : ', pvalue)\n",
    "    result = interpretation_of_kstest(statistic, pvalue)\n",
    "    # print('result : ', result)\n",
    "    \n",
    "    # Initialize : use feature if it has normal distribution OR if normal distribution can not be found\n",
    "    norm_feat = feat\n",
    "    \n",
    "    if result == 0 and np.isnan(feat).any() == False:\n",
    "        # Does not work for negative values, so shift up a little bit above zero\n",
    "        pos_shift = feat - (np.min(feat)-0.000001)\n",
    "        # print('min val : ', np.min(pos_shift))\n",
    "\n",
    "        # ----------------\n",
    "        \n",
    "        normaldist0 = stats.boxcox(pos_shift)\n",
    "        # ----------------\n",
    "        # OR\n",
    "        # ----------------\n",
    "        # Stack two signals because the functions can not process a single signal alone\n",
    "        X = pos_shift, pos_shift\n",
    "        X = np.transpose(X)\n",
    "        # print('shape of X : ', X.shape)\n",
    "        # ----------------\n",
    "        \n",
    "        # https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html\n",
    "        \n",
    "        # ----------------\n",
    "        bc = PowerTransformer(method='box-cox')\n",
    "        normaldist1 = bc.fit(X).transform(X)\n",
    "        # ----------------\n",
    "\n",
    "        # ----------------\n",
    "        yj = PowerTransformer(method='yeo-johnson')\n",
    "        normaldist2 = yj.fit(X).transform(X)\n",
    "        # ----------------\n",
    "\n",
    "        # ----------------\n",
    "        \n",
    "        rng = np.random.RandomState(0)\n",
    "        num_of_samps = len(feat)\n",
    "        qt = QuantileTransformer(n_quantiles=num_of_samps, output_distribution='normal', random_state=rng)\n",
    "        normaldist3 = qt.fit(X).transform(X)\n",
    "        # ----------------\n",
    "    \n",
    "        # Some of the distribution transformations do not always work.  Test to see if the test suceeded.\n",
    "        statistic, pvalue = stats.kstest(normaldist0[0], 'norm')\n",
    "        result0 = interpretation_of_kstest(statistic, pvalue)\n",
    "        \n",
    "        statistic, pvalue = stats.kstest(normaldist1[:,0], 'norm')\n",
    "        result1 = interpretation_of_kstest(statistic, pvalue)\n",
    "                                           \n",
    "        statistic, pvalue = stats.kstest(normaldist2[:,0], 'norm')\n",
    "        result2 = interpretation_of_kstest(statistic, pvalue)\n",
    "        \n",
    "        statistic, pvalue = stats.kstest(normaldist3[:,0], 'norm')\n",
    "        result3 = interpretation_of_kstest(statistic, pvalue)\n",
    "        \n",
    "        allres = [result0, result1, result2, result3]\n",
    "        # print('allres : ', allres)\n",
    "                                           \n",
    "        all_dat = normaldist0[0], list(normaldist1[:,0]), list(normaldist2[:,0]), list(normaldist3[:,0])\n",
    "        all_dat = np.transpose(all_dat)\n",
    "        \n",
    "        for i in range(len(allres)):\n",
    "            if allres[i] == 1:\n",
    "                norm_feat = all_dat[:,i]\n",
    "                break\n",
    "    \n",
    "        # if plotORnot == 1:\n",
    "            # # histogram\n",
    "            # print('shape of all_dat : ', all_dat.shape)\n",
    "            # dfout = pd.DataFrame(data=all_dat)\n",
    "\n",
    "            \n",
    "            # fig, ax=plt.subplots(4,1)\n",
    "            # sns.distplot(dfout[0], ax=ax[0], bins=30, label=\"stats\") \n",
    "            # sns.distplot(dfout[1], ax=ax[1], bins=30, label='bc')\n",
    "            # sns.distplot(dfout[2], ax=ax[2], bins=30, label='yj')\n",
    "            # sns.distplot(dfout[3], ax=ax[3], bins=30, label='qt')\n",
    "    \n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46cb8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_intercurrentpt_makeshortSIGlong_interp1d(shortSIG, longSIG):\n",
    "\n",
    "    x = np.linspace(shortSIG[0], len(shortSIG), num=len(shortSIG), endpoint=True)\n",
    "    y = shortSIG\n",
    "    # print('x : ', x)\n",
    "\n",
    "\n",
    "    # -------------\n",
    "    kind = 'linear'\n",
    "    # kind : Specifies the kind of interpolation as a string or as an integer specifying \n",
    "    # the order of the spline interpolator to use. The string has to be one of ‘linear’, ‘nearest’, \n",
    "    # ‘nearest-up’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘previous’, or ‘next’. ‘zero’, ‘slinear’, \n",
    "    # ‘quadratic’ and ‘cubic’ refer to a spline interpolation of zeroth, first, second or third order; \n",
    "    # ‘previous’ and ‘next’ simply return the previous or next value of the point; ‘nearest-up’ and \n",
    "    # ‘nearest’ differ when interpolating half-integers (e.g. 0.5, 1.5) in that ‘nearest-up’ rounds \n",
    "    # up and ‘nearest’ rounds down. Default is ‘linear’.\n",
    "\n",
    "    if kind == 'linear':\n",
    "        f = interp1d(x, y)\n",
    "    elif kind == 'cubic':\n",
    "        f = interp1d(x, y, kind='cubic')\n",
    "    # -------------\n",
    "\n",
    "    xnew = np.linspace(shortSIG[0], len(shortSIG), num=len(longSIG), endpoint=True)\n",
    "    # print('xnew : ', xnew)\n",
    "\n",
    "    siglong = f(xnew)\n",
    "\n",
    "    return siglong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c40f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# level : the number of levels to decompose the time signal, le nombre des marquers par signale\n",
    "def tsig_2_discrete_wavelet_transform(sig, waveletname, level, plotORnot):\n",
    "\n",
    "    # On peut calculater dans deux façons: 0) dwt en boucle and then idwt, 1) wavedec et waverec\n",
    "    # Mais le deux ne donnent pas le meme reponses, wavedec et waverec semble plus raisonable.\n",
    "\n",
    "    facon_a_faire = 1\n",
    "\n",
    "\n",
    "    if facon_a_faire == 0:\n",
    "        coef0 = {}\n",
    "        coef1 = {}\n",
    "        for k in range(level):\n",
    "            (coef0[k], coef1[k]) = pywt.dwt(sig, waveletname)\n",
    "            sig = coef0[k]\n",
    "            # first coefficient (coef0) is the last level (it has less data points than coef1), feed coef0\n",
    "            # (the lowest decomposed signal) back into dwt to decompose the signal further\n",
    "\n",
    "        if plotORnot == 1:\n",
    "            fig, axx = plt.subplots(nrows=level, ncols=2, figsize=(5,5))\n",
    "            axx[0,0].set_title(\"coef0\")  # Approximation coefficients\n",
    "            axx[0,1].set_title(\"coef1\")  # Detail coefficients\n",
    "            for k in range(level):\n",
    "                axx[k,0].plot(coef0[k], 'r') # output of the low pass filter (averaging filter) of the DWT\n",
    "                axx[k,1].plot(coef1[k], 'g') # output of the high pass filter (difference filter) of the DWT\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        coeff = coef1\n",
    "\n",
    "    elif facon_a_faire == 1:\n",
    "\n",
    "        coeff = wavedec(sig, waveletname, level=level)\n",
    "\n",
    "        if plotORnot == 1:\n",
    "            fig, axx = plt.subplots(nrows=level, ncols=1, figsize=(5,5))\n",
    "            axx[0].set_title(\"coef\")  # Pas certain si c'est coef0 ou coef1\n",
    "            for k in range(level):\n",
    "                axx[k].plot(coeff[k], 'r') # output of the low pass filter (averaging filter) of the DWT\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    coeff_interp = []\n",
    "    for i in range(level):\n",
    "        # Interpolate toutes les signales dans coeff avoir le meme taille que sig \n",
    "        siglong = linear_intercurrentpt_makeshortSIGlong_interp1d(coeff[i], sig)\n",
    "\n",
    "        # -----------------------------------\n",
    "\n",
    "        # Normalize img_flatten values: on ne veut pas que des valeurs sont pres de zero\n",
    "        siglong1 = scale_feature_data(siglong, plotORnot=plotORnot)\n",
    "\n",
    "        # -----------------------------------\n",
    "\n",
    "        coeff_interp.append(siglong1)\n",
    "\n",
    "\n",
    "    return coeff_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a06761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsig_2_spectrogram(sig, fs, nfft, noverlap, plotORnot):\n",
    "\n",
    "    # -----------------------------------\n",
    "    fig,ax = plt.subplots(1)\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    # spectrum2D array : Columns are the periodograms of successive segments.\n",
    "    # freqs1-D array : The frequencies corresponding to the rows in spectrum.\n",
    "    # t1-D array : The times corresponding to midpoints of segments (i.e., the columns in spectrum).\n",
    "    # imAxesImage : The image created by imshow containing the spectrogram.\n",
    "    pxx, freqs, bins, img = ax.specgram(sig, nfft, fs, noverlap=noverlap)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    my_image = 'temp.png'\n",
    "    fig.savefig(my_image)\n",
    "    fname = os.path.abspath(os.getcwd()) + \"/\" +  my_image\n",
    "\n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)  \n",
    "\n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)         # PIL: img is not in array form, it is a PIL.PngImagePlugin.PngImageFile \n",
    "\n",
    "    # Plot raw pixel data\n",
    "    # plt.imshow(img)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Flatten im\n",
    "    dlen = len(sig)\n",
    "    # print('dlen: ', dlen)\n",
    "\n",
    "    num_px = int(np.floor(np.sqrt(dlen)))\n",
    "    # print('num_px: ', num_px)\n",
    "\n",
    "    rgb_image = img.convert('RGB')\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(num_px), int(num_px)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "\n",
    "    # Convert image to an array\n",
    "    image = np.array(img3)\n",
    "\n",
    "    type_out = '2D'\n",
    "    # Transformer l'image à 2D ou 3D\n",
    "    if type_out == '2D':  # get array back\n",
    "        # Convert image back to a 2D array\n",
    "        mat_resized = np.mean(image, axis=2)\n",
    "    elif type_out == '3D':  # get image back\n",
    "        mat_resized = image\n",
    "\n",
    "    # Normalize the images to [-1, 1]\n",
    "    mat_resized = (mat_resized - 127.5) / 127.5\n",
    "\n",
    "    # plot raw pixel data\n",
    "    # plt.imshow(mat_resized)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Convert image to an array\n",
    "    image = np.array(mat_resized)\n",
    "    # print('image.shape: ', image.shape)\n",
    "\n",
    "    # Flatten image into a vector\n",
    "    myimage_flatten = np.reshape(np.ravel(image), (num_px*num_px, ), order='F')\n",
    "    # print('myimage_flatten.shape: ', myimage_flatten.shape)\n",
    "\n",
    "    # print('taille de sig : ', len(sig))\n",
    "\n",
    "    # Assurez que l'image taille est le meme que sig : interpolate\n",
    "    img_flatten0 = linear_intercurrentpt_makeshortSIGlong_interp1d(myimage_flatten, sig)\n",
    "    # print('img_flatten.shape: ', img_flatten.shape)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Normalize img_flatten values: on ne veut pas que des valeurs sont pres de zero\n",
    "    img_flatten1 = scale_feature_data(img_flatten0, plotORnot=plotORnot)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    return img_flatten1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsig_2_continuous_wavelet_transform(t_sig, sig, scales, waveletname, plotORnot):\n",
    "    \n",
    "    dt = t_sig[1] - t_sig[0]\n",
    "    \n",
    "    [coefficients, frequencies] = pywt.cwt(sig, scales, waveletname, dt)\n",
    "    \n",
    "    power = (abs(coefficients)) ** 2\n",
    "    period = 1. / frequencies\n",
    "    \n",
    "    levels = [0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8]\n",
    "    contourlevels = np.log2(levels)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    cmap = plt.cm.seismic\n",
    "    \n",
    "    x = t_sig \n",
    "    y = np.log2(period)\n",
    "    z = np.log2(power)\n",
    "    im = ax.contourf(x, y, z, contourlevels, extend='both',cmap=cmap) # matplotlib.contour.QuadContourSet object\n",
    "    \n",
    "    # ax.set_title('Wavelet Transform (Power Spectrum) of signal', fontsize=20)\n",
    "    # ax.set_ylabel('Period', fontsize=18)\n",
    "    # ax.set_xlabel('Time', fontsize=18)\n",
    "    \n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period.min())), np.ceil(np.log2(period.max())))\n",
    "    ax.set_yticks(np.log2(yticks))\n",
    "    ax.set_yticklabels(yticks)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ylim = ax.get_ylim()\n",
    "    # print('ylim: ', ylim)\n",
    "    ax.set_ylim(ylim[0], -1)\n",
    "    \n",
    "    # Colorbar\n",
    "    # cbar_ax = fig.add_axes([0.95, 0.5, 0.03, 0.25])\n",
    "    # fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\")\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    my_image = 'temp.png'\n",
    "    fig.savefig(my_image)\n",
    "    fname = os.path.abspath(os.getcwd()) + \"/\" +  my_image\n",
    "\n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)  \n",
    "\n",
    "    # Convert image to an array:\n",
    "    # Read image \n",
    "    img = Image.open(fname)         # PIL: img is not in array form, it is a PIL.PngImagePlugin.PngImageFile \n",
    "\n",
    "    # Plot raw pixel data\n",
    "    # plt.imshow(img)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Flatten im\n",
    "    dlen = len(sig)\n",
    "    # print('dlen: ', dlen)\n",
    "\n",
    "    num_px = int(np.floor(np.sqrt(dlen)))\n",
    "    # print('num_px: ', num_px)\n",
    "\n",
    "    rgb_image = img.convert('RGB')\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(num_px), int(num_px)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "\n",
    "    # Convert image to an array\n",
    "    image = np.array(img3)\n",
    "\n",
    "    type_out = '2D'\n",
    "    # Transformer l'image à 2D ou 3D\n",
    "    if type_out == '2D':  # get array back\n",
    "        # Convert image back to a 2D array\n",
    "        mat_resized = np.mean(image, axis=2)\n",
    "    elif type_out == '3D':  # get image back\n",
    "        mat_resized = image\n",
    "\n",
    "    # Normalize the images to [-1, 1]\n",
    "    mat_resized = (mat_resized - 127.5) / 127.5\n",
    "\n",
    "    # plot raw pixel data\n",
    "    # plt.imshow(mat_resized)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Convert image to an array\n",
    "    image = np.array(mat_resized)\n",
    "    # print('image.shape: ', image.shape)\n",
    "\n",
    "    # Flatten image into a vector\n",
    "    myimage_flatten = np.reshape(np.ravel(image), (num_px*num_px, ), order='F')\n",
    "    # print('myimage_flatten.shape: ', myimage_flatten.shape)\n",
    "\n",
    "    # print('taille de sig : ', len(sig))\n",
    "\n",
    "    # Assurez que l'image taille est le meme que sig : interpolate\n",
    "    img_flatten0 = linear_intercurrentpt_makeshortSIGlong_interp1d(myimage_flatten, sig)\n",
    "    # print('img_flatten.shape: ', img_flatten.shape)\n",
    "\n",
    "    # -----------------------------------\n",
    "    \n",
    "    # Normalize img_flatten values: on ne veut pas que des valeurs sont pres de zero\n",
    "    img_flatten1 = scale_feature_data(img_flatten0, plotORnot=plotORnot)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    \n",
    "    return img_flatten1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d651ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(ax_val, ss_val):\n",
    "    \n",
    "    if ax_val == 'all' and ss_val == 'all':\n",
    "        # All the data\n",
    "        df_timeseries_exp[exp].head()\n",
    "        df = df_timeseries_exp[exp]\n",
    "    elif ax_val != 'all' and ss_val == 'all':\n",
    "        # Prediction for each axis\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "    elif ax_val == 'all' and ss_val != 'all':\n",
    "        # Prediction per sup/sub\n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0)]\n",
    "    elif ax_val != 'all' and ss_val != 'all':\n",
    "        # Prediction per axis and sup/sub\n",
    "        if ax_val == 'ax0':\n",
    "            ax_val_n = 0\n",
    "        elif ax_val == 'ax1':\n",
    "            ax_val_n = 1\n",
    "        elif ax_val == 'ax2':\n",
    "            ax_val_n = 2\n",
    "        \n",
    "        if ss_val == 'sup':  # sup\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss > 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "        elif ss_val == 'sub':  # sub\n",
    "            df = df_timeseries_exp[exp][(df_timeseries_exp[exp].ss < 0) & (df_timeseries_exp[exp].ax == ax_val_n)]\n",
    "\n",
    "    print('Confirmation : exp=', exp, ', ax_val=', ax_val, ', ss_val=', ss_val)\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b3331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the joystick stimulus axis data and put it in a pandas column\n",
    "def indexit(row):\n",
    "    joy_mat = [row.JOY_ax0, row.JOY_ax1, row.JOY_ax2]\n",
    "    return joy_mat[int(row.ax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb2e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised_labels2\n",
    "# <script src=\"https://gist.github.com/j622amilah/3c12c0ae204323c1a1ec031e607581d4.js\"></script>\n",
    "def unsupervised_lab_kmeans_clustering(*arg):\n",
    "    \n",
    "    n_clusters = arg[0]\n",
    "    X = arg[1]\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    if len(X.shape) == 1 or X.shape[1] == 1:\n",
    "        X = np.ravel(X)\n",
    "        out = pd.Series(X)\n",
    "        X = pd.concat([out, out], axis=1).to_numpy()\n",
    "    \n",
    "    from sklearn import cluster\n",
    "    \n",
    "    kmeans = cluster.KMeans(n_clusters=n_clusters, init='k-means++',algorithm='elkan', random_state=2)\n",
    "    # n_clusters : The number of clusters to form as well as the number of centroids to generate. (int, default=8)\n",
    "    \n",
    "    # init : Method for initialization : (default=’k-means++’)\n",
    "    # init='k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "    # init='random': choose n_clusters observations (rows) at random from data for the initial centroids.\n",
    "    \n",
    "    # n_init : Number of time the k-means algorithm will be run with different centroid seeds (int, default=10)\n",
    "    \n",
    "    # max_iter : Maximum number of iterations of the k-means algorithm for a single run. (int, default=300)\n",
    "    \n",
    "    # tol : Relative tolerance with regards to Frobenius norm of the difference in the cluster centers \n",
    "    # of two consecutive iterations to declare convergence. (float, default=1e-4)\n",
    "    \n",
    "    # (extremly important!) random_state : Determines random number generation for centroid initialization\n",
    "    #(int, RandomState instance or None, default=None)\n",
    "    \n",
    "    # algorithm{“auto”, “full”, “elkan”}, default=”auto”\n",
    "    # K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more \n",
    "    # efficient on data with well-defined clusters, by using the triangle inequality. However it’s more \n",
    "    # memory intensive due to the allocation of an extra array of shape (n_samples, n_clusters).\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # print('shape of X : ', X.shape)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # ------------------------------\n",
    "\n",
    "    # Get the prediction of each category : predicted label\n",
    "    label = kmeans.labels_\n",
    "    # print('clusters_out : ' + str(clusters_out))\n",
    "    # OR\n",
    "    label = kmeans.predict(X)\n",
    "    # print('clusters_out : ' + str(clusters_out))\n",
    "    # print('length of clusters_out', len(clusters_out))\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # Centroid values for feature space : this is the center cluster value per feature in X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    # print('centroids org : ' + str(centroids))\n",
    "\n",
    "    # ------------------------------\n",
    "\n",
    "    return kmeans, label, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_and_initial_feature(df):\n",
    "    # IC = 1\n",
    "    # EC = 2, 4, 5\n",
    "    # NC = 3, 6, 7\n",
    "    # NR = 9\n",
    "    # (IC) - sham (do not use) = 8\n",
    "    # (NC) - sham (do not use) = 10\n",
    "\n",
    "    # Just to confirm, what are the unique values of res_type\n",
    "    df.res_type.value_counts(ascending=True)\n",
    "\n",
    "    # Construction of SD_label : How do we define disorientation?\n",
    "\n",
    "    # Way 0 : lenient\n",
    "    # 0 = If participates got the result CORRECT for the trial, they were NOT disoriented. (IC, EC)\n",
    "    # 1 = If participates got the result WRONG or did not respond, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1) | (df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.lenient = ''  # define a new column , rows 8 and 10 will be NaN, need to do dropna for rows\n",
    "    df.loc[idx_NDS, 'lenient'] = 0\n",
    "    df.loc[idx_DS, 'lenient'] = 1\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Way 1 : strict simple\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants were WRONG at any point, they were disoriented. (EC, NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5) | (df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.strict = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'strict'] = 0\n",
    "    df.loc[idx_DS, 'strict'] = 1\n",
    "\n",
    "    # Way 2 : st_complex\n",
    "    # 0 = If participants got the result initially CORRECT, they were NOT disoriented. (IC)\n",
    "    # 1 = If participants got the result eventually CORRECT, they were MILDLY disoriented. (EC)\n",
    "    # 2 = If participants were WRONG for the trial, they were disoriented. (NC, NR)\n",
    "\n",
    "    idx_NDS = df.index[(df.res_type == 1)].to_list()\n",
    "    idx_MDS = df.index[(df.res_type == 2) | (df.res_type == 4) | (df.res_type == 5)].to_list()\n",
    "    idx_DS = df.index[(df.res_type == 3) | (df.res_type == 6) | (df.res_type == 7) | (df.res_type == 9)].to_list()\n",
    "    df.st_complex = ''  # define a new column\n",
    "    df.loc[idx_NDS, 'st_complex'] = 0\n",
    "    df.loc[idx_MDS, 'st_complex'] = 1\n",
    "    df.loc[idx_DS, 'st_complex'] = 2\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Create features :  (1) position\n",
    "    df['joy_stim'] = df.apply(indexit, axis='columns')  # fill in joy_stim\n",
    "    \n",
    "    # -------------------------------------\n",
    "\n",
    "    # Make DataFrame for trial start-stop index\n",
    "    # Cut the data up per trial across subjects\n",
    "    tr_vec = df.tr.to_numpy()\n",
    "\n",
    "    st = [0]\n",
    "    ender = []\n",
    "    for i in range(len(tr_vec)-1):\n",
    "        if tr_vec[i] != tr_vec[i+1]:\n",
    "            st = st + [i+1]\n",
    "            ender = ender + [i]\n",
    "    ender = ender + [len(tr_vec)-1]\n",
    "\n",
    "    # See start-stop index clearly\n",
    "    e0 = np.reshape(st, (len(st),1))\n",
    "    e1 = np.reshape(ender, (len(st),1))\n",
    "    data = np.ravel(e0), np.ravel(e1)\n",
    "    data = np.transpose(data)\n",
    "    columns = ['stind', 'endind']\n",
    "    temp = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Find the longest trial signal in df_rot['joy_stim']\n",
    "    temp['diff'] = temp.endind - temp.stind\n",
    "    temp['timediff'] = [df.time.iloc[temp.endind[i]] - df.time.iloc[temp.stind[i]] for i in range(len(temp.endind))]\n",
    "    outmin = temp['diff'].min()\n",
    "    outmax = temp['diff'].max()\n",
    "\n",
    "    tomin = temp['timediff'][(temp['diff'] == outmin)]\n",
    "    tomax = temp['timediff'][(temp['diff'] == outmax)]\n",
    "    # print('outmin : ', outmin, 't :', tomin)\n",
    "    # print('outmax : ', outmax, 't :', tomax)\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    # Interpolate : make each trial the same number of data points\n",
    "    from scipy.interpolate import interp1d\n",
    "    feat0 = []\n",
    "    t_feat0 = []\n",
    "    y1_feat0 = []\n",
    "    y2_feat0 = []\n",
    "    y3_feat0 = []\n",
    "    for i in range(len(temp.stind)):\n",
    "        \n",
    "        # X\n",
    "        sSIG = df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        t_sSIG = df['time'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "\n",
    "        # labels\n",
    "        y1 = df['lenient'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y2 = df['strict'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        y3 = df['st_complex'][temp.stind.iloc[i]:temp.endind.iloc[i]].to_numpy()\n",
    "        \n",
    "        # Check if trial data is less than the maximum length\n",
    "        if len(df['joy_stim'][temp.stind.iloc[i]:temp.endind.iloc[i]]) < outmax:\n",
    "            \n",
    "            # The trial length is different so interpolate the time-series to make them the same length signal \n",
    "            x = np.linspace(sSIG[0], len(sSIG), num=len(sSIG), endpoint=True)\n",
    "            xnew = np.linspace(sSIG[0], len(sSIG), num=outmax, endpoint=True)\n",
    "\n",
    "            # joystick on stim\n",
    "            f = interp1d(x, sSIG)\n",
    "            sSIGl = f(xnew)\n",
    "\n",
    "            # time\n",
    "            f = interp1d(x, t_sSIG)\n",
    "            t_sSIGl = f(xnew)\n",
    "\n",
    "            # y1\n",
    "            f = interp1d(x, y1)\n",
    "            y1_sSIGl = f(xnew)\n",
    "\n",
    "            # y2\n",
    "            f = interp1d(x, y2)\n",
    "            y2_sSIGl = f(xnew)\n",
    "\n",
    "            # y3\n",
    "            f = interp1d(x, y3)\n",
    "            y3_sSIGl = f(xnew)\n",
    "\n",
    "            # python : you can not create a matrix in real-time in pandas\n",
    "            # you only assign the full matrix at the end\n",
    "            # (0) position\n",
    "            feat0 = feat0 + [sSIGl]\n",
    "            t_feat0 = t_feat0 + [t_sSIGl]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1_sSIGl)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2_sSIGl)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3_sSIGl)]\n",
    "            \n",
    "            del x, f, sSIGl, t_sSIGl, y1_sSIGl, y2_sSIGl, y3_sSIGl\n",
    "        else:\n",
    "            feat0 = feat0 + [sSIG]\n",
    "            t_feat0 = t_feat0 + [t_sSIG]\n",
    "            y1_feat0 = y1_feat0 + [np.ravel(y1)]\n",
    "            y2_feat0 = y2_feat0 + [np.ravel(y2)]\n",
    "            y3_feat0 = y3_feat0 + [np.ravel(y3)]\n",
    "\n",
    "    # Clean up\n",
    "    del df\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7644c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_feature_data(feat, plotORnot):\n",
    "\n",
    "    if int(np.mean(feat)) == 0:\n",
    "        scaled_data_mlx = feat\n",
    "    else:\n",
    "        columns = ['0']\n",
    "        dat = pd.DataFrame(data=feat, columns=columns)\n",
    "        scaled_data0 = minmax_scaling(dat, columns=columns)\n",
    "        scaled_data_mlx = list(scaled_data0.to_numpy().ravel())\n",
    "        # OR \n",
    "        scaled_data_norma = []\n",
    "        for q in range(len(feat)):\n",
    "            scaled_data_norma.append( (feat[q] - np.min(feat))/(np.max(feat) - np.min(feat)) )  # normalization : same as mlxtend\n",
    "        # OR \n",
    "        shift_up = [i - np.min(feat) for i in feat]\n",
    "        scaled_data_posnorma = [q/np.max(shift_up) for q in shift_up]  # positive normalization : same as mlxtend\n",
    "        # OR \n",
    "        scaled_data_standardization = [(q - np.mean(feat))/np.std(feat) for q in feat]  # standardization\n",
    "\n",
    "        if plotORnot == 1:\n",
    "            fig = make_subplots(rows=2, cols=1)\n",
    "            config = dict({'scrollZoom': True, 'displayModeBar': True, 'editable': True})\n",
    "            xxORG = list(range(len(feat)))\n",
    "            fig.add_trace(go.Scatter(x=xxORG, y=feat, name='feat', line = dict(color='red', width=2, dash='solid'), showlegend=True), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(x=xxORG, y=scaled_data_mlx, name='scaled : mlxtend', line = dict(color='red', width=2, dash='solid'), showlegend=True), row=2, col=1)\n",
    "            fig.add_trace(go.Scatter(x=xxORG, y=scaled_data_norma, name='scaled : normalization', line = dict(color='cyan', width=2, dash='solid'), showlegend=True), row=2, col=1)\n",
    "            fig.add_trace(go.Scatter(x=xxORG, y=scaled_data_posnorma, name='scaled : positive normalization', line = dict(color='blue', width=2, dash='solid'), showlegend=True), row=2, col=1)\n",
    "            fig.add_trace(go.Scatter(x=xxORG, y=scaled_data_standardization, name='scaled : standardization', line = dict(color='orange', width=2, dash='solid'), showlegend=True), row=2, col=1)\n",
    "            fig.update_layout(title='feature vs scaled featue', xaxis_title='data points', yaxis_title='amplitude')\n",
    "            fig.show(config=config)\n",
    "\n",
    "    return scaled_data_mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d1b43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creation_preprocessing(feat0, t_feat0):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # ----------------\n",
    "    # Make your features\n",
    "    # ----------------\n",
    "    df_feat = pd.DataFrame()\n",
    "\n",
    "    n = 4   # filter order\n",
    "    fs = 250 # data sampling frequency (Hz)\n",
    "    fcc = 10  # Cut-off frequency of the filter\n",
    "    w = fcc / (fs / 2) # Normalize the frequency\n",
    "    b, a = signal.butter(n, w, 'low')  # 3rd order\n",
    "\n",
    "    scales = np.arange(1, 128)\n",
    "\n",
    "    print('num de samples avant dropna : ', len(feat0))\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Drop nan values from feat0\n",
    "    temp = pd.DataFrame(feat0)\n",
    "    temp0 = my_dropna_python(temp)\n",
    "    feat0 = temp0.to_numpy()\n",
    "    print('num de samples apres dropna : ', len(feat0))\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Drop all zero samples from feat0\n",
    "    temp = []\n",
    "    for i in range(len(feat0)):\n",
    "        if int(np.mean(feat0[i])) != 0:\n",
    "            temp.append(feat0[i])\n",
    "    del feat0\n",
    "    feat0 = temp\n",
    "    del temp\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "\n",
    "    # Need to find when one trial starts and end - take derivative from start-stop periods\n",
    "    for i in range(len(feat0)):\n",
    "\n",
    "        if i == 0:\n",
    "            plotORnot = 0 #1\n",
    "        else:\n",
    "            plotORnot = 0\n",
    "\n",
    "        # ----------------------------\n",
    "        # (0) position - causale ordre\n",
    "        col0 = scale_feature_data(feat0[i], plotORnot)\n",
    "        # col0 = np.reshape(col0, (len(col0),1))\n",
    "        col0 = np.array(col0)\n",
    "        # print('col0 : ', col0.shape)   # col0 :  (471,)\n",
    "\n",
    "        # (1) velocity - causale ordre\n",
    "        vel = numderiv(feat0[i], t_feat0[i])\n",
    "        col1 = scale_feature_data(vel, plotORnot)\n",
    "        # col1 = np.reshape(col1, (len(col1),1))\n",
    "        col1 = np.array(col1)\n",
    "        # print('col1 : ', col1.shape)   # col1 :  (471,)\n",
    "\n",
    "        # (2) acceleration - causale ordre\n",
    "        acc = numderiv(vel, t_feat0[i])\n",
    "        filtacc = signal.filtfilt(b, a, acc) # the signal is noisy\n",
    "        col2 = scale_feature_data(filtacc, plotORnot)\n",
    "        # col2 = np.reshape(col2, (len(col2),1))\n",
    "        col2 = np.array(col2)\n",
    "        # print('col2 : ', col2.shape)   # col2 :  (471,)\n",
    "        # ----------------------------\n",
    "\n",
    "        # ----------------------------\n",
    "        # (4) position - non-causale ordre\n",
    "        col3 = normal_distribution_feature_data(col0, plotORnot)\n",
    "        col3 = np.array(col3)\n",
    "        # print('col3 : ', col3.shape)   # col3 :  (471,)\n",
    "\n",
    "        # (5) velocity - non-causale ordre\n",
    "        col4 = normal_distribution_feature_data(col1, plotORnot)\n",
    "        col4 = np.array(col4)\n",
    "        # print('col4 : ', col4.shape)   # col4 :  (471,)\n",
    "\n",
    "        # (6) acceleration - non-causale ordre\n",
    "        col5 = normal_distribution_feature_data(col2, plotORnot)\n",
    "        col5 = np.array(col5)\n",
    "        # print('col5 : ', col5.shape)   # col5 :  (471,)\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # Frequence marquers : sublevels of frequency pattern\n",
    "        # ----------------------------\n",
    "        # (7-22) une transformation de fréquence (ondelettes)\n",
    "        coeff = tsig_2_discrete_wavelet_transform(col0, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols6 = pd.DataFrame(coeff).T\n",
    "        out6 = cols6.to_numpy()\n",
    "        # print('col6 : ', out6.shape)   # col6 :  (471, 5)\n",
    "\n",
    "        coeff = tsig_2_discrete_wavelet_transform(col1, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols7 = pd.DataFrame(coeff).T\n",
    "        out7 = cols7.to_numpy()\n",
    "        # print('col7 : ', out7.shape)   # col7 :  (471, 5)\n",
    "\n",
    "        coeff = tsig_2_discrete_wavelet_transform(col2, waveletname='sym5', level=5, plotORnot=0)\n",
    "        cols8 = pd.DataFrame(coeff).T\n",
    "        out8 = cols8.to_numpy()\n",
    "        # print('col8 : ', out8.shape)   # col8 :  (471, 5)\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # Hybrid marquers : temporalle et frequence information\n",
    "        # ----------------------------\n",
    "        # (8) spectrogram flatten - periodogram (fft)\n",
    "        col9 = tsig_2_spectrogram(col0, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        col9 = np.array(col9)\n",
    "        # print('col9 : ', col9.shape)   # col9 :  (471,)\n",
    "\n",
    "        col10 = tsig_2_spectrogram(col1, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        col10 = np.array(col10)\n",
    "        # print('col10 : ', col10.shape)   # col10 :  (471,)\n",
    "\n",
    "        col11 = tsig_2_spectrogram(col2, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "        col11 = np.array(col11)\n",
    "        # print('col11 : ', col11.shape)   # col11 :  (471,)\n",
    "        # ----------------------------\n",
    "\n",
    "        # ----------------------------\n",
    "        # (9) continuous wavelet transform flatten\n",
    "        # continuous_wavelets = ['mexh', 'morl', 'cgau5', 'gaus5']\n",
    "        col12 = tsig_2_continuous_wavelet_transform(t_feat0[i], col0, scales, waveletname='mexh', plotORnot=0)\n",
    "        col12 = np.array(col12)\n",
    "        # print('col12 : ', col12.shape)   # col12 :  (471,)\n",
    "\n",
    "        col13 = tsig_2_continuous_wavelet_transform(t_feat0[i], col1, scales, waveletname='mexh', plotORnot=0)\n",
    "        col13 = np.array(col13)\n",
    "        # print('col13 : ', col13.shape)   # col13 :  (471,)\n",
    "\n",
    "        col14 = tsig_2_continuous_wavelet_transform(t_feat0[i], col2, scales, waveletname='mexh', plotORnot=0)\n",
    "        col14 = np.array(col14)\n",
    "        # print('col14 : ', col14.shape)   # col14 :  (471,)\n",
    "        # ----------------------------\n",
    "\n",
    "        # ----------------------------\n",
    "        # (10) kmeans\n",
    "        n_clusters = 2\n",
    "        col1 = np.reshape(col1, (len(col1),1))\n",
    "        col2 = np.reshape(col2, (len(col2),1))\n",
    "        col3 = np.reshape(col3, (len(col3),1))\n",
    "        X = [col1, col2, col3]\n",
    "        X = np.reshape(X, (len(col3),3))\n",
    "\n",
    "        kmeans, col15, centroids = unsupervised_lab_kmeans_clustering(n_clusters, X)\n",
    "        col15 = np.array(col15)\n",
    "        # print('col15 : ', col15.shape)   # col15 :  (471,)\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # Peut-être faire des non-causale frequence marquers pour des mieux frequence et hybrid \n",
    "        num = i*np.ones((len(col0)))\n",
    "        # print('num : ', num.shape)   # num :  (471,)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        # num = pd.Series(num)\n",
    "        # c0 = pd.Series(col0)\n",
    "        # c1 = pd.Series(col1)\n",
    "        # c2 = pd.Series(col2)\n",
    "        # c3 = pd.Series(col3)\n",
    "        # c4 = pd.Series(col4)\n",
    "        # c5 = pd.Series(col5)\n",
    "        # OU\n",
    "        num = np.reshape(num, (len(num),1))\n",
    "        col0 = np.reshape(col0, (len(num),1))\n",
    "        col1 = np.reshape(col1, (len(num),1))\n",
    "        col2 = np.reshape(col2, (len(num),1))\n",
    "        col3 = np.reshape(col3, (len(num),1))\n",
    "        col4 = np.reshape(col4, (len(num),1))\n",
    "        col5 = np.reshape(col5, (len(num),1))\n",
    "        part1 = [num, col0, col1, col2, col3, col4, col5]\n",
    "        part1 = np.reshape(part1, (len(num), 7))\n",
    "        part1 = pd.DataFrame(part1).T\n",
    "\n",
    "        cs6 = cols6\n",
    "        cs7 = cols7\n",
    "        cs8 = cols8\n",
    "\n",
    "        # c9 = pd.Series(col9)\n",
    "        # c10 = pd.Series(col10)\n",
    "        # c11 = pd.Series(col11)\n",
    "        # c12 = pd.Series(col12)\n",
    "        # c13 = pd.Series(col13)\n",
    "        # c14 = pd.Series(col14)\n",
    "        # c15 = pd.Series(col15)\n",
    "        # OU\n",
    "        col9 = np.reshape(col9, (len(num),1))\n",
    "        col10 = np.reshape(col10, (len(num),1))\n",
    "        col11 = np.reshape(col11, (len(num),1))\n",
    "        col12 = np.reshape(col12, (len(num),1))\n",
    "        col13 = np.reshape(col13, (len(num),1))\n",
    "        col14 = np.reshape(col14, (len(num),1))\n",
    "        col15 = np.reshape(col15, (len(num),1))\n",
    "        part3 = [col9, col10, col11, col12, col13, col14, col15]\n",
    "        part3 = np.reshape(part3, (len(num), 7))\n",
    "        part3 = pd.DataFrame(part3).T\n",
    "\n",
    "\n",
    "        # Clean up\n",
    "        del vel, acc, filtacc, coeff\n",
    "        del col0, col1, col2, col3, col4, col5, cols6, cols7, cols8, col9, col10, col11, col12, col13, col14, col15\n",
    "        # temp = pd.concat([num, c0, c1, c2, c3, c4, c5, cs6, cs7, cs8, c9, c10, c11, c12, c13, c14, c15], axis=1)\n",
    "        # del tr, c0, c1, c2, c3, c4, c5, cs6, cs7, cs8, c9, c10, c11, c12, c13, c14, c15\n",
    "        # OU\n",
    "        temp = pd.concat([part1, cs6, cs7, cs8, part3], axis=1)\n",
    "        del part1, cs6, cs7, cs8, part3\n",
    "\n",
    "        df_feat = pd.concat([df_feat, temp], axis=0)\n",
    "\n",
    "        del temp\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    end = time.time()\n",
    "    print('Elasped time for feature processing : ', end - start)\n",
    "\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8143131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rename_columns(df, col_list):\n",
    "    \n",
    "    way = 1\n",
    "    \n",
    "    if way == 0:\n",
    "        # Façon difficile\n",
    "        # Rename columns\n",
    "        onames = df.columns.to_numpy()\n",
    "        dictout = {}\n",
    "        for nf in range(len(col_list)):\n",
    "            dictout[onames[nf]] = '%s' % (col_list[nf])\n",
    "            \n",
    "        # Determinez quels columns de df repeter\n",
    "        uq = Counter(onames).most_common()\n",
    "        d = {}\n",
    "        for i in range(len(uq)):\n",
    "            temp = []\n",
    "            for ind, val in enumerate(onames):\n",
    "                if uq[i][0] == val:\n",
    "                    temp.append(col_list[ind])\n",
    "            d[i] = temp    \n",
    "        \n",
    "        # if the column name is a key of d pop the names in the list, else return the column name\n",
    "        df.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    \n",
    "    elif way == 1:\n",
    "        # Façon facile\n",
    "        df.columns = col_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd76362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data_2makeclasses_equivalent(df_feat):\n",
    "    # Remove nan value per row\n",
    "    # df_test_noNan = df_feat.dropna(axis=0)\n",
    "    # OR\n",
    "    df_test_noNan = my_dropna_python(df_feat)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Confirm that there are no nan values\n",
    "    out = df_test_noNan.isnull().values.any()\n",
    "    print('Are there nan valeus in the data : ', out)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Check class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test_noNan)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    print('shape of dataframe before padding : ', df_test_noNan.shape)\n",
    "\n",
    "    # ----------------\n",
    "    # Pad the DataFrame\n",
    "    n_classes = len(count_index)\n",
    "    n_samples = len(st)\n",
    "\n",
    "    df_2add_on = pd.DataFrame()\n",
    "    \n",
    "    # Le derniere sample dans df_test_noNan\n",
    "    df_coupe_proche = df_test_noNan.iloc[st[-1]:endd[-1], :]\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        #print('i : ', i)\n",
    "        # Pad short length classes\n",
    "        for j in range(needed_samps_class[i]):\n",
    "            #print('j : ', j) \n",
    "            flag = 0\n",
    "            while flag == 0:\n",
    "                permvec = np.random.permutation(n_samples)\n",
    "                index = permvec[0]  #random choosen index\n",
    "                \n",
    "                # look for each class : on veut le classe être le meme\n",
    "                if i == int(df_test_noNan.y.iloc[st[index]]):\n",
    "                    #print('Class match was found : i = ', i, ', data index = ', int(df_test_noNan.y_scalar.iloc[index]), ', index = ', index)\n",
    "                    \n",
    "                    # Append the data with padded data entry\n",
    "                    df_coupe = df_test_noNan.iloc[st[index]:endd[index], :]\n",
    "                    \n",
    "                    # Le derriere sample ne sont pas le meme que le sample actuelle\n",
    "                    if int(df_coupe.iloc[0,0] - df_coupe_proche.iloc[0,0]) != 0:\n",
    "                        df_coupe_proche = df_coupe\n",
    "                        df_2add_on = pd.concat([df_2add_on, df_coupe], axis=0)\n",
    "                        flag = 1 # to brake while\n",
    "                        \n",
    "    # ----------------\n",
    "\n",
    "    # DataFrame a besoin les noms de columns d'avoir le meme noms que df_test_noNan\n",
    "    df_2add_on = df_2add_on.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    col_list = df_test_noNan.columns\n",
    "    df_2add_on = pandas_rename_columns(df_2add_on, col_list)\n",
    "    df_2add_on\n",
    "\n",
    "    print('shape of dataframe to add to original dataframe: ', df_2add_on.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # want to arrange the dataframe with respect to rows (stack on top of the other): so axis=0 \n",
    "    # OR think of it as the rows of the df change so you put axis=0 for rows\n",
    "    df_test2 = pd.concat([df_test_noNan, df_2add_on], axis=0)\n",
    "    df_test2 = df_test2.reset_index(drop=True)  # reset index : delete the old index column\n",
    "\n",
    "    print('shape of padded dataframe (original + toadd) : ', df_test2.shape)\n",
    "\n",
    "    del df_test_noNan, df_2add_on\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Final check of class balance\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Enlevez des columns unnecessaires : num, y\n",
    "    # df_test2 = df_test2.drop(['num', 'y'], axis=1)  # 1 is the axis number (0 for rows and 1 for columns)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Rename each feature column a number, and the label column with y \n",
    "    # col_list = list(map(str, np.arange(len(df_test2.columns) - 1)))\n",
    "    # col_list.append('y')\n",
    "    # df_test2 = pandas_rename_columns(df_test2, col_list)\n",
    "    \n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5939b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nonconsecutive_values_debut_fin_pt(vec):\n",
    "  \n",
    "    st = [0]\n",
    "    endd = []\n",
    "    \n",
    "    for i in range(len(vec)-1):\n",
    "        if vec[i] != vec[i+1]:\n",
    "            st.append(i+1)\n",
    "            endd.append(i)\n",
    "    \n",
    "    endd.append(len(vec)-1)\n",
    "    \n",
    "    return st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79951844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(df_test_noNan):\n",
    "\n",
    "    # Get start and end index values for each sample\n",
    "    num = list(map(int, df_test_noNan.num.to_numpy()))\n",
    "    st, endd = detect_nonconsecutive_values_debut_fin_pt(num)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    yy = list(map(int, df_test_noNan.y.to_numpy()))\n",
    "    y_short = []\n",
    "    for i in range(len(st)):\n",
    "        y_short.append(yy[st[i]:st[i]+1][0])\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    liste = Counter(y_short).most_common()\n",
    "    count_index, counted_value = list(map(list, zip(*liste)))\n",
    "\n",
    "    print('Before sorting counted_value : ', counted_value)\n",
    "    print('Before sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Sort counted_value by count_index; in ascending order\n",
    "    sind = np.argsort(count_index)\n",
    "    count_index = [count_index[i] for i in sind]\n",
    "    counted_value = [counted_value[i] for i in sind]\n",
    "    print('After sorting counted_value : ', counted_value)\n",
    "    print('After sorting count_index : ', count_index)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Determine how much to pad each class label\n",
    "    needed_samps_class = np.max(counted_value) - counted_value\n",
    "    print('needed_samps_class : ', needed_samps_class)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    return needed_samps_class, counted_value, count_index, st, endd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f549b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dat_pickle(outSIG, file_name=\"outSIG.pkl\"):\n",
    "    # Save data matrices to file\n",
    "    open_file = open(file_name, \"wb\")\n",
    "    pickle.dump(outSIG, open_file)\n",
    "    open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d9ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dat_pickle(file_name=\"outSIG.pkl\"):\n",
    "    open_file = open(file_name, \"rb\")\n",
    "    dataout = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return dataout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1694504",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efcd00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "varr = {}\n",
    "varr['main_path'] = \"/home/oem2/Documents/9_Motor_classification_2018-22/Coding_version3_python_FINAL\"\n",
    "varr['main_path1'] = \"%s/a_data_standardization\" % (varr['main_path'])\n",
    "varr['main_path2'] = \"%s/b_data_preprocessing\" % (varr['main_path'])\n",
    "varr['main_path3'] = \"%s/c_calculate_metrics\" % (varr['main_path'])\n",
    "\n",
    "df_timeseries_exp = put_timeseries_trialdata_into_pandas(varr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "633b0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[rot] :  (86006, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86001</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.453208</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699437</td>\n",
       "      <td>-1.514817</td>\n",
       "      <td>0.803347</td>\n",
       "      <td>-4.719625</td>\n",
       "      <td>0.301392</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-4.927108</td>\n",
       "      <td>0.850650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.927108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86002</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.391906</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.461697</td>\n",
       "      <td>0.803352</td>\n",
       "      <td>-4.719593</td>\n",
       "      <td>0.333524</td>\n",
       "      <td>-0.082369</td>\n",
       "      <td>-4.917930</td>\n",
       "      <td>0.902199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.917930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86003</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>16.500001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.313130</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.382446</td>\n",
       "      <td>0.807940</td>\n",
       "      <td>-4.719292</td>\n",
       "      <td>0.356304</td>\n",
       "      <td>-0.086946</td>\n",
       "      <td>-4.908213</td>\n",
       "      <td>0.953649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.908213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86004</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>16.599999</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.221655</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.284506</td>\n",
       "      <td>0.807370</td>\n",
       "      <td>-4.719829</td>\n",
       "      <td>0.362870</td>\n",
       "      <td>-0.089799</td>\n",
       "      <td>-4.897957</td>\n",
       "      <td>1.004994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.897957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86005</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.125844</td>\n",
       "      <td>0.804159</td>\n",
       "      <td>-4.699425</td>\n",
       "      <td>-1.179229</td>\n",
       "      <td>0.805081</td>\n",
       "      <td>-4.719719</td>\n",
       "      <td>0.364322</td>\n",
       "      <td>-0.088796</td>\n",
       "      <td>-4.887165</td>\n",
       "      <td>1.056230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.887165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp       time  res_type  SIGCOM_ax0  \\\n",
       "86001     17.0  20.0 -1.0  0.0  163.0  16.300000       7.0   -1.453208   \n",
       "86002     17.0  20.0 -1.0  0.0  164.0  16.400000       7.0   -1.391906   \n",
       "86003     17.0  20.0 -1.0  0.0  165.0  16.500001       7.0   -1.313130   \n",
       "86004     17.0  20.0 -1.0  0.0  166.0  16.599999       7.0   -1.221655   \n",
       "86005     17.0  20.0 -1.0  0.0  167.0  16.700000       7.0   -1.125844   \n",
       "\n",
       "       SIGCOM_ax1  SIGCOM_ax2   SIG_ax0   SIG_ax1   SIG_ax2   JOY_ax0  \\\n",
       "86001    0.804159   -4.699437 -1.514817  0.803347 -4.719625  0.301392   \n",
       "86002    0.804159   -4.699425 -1.461697  0.803352 -4.719593  0.333524   \n",
       "86003    0.804159   -4.699425 -1.382446  0.807940 -4.719292  0.356304   \n",
       "86004    0.804159   -4.699425 -1.284506  0.807370 -4.719829  0.362870   \n",
       "86005    0.804159   -4.699425 -1.179229  0.805081 -4.719719  0.364322   \n",
       "\n",
       "        JOY_ax1   JOY_ax2  NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "86001 -0.082375 -4.927108   0.850650        0.0  -4.927108  \n",
       "86002 -0.082369 -4.917930   0.902199        0.0  -4.917930  \n",
       "86003 -0.086946 -4.908213   0.953649        0.0  -4.908213  \n",
       "86004 -0.089799 -4.897957   1.004994        0.0  -4.897957  \n",
       "86005 -0.088796 -4.887165   1.056230        0.0  -4.887165  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[rot] : ', df_timeseries_exp['rot'].shape)\n",
    "df_timeseries_exp['rot'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2552b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_timeseries_exp[trans] :  (43754, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>tr</th>\n",
       "      <th>ss</th>\n",
       "      <th>ax</th>\n",
       "      <th>dp</th>\n",
       "      <th>time</th>\n",
       "      <th>res_type</th>\n",
       "      <th>SIGCOM_ax0</th>\n",
       "      <th>SIGCOM_ax1</th>\n",
       "      <th>SIGCOM_ax2</th>\n",
       "      <th>SIG_ax0</th>\n",
       "      <th>SIG_ax1</th>\n",
       "      <th>SIG_ax2</th>\n",
       "      <th>JOY_ax0</th>\n",
       "      <th>JOY_ax1</th>\n",
       "      <th>JOY_ax2</th>\n",
       "      <th>NOISE_ax0</th>\n",
       "      <th>NOISE_ax1</th>\n",
       "      <th>NOISE_ax2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43749</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.3868</td>\n",
       "      <td>51.1975</td>\n",
       "      <td>-1.3760</td>\n",
       "      <td>68.5344</td>\n",
       "      <td>31.0360</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43750</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.7618</td>\n",
       "      <td>51.4284</td>\n",
       "      <td>-1.3559</td>\n",
       "      <td>68.9152</td>\n",
       "      <td>31.1329</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43751</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.1368</td>\n",
       "      <td>51.6561</td>\n",
       "      <td>-1.3360</td>\n",
       "      <td>69.2979</td>\n",
       "      <td>31.2316</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43752</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.5118</td>\n",
       "      <td>51.7815</td>\n",
       "      <td>-1.3128</td>\n",
       "      <td>69.6821</td>\n",
       "      <td>31.3353</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43753</th>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.7139</td>\n",
       "      <td>51.7700</td>\n",
       "      <td>-1.2916</td>\n",
       "      <td>70.0518</td>\n",
       "      <td>31.4619</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.0273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject    tr   ss   ax     dp  time  res_type  SIGCOM_ax0  SIGCOM_ax1  \\\n",
       "43749     13.0  16.0  1.0  1.0  197.0  19.7       7.0         0.0     69.3868   \n",
       "43750     13.0  16.0  1.0  1.0  198.0  19.8       7.0         0.0     69.7618   \n",
       "43751     13.0  16.0  1.0  1.0  199.0  19.9       7.0         0.0     70.1368   \n",
       "43752     13.0  16.0  1.0  1.0  200.0  20.0       7.0         0.0     70.5118   \n",
       "43753     13.0  16.0  1.0  1.0  201.0  20.1       7.0         0.0     70.7139   \n",
       "\n",
       "       SIGCOM_ax2  SIG_ax0  SIG_ax1  SIG_ax2  JOY_ax0  JOY_ax1  JOY_ax2  \\\n",
       "43749     51.1975  -1.3760  68.5344  31.0360  -0.0412  -0.0293      0.0   \n",
       "43750     51.4284  -1.3559  68.9152  31.1329  -0.0412  -0.0294      0.0   \n",
       "43751     51.6561  -1.3360  69.2979  31.2316  -0.0412  -0.0272      0.0   \n",
       "43752     51.7815  -1.3128  69.6821  31.3353  -0.0408  -0.0273      0.0   \n",
       "43753     51.7700  -1.2916  70.0518  31.4619  -0.0408  -0.0273      0.0   \n",
       "\n",
       "       NOISE_ax0  NOISE_ax1  NOISE_ax2  \n",
       "43749       3.75       -0.0        0.0  \n",
       "43750       3.75       -0.0        0.0  \n",
       "43751       3.75       -0.0        0.0  \n",
       "43752       3.75       -0.0        0.0  \n",
       "43753       0.00        0.0        0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of df_timeseries_exp[trans] : ', df_timeseries_exp['trans'].shape)\n",
    "df_timeseries_exp['trans'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f95d2",
   "metadata": {},
   "source": [
    "## Verifier des marquers : Plot des marquers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos, label=\"num\")\n",
    "sns.lineplot(data=df_feat.vel, label=\"num\", color='red')\n",
    "sns.lineplot(data=df_feat.acc, label=\"num\", color='green')\n",
    "plt.title(\"position, velocity, acceleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_sl0, label=\"position\")\n",
    "sns.lineplot(data=df_feat.vel_sl0, label=\"velocity\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_sl0, label=\"acceleration\", color='green')\n",
    "plt.title(\"wavelet sublevel : position, velocity, accleration\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44874b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_spec, label=\"pos_spec\")\n",
    "sns.lineplot(data=df_feat.vel_spec, label=\"vel_spec\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_spec, label=\"acc_spec\", color='green')\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))  # width, height of the figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.lineplot(data=df_feat.pos_cwt, label=\"pos_cwt\")\n",
    "sns.lineplot(data=df_feat.vel_cwt, label=\"vel_cwt\", color='red')\n",
    "sns.lineplot(data=df_feat.acc_cwt, label=\"acc_cwt\", color='green')\n",
    "plt.title(\"Continuous Wavelet Transform\")\n",
    "plt.xlabel(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389323f",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d19198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_tensorflow(model, X_test, Y_test):\n",
    "\n",
    "    Y_test_1D = [Y_test[i,0:1] for i in range(Y_test.shape[0])]\n",
    "\n",
    "    # First, a baseline metric, defined by scoring,\n",
    "    # Obtenez mean absolute error\n",
    "    y_hat_test = model.predict(X_test, verbose=0)\n",
    "    baseline_mae = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "\n",
    "    vals = {}\n",
    "    # Shuffle each feature columns at a time\n",
    "    for featcol in range(X_test.shape[2]):\n",
    "\n",
    "        # Define a modifiable temporary variable\n",
    "        temp = X_test\n",
    "\n",
    "        # select a column\n",
    "        feat_slice = temp[:,:,featcol]\n",
    "\n",
    "        # Must flatten the matrix because np.random.permutation or \n",
    "        # np.random.shuffle don't work\n",
    "        t = feat_slice.flatten()\n",
    "        t_shuf = np.random.permutation(t)\n",
    "        feat_slice =  np.reshape(t_shuf, (feat_slice.shape))\n",
    "\n",
    "        # put feat_slice back into temp\n",
    "        temp[:,:,featcol] = feat_slice\n",
    "\n",
    "        y_hat_test = model.predict(temp, verbose=0)\n",
    "        mae_per_col = np.mean(np.abs(y_hat_test - Y_test_1D))\n",
    "        vals[featcol] = mae_per_col\n",
    "\n",
    "    # Sort the columns from largest to smallest mae\n",
    "    laquelle = sort_dict_by_value(vals, reverse = True)\n",
    "    \n",
    "    # Determinez le nombres des columns qui sont plus grande que le baseline_mae\n",
    "    # C'est des marqueurs qui sont importants\n",
    "    feat = list(laquelle.keys())\n",
    "    cnt = [1 for i in range(len(feat)) if feat[i] > baseline_mae]\n",
    "    cnt = np.sum(cnt)\n",
    "    \n",
    "    allout = list(laquelle.items())\n",
    "    nout = [allout[i] for i in range(cnt)]\n",
    "    marquers_important = dict(nout)\n",
    "    \n",
    "    return marquers_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073adc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df_test2, ynum):\n",
    "    \n",
    "    # Loop over each model to test\n",
    "    for wm in [3]:\n",
    "        res_permod = []\n",
    "        \n",
    "        for fea in range(4):\n",
    "            print('fea : ', fea)\n",
    "            \n",
    "            # ----------------\n",
    "            # Order of which features to use in a model\n",
    "            if fea == 0:\n",
    "                # 1) All features\n",
    "                X_cols = list(np.arange(1, df_test2.shape[1]-1, 1))\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 1:\n",
    "                # 2) first 3 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(3)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 2:\n",
    "                # 3) first 2 from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[oo][0] for oo in range(2)]\n",
    "                y_cols = [df_test2.shape[1]-1]\n",
    "            elif fea == 3:\n",
    "                # 4) first feature from permutation_importance\n",
    "                X_cols = [list(marquers_important.items())[0][0]]\n",
    "                y_cols = [df_test2.shape[1]-1] \n",
    "            \n",
    "            if wm == 0:\n",
    "                # Sequential : Support Vector Machine\n",
    "                m_name = 'SVC'\n",
    "                batch_size = 24\n",
    "                X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "                model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = svm_batch(X_train, X_test, Y_train, Y_test, info, batch_size)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    # Stack all data over batches\n",
    "                    X_test_batch = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "                    Y_test_1D_batch = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "\n",
    "                    r = permutation_importance(model, X_test_batch, Y_test_1D_batch, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "                    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "                    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 1:\n",
    "                # Sequential : LSTM - changes within a window of points\n",
    "                m_name = 'LSTM'\n",
    "                model, dict_out, X_test, Y_test = run_LSTM(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = [dict_out['delay_train'], dict_out['delay_test']]    \n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 2:\n",
    "                # Sequential : Transformer - changes between windows of points\n",
    "                m_name = 'Trans'\n",
    "                model, dict_out, X_test, Y_test = run_Transformer(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 3:\n",
    "                # Spatial (find global trends in the feature) : RandomForest - partitioned subspace\n",
    "                m_name = 'RF'\n",
    "                X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "                \n",
    "                # Stack all data over batch\n",
    "                X_train = np.reshape(X_train, (info['batch_train']*info['timesteps_train'], info['feature_train']))\n",
    "                Y_train_1D = np.reshape(Y_train, (info['batch_train']*info['timesteps_train'], info['n_outputs']))\n",
    "                X_test = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "                Y_test_1D = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "                \n",
    "                model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score, dfs = RandomForest(X_train, X_test, Y_train_1D, Y_test_1D)\n",
    "\n",
    "                value_pack_train = evaluation_methods(model, X_train, Y_train_1D, Y_train_1D_predict, Y_train_bin_pp, Y_train_score, dfs)\n",
    "                value_pack_test = evaluation_methods(model, X_test, Y_test_1D, Y_test_1D_predict, Y_test_bin_pp, Y_test_score, dfs)\n",
    "\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    r = permutation_importance(model, X_test, Y_test_1D, n_repeats=10, random_state=0, scoring='accuracy')\n",
    "                    vals = dict(zip(np.arange(len(r.importances_mean)), r.importances_mean))\n",
    "                    marquers_important = sort_dict_by_value(vals, reverse = True) # Sort the columns from largest to smallest mae\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 4:\n",
    "                # Spatial (find global trends in the feature) : CNN\n",
    "                m_name = 'CNN'\n",
    "                model, dict_out, X_test, Y_test = run_CNN(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "            elif wm == 5:\n",
    "                # Sequential & Spatial : LSTM-CNN\n",
    "                m_name = 'LSTM-CNN'\n",
    "                model, dict_out, X_test, Y_test = run_LSTM_CNN(df_test2, X_cols, y_cols, ynum)\n",
    "                acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(dict_out, q=0)\n",
    "                acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(dict_out, q=1)\n",
    "                extra = np.nan\n",
    "                # ----------------\n",
    "                # Permutation importance of features : probe which features are most predictive\n",
    "                if fea == 0:\n",
    "                    marquers_important = permutation_importance_tensorflow(model, X_test, Y_test)\n",
    "                # ----------------\n",
    "                \n",
    "\n",
    "            # ----------------\n",
    "            # Save all data to array \n",
    "            res_permod.append([ynum, m_name, fea, X_cols, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test, marquers_important, extra])\n",
    "            # ----------------\n",
    "            \n",
    "        # Save data matrices to file per model result :\n",
    "        file_name = \"res_exp_%s_%s_%s_ynum%d_%s.pkl\" % (exp, ax_val, ss_val, ynum, m_name)\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(res_permod, open_file)\n",
    "        open_file.close()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mettez_dictout_dans_vars(dict_out, q):\n",
    "    if q == 0:\n",
    "        acc = dict_out['acc_train']\n",
    "        prec = dict_out['prec_train']\n",
    "        recall = dict_out['recall_train']\n",
    "        roc_auc = dict_out['roc_auc_train']\n",
    "    elif q == 1:\n",
    "        acc = dict_out['acc_test']\n",
    "        prec = dict_out['prec_test']\n",
    "        recall = dict_out['recall_test']\n",
    "        roc_auc = dict_out['roc_auc_test']\n",
    "    elif q == 2:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_dircalc']\n",
    "    elif q == 3:\n",
    "        acc = dict_out['acc_dircalc']\n",
    "        prec = dict_out['prec_dircalc']\n",
    "        recall = dict_out['recall_dircalc']\n",
    "        roc_auc = dict_out['rocauc_dircalc']\n",
    "            \n",
    "    return acc, prec, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols):\n",
    "\n",
    "    # Ensure that the X matrix size is correct\n",
    "    # df_test2 : (dp_per_sample*n_values, feature)\n",
    "    all_dp, cols = df_test2.shape\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Il faut change df_test2 à : (batch, timesteps, feature)\n",
    "    needed_samps_class, counted_value, count_index, st, endd = count_classes(df_test2)\n",
    "\n",
    "    tot = [endd[i]-st[i] for i in range(len(st))]\n",
    "    val = min(tot)\n",
    "\n",
    "    # Ensurez que X est le meme taille\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(tot)):\n",
    "        isamp = endd[i]-st[i]\n",
    "        diff = isamp - val\n",
    "        X.append(df_test2.iloc[st[i]:endd[i]-diff, X_cols].to_numpy())\n",
    "        Y.append(df_test2.iloc[st[i]:endd[i]-diff, y_cols].to_numpy())\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Tensorflow says to use :\n",
    "    batch, timesteps, feature = X.shape\n",
    "\n",
    "    print('batch:' , batch)\n",
    "    print('timesteps:' , timesteps)\n",
    "    print('feature:' , feature)\n",
    "    \n",
    "    print('taille de X:' , X.shape)\n",
    "    # X.shape =  (104570, 20, 1)   # batch, timesteps/sequence length, feature\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    batch, timesteps, n_outputs = Y.shape   # batch, timesteps, 1\n",
    "    print('taille de Y:' , Y.shape)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Split the X an y data into test and train\n",
    "    seed = 0\n",
    "    test_size = 0.25 # default\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = seed, test_size = test_size)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    \n",
    "    print('X_train:' , X_train.shape)\n",
    "    print('Y_train:' , Y_train.shape)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    print('X_test:' , X_test.shape)\n",
    "    print('Y_test:' , Y_test.shape)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Ensure that Y_train and Y_test are integers\n",
    "    Y_test = Y_test.astype(int)\n",
    "    Y_train = Y_train.astype(int)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # batch_train, timesteps_train, feature_train = X_train.shape\n",
    "    # batch_test, timesteps_test, feature_test = X_test.shape\n",
    "    \n",
    "    # OU\n",
    "    \n",
    "    suf = ['train', 'test']\n",
    "    noms = ['batch_', 'timesteps_', 'feature_']\n",
    "    dictkeys = [j+i for i in suf for j in noms]\n",
    "    #print('dictkeys: ', dictkeys)\n",
    "\n",
    "    dictvals = []\n",
    "    dictvals.append(list(X_train.shape))\n",
    "    dictvals.append(list(X_test.shape))\n",
    "    dictvals = np.ravel(dictvals)\n",
    "    \n",
    "    info = dict(zip(dictkeys, dictvals))\n",
    "    info['n_outputs'] = n_outputs\n",
    "    # ----------------\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235fe8f",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_batch(X_train, X_test, Y_train, Y_test, info, batch_size):\n",
    "    \n",
    "    # ----------------------------\n",
    "    \n",
    "    # batch_size = 24\n",
    "    which_mod = 'SVC'  # 'NuSVC', 'SVC'\n",
    "    para = 'adaptive' # 'hyperparmetre', 'adaptive'\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    class_len = len(np.unique(Y_train))\n",
    "    if class_len <= 2:\n",
    "        dfs = 'binary'\n",
    "        dfs_var = 'ovo'\n",
    "    elif class_len > 2:\n",
    "        dfs = 'multi'\n",
    "        dfs_var = 'ovr'\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    testall = 'batch'\n",
    "    if testall == 'batch':\n",
    "        # Test on same batch for all X_train\n",
    "        st = np.random.permutation(int(np.ceil(info['batch_test']/batch_size)))[0]*batch_size\n",
    "        endd = st+batch_size\n",
    "\n",
    "        if endd < info['batch_test']:\n",
    "            X_test_batch = np.reshape(X_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['feature_test']))\n",
    "            Y_test_1D_batch = np.reshape(Y_test[st:endd,:,:], (batch_size*info['timesteps_test'], info['n_outputs']))\n",
    "        else:\n",
    "            batch_mod = info['batch_test']-st\n",
    "            endd = st+batch_mod\n",
    "            X_test_batch = np.reshape(X_test[st:endd,:,:], (batch_mod*info['timesteps_test'], info['feature_test']))\n",
    "            Y_test_1D_batch = np.reshape(Y_test[st:endd,:,:], (batch_mod*info['timesteps_test'], info['n_outputs']))\n",
    "    elif testall == 'toutes_donnes':\n",
    "        # Toutes des donnes\n",
    "        X_test_batch = np.reshape(X_test, (info['batch_test']*info['timesteps_test'], info['feature_test']))\n",
    "        Y_test_1D_batch = np.reshape(Y_test, (info['batch_test']*info['timesteps_test'], info['n_outputs']))\n",
    "\n",
    "    n = int(np.ceil(info['batch_train']/batch_size))\n",
    "    print('n: ', n)\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    C_l = np.linspace(0.9, 2, n)\n",
    "    gamma_l = np.linspace(1/info['feature_train'], 1/2, n)\n",
    "\n",
    "    C = 1  # Defaut\n",
    "    gamma = 1/info['feature_train']\n",
    "    inc_C = (0.9)*(1/10)\n",
    "    inc_gamma = (1/info['feature_train'])*(1/10)\n",
    "    # ----------------------------\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(n):\n",
    "        st = i*batch_size\n",
    "        endd = st+batch_size\n",
    "\n",
    "        # Stack all data over batches\n",
    "        if endd < info['batch_train']:\n",
    "            X_train_batch = np.reshape(X_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['feature_train']))\n",
    "            Y_train_1D_batch = np.reshape(Y_train[st:endd,:,:], (batch_size*info['timesteps_train'], info['n_outputs']))\n",
    "        else:\n",
    "            batch_mod = info['batch_train']-st\n",
    "            endd = st+batch_mod\n",
    "            X_train_batch = np.reshape(X_train[st:endd,:,:], (batch_mod*info['timesteps_train'], info['feature_train']))\n",
    "            Y_train_1D_batch = np.reshape(Y_train[st:endd,:,:], (batch_mod*info['timesteps_train'], info['n_outputs']))\n",
    "\n",
    "        # print('shape of X_train_batch : ', X_train_batch.shape)\n",
    "        # print('shape of Y_train_1D_batch : ', Y_train_1D_batch.shape)\n",
    "\n",
    "        # “one-versus-one” : binary ONLY, Y_train_1D, same implementation as libsvm \n",
    "        # (uses 1/lambda instead of C in cost function)\n",
    "        if para == 'hyperparmetre':\n",
    "            if which_mod == 'NuSVC':\n",
    "                model = svm.NuSVC(decision_function_shape=dfs_var, gamma=gamma_l[i], probability=True, max_iter=-1)\n",
    "            elif which_mod == 'SVC':\n",
    "                model = svm.SVC(decision_function_shape=dfs_var, C=C_l[i], probability=True, max_iter=-1)\n",
    "        elif para == 'adaptive':\n",
    "            if which_mod == 'NuSVC':\n",
    "                model = svm.NuSVC(decision_function_shape=dfs_var, gamma=gamma, probability=True, max_iter=-1)\n",
    "            elif which_mod == 'SVC':\n",
    "                model = svm.SVC(decision_function_shape=dfs_var, C=C, probability=True, max_iter=-1)\n",
    "\n",
    "        model.fit(X_train_batch, Y_train_1D_batch)\n",
    "        # ----------------------------\n",
    "\n",
    "        Y_train_1D_predict = model.predict(X_train_batch)\n",
    "        Y_test_1D_predict = model.predict(X_test_batch)\n",
    "\n",
    "        # The prediction probability of each class : is size [n_samples, n_classes]\n",
    "        Y_train_bin_pp = model.predict_proba(X_train_batch) \n",
    "        Y_test_bin_pp = model.predict_proba(X_test_batch)\n",
    "\n",
    "        Y_train_bin_pp = np.array(Y_train_bin_pp)\n",
    "        # print('shape of Y_train_bin_pp : ', Y_train_bin_pp.shape)\n",
    "        Y_test_bin_pp = np.array(Y_test_bin_pp)\n",
    "        # print('shape of Y_test_bin_pp : ', Y_test_bin_pp.shape)\n",
    "\n",
    "        # How confidently each value predicted for x_test by the classifier is Positive ( large-magnitude Positive value ) or Negative ( large-magnitude Negative value)\n",
    "        Y_train_score = model.decision_function(X_train_batch)  # size is [n_samples, 1]\n",
    "        Y_test_score = model.decision_function(X_test_batch)\n",
    "\n",
    "        Y_train_score = np.array(Y_train_score)\n",
    "        Y_test_score = np.array(Y_test_score)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        value_pack_train = evaluation_methods(model, X_train_batch, Y_train_1D_batch, \n",
    "                                              Y_train_1D_predict, Y_train_bin_pp, Y_train_score, dfs)\n",
    "        value_pack_test = evaluation_methods(model, X_test_batch, Y_test_1D_batch, \n",
    "                                              Y_test_1D_predict, Y_test_bin_pp, Y_test_score, dfs)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        if i > 0:\n",
    "            acc_test_prev = acc_test\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        acc_train, prec_train, recall_train, roc_auc_train = mettez_dictout_dans_vars(value_pack_train, q=2)\n",
    "        acc_test, prec_test, recall_test, roc_auc_test = mettez_dictout_dans_vars(value_pack_test, q=3) \n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "        if which_mod == 'SVC':\n",
    "            if i > 0:\n",
    "                if acc_test_prev > acc_test:\n",
    "                    C = C - inc_C\n",
    "                else:\n",
    "                    C = C + inc_C\n",
    "            results.append([i, C, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test])\n",
    "\n",
    "        elif which_mod == 'NuSVC':\n",
    "            if i > 0:\n",
    "                if acc_test_prev > acc_test:\n",
    "                    gamma = gamma + inc_gamma\n",
    "                else:\n",
    "                    gamma = gamma - inc_gamma\n",
    "            results.append([i, gamma, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test])\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "\n",
    "        # Save model to file\n",
    "        file_name = \"model_%d.pkl\" % (i)\n",
    "        save_dat_pickle(model, file_name=file_name)\n",
    "\n",
    "        # Delete model\n",
    "        del model\n",
    "        del Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score\n",
    "\n",
    "\n",
    "    # Evaluatez quel modeles est mieux : AUCROC_test > 0.5 et max accuracy\n",
    "    results = np.array(results)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df2 = df[(df.iloc[:,9] > 0.5)]\n",
    "    best = df2.iloc[:,6].idxmax()\n",
    "    print('best : ', best)\n",
    "    print('C ou gamma : ', df.iloc[best,1])\n",
    "\n",
    "    acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test = map(float, results[best,2::])\n",
    "\n",
    "    # Load best model only for permutation importance\n",
    "    file_name = \"model_%d.pkl\" % (best)\n",
    "    model = load_dat_pickle(file_name=file_name)\n",
    "\n",
    "    # Delete all .pkl files\n",
    "    # del results\n",
    "    rm_list = [j for j in range(n) if j != best]\n",
    "    for i in rm_list:\n",
    "        os.remove(\"model_%d.pkl\" % (i))\n",
    "    \n",
    "    return model, acc_train, prec_train, recall_train, roc_auc_train, acc_test, prec_test, recall_test, roc_auc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_Y_bin_pp_2_Y_1D_pp(Y_1D, Y_bin_pp):\n",
    "\n",
    "    # Y_bin_pp is size [n_samples, n_classes=2]\n",
    "    # Take the column of Y_bin_pp for the class of Y_1D, because both vectors need to be [n_samples, 1]\n",
    "    Y_1D_pp = []\n",
    "    for q in range(len(Y_1D)):\n",
    "        desrow = Y_bin_pp[q]\n",
    "        Y_1D_pp.append(desrow[int(Y_1D[q])])\n",
    "    Y_1D_pp = np.ravel(Y_1D_pp)\n",
    "    \n",
    "    Y_1D_pp = np.array(Y_1D_pp)\n",
    "    \n",
    "    return Y_1D_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_methods(model, X, Y_1D, Y_1D_predict, Y_bin_pp, Y_score, dfs):\n",
    "    \n",
    "    average = 'micro'\n",
    "    acc_dircalc = metrics.accuracy_score(Y_1D, Y_1D_predict)\n",
    "    prec_dircalc = metrics.precision_score(Y_1D, Y_1D_predict, average=average)\n",
    "    recall_dircalc = metrics.recall_score(Y_1D, Y_1D_predict, average=average)\n",
    "    f1_dircalc = metrics.f1_score(Y_1D, Y_1D_predict, average=average)\n",
    "    \n",
    "    if dfs == 'binary':\n",
    "        # ----------------------------\n",
    "        Y_1D_pp = transform_Y_bin_pp_2_Y_1D_pp(Y_1D, Y_bin_pp)\n",
    "        Y_1D = np.array(Y_1D)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # prediction probability\n",
    "        rocauc_dircalc = metrics.roc_auc_score(Y_1D, Y_1D_pp, average=average)\n",
    "        \n",
    "    elif dfs == 'multi':\n",
    "    \n",
    "        # ----------------------------\n",
    "        # Need to binarize Y into size [n_samples, n_classes]\n",
    "        Y_bin, unique_classes = binarize_Y1Dvec_2_Ybin(Y_1D)\n",
    "        Y_bin = np.array(Y_bin)\n",
    "        # ----------------------------\n",
    "        \n",
    "        # decision function\n",
    "        rocauc_dircalc = metrics.roc_auc_score(Y_bin, Y_score, average=average)\n",
    "\n",
    "    \n",
    "    value_pack = {}\n",
    "    var_list = ['acc_dircalc', 'prec_dircalc', 'recall_dircalc', 'f1_dircalc', 'rocauc_dircalc']\n",
    "    var_list_num = [acc_dircalc, prec_dircalc, recall_dircalc, f1_dircalc, rocauc_dircalc]\n",
    "    \n",
    "    for q in range(len(var_list)):\n",
    "        value_pack['%s' % (var_list[q])] = var_list_num[q]\n",
    "    \n",
    "    return value_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78a3cb",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, X_test, Y_train_1D, Y_test_1D):\n",
    "    \n",
    "    # Determine if classes are binary or multiclass:\n",
    "    class_len = len(np.unique(Y_train_1D))\n",
    "    if class_len <= 2:\n",
    "        dfs = 'binary'\n",
    "    elif class_len > 2:\n",
    "        dfs = 'multi'\n",
    "    \n",
    "    forest = RandomForestClassifier(random_state=1, min_samples_leaf=50)  # min_samples_leaf is 100 by default\n",
    "    model = MultiOutputClassifier(forest, n_jobs=-1) #n_jobs=-1 means apply parallel processing\n",
    "    \n",
    "    Y_train_1D = np.reshape(Y_train_1D, (len(Y_train_1D), 1))  # Y needs to have a defined shape ***\n",
    "    model.fit(X_train, Y_train_1D)\n",
    "\n",
    "    # ------------------------------\n",
    "    \n",
    "    Y_train_1D_predict = model.predict(X_train)\n",
    "    Y_test_1D_predict = model.predict(X_test)\n",
    "\n",
    "    # ------------------------------\n",
    "    \n",
    "    # Binary : the prediction probability of each class : is size [n_samples, n_classes]\n",
    "    # Multi-class : the prediction probability of each class : size is [1, n_samples, n_classes]\n",
    "    Y_train_bin_pp = model.predict_proba(X_train) \n",
    "    Y_test_bin_pp = model.predict_proba(X_test)\n",
    "    \n",
    "    if dfs == 'binary':\n",
    "        Y_train_bin_pp = np.reshape(Y_train_bin_pp, (len(Y_train_1D_predict), 2))\n",
    "        Y_test_bin_pp = np.reshape(Y_test_bin_pp, (len(Y_test_1D_predict), 2))\n",
    "    elif dfs == 'multi':\n",
    "        unique_classes = np.unique(Y_train_1D)\n",
    "        Y_train_bin_pp = np.reshape(Y_train_bin_pp, (len(Y_train_1D), len(unique_classes)))\n",
    "        Y_test_bin_pp = np.reshape(Y_test_bin_pp, (len(Y_test_1D), len(unique_classes)))\n",
    "    \n",
    "    # ------------------------------\n",
    "    \n",
    "    # There is NO decision_function\n",
    "    # ------------------------------\n",
    "    if dfs == 'binary':\n",
    "        # size is [n_samples, 1]\n",
    "        Y_train_score = transform_Y_bin_pp_2_Y_1D_pp(Y_train_1D, Y_train_bin_pp)\n",
    "        Y_test_score = transform_Y_bin_pp_2_Y_1D_pp(Y_test_1D, Y_test_bin_pp)\n",
    "        # OR\n",
    "        # How confidently each value predicted for x_test by the classifier is Positive ( large-magnitude Positive value ) or Negative ( large-magnitude Negative value)\n",
    "        #Y_train_1D_score = model.decision_function(X_train)  # size is [n_samples, 1]\n",
    "        #Y_test_1D_score = model.decision_function(X_test)\n",
    "    elif dfs == 'multi':\n",
    "        # size is [n_samples, n_classes]\n",
    "        Y_train_score = Y_train_bin_pp\n",
    "        Y_test_score = Y_test_bin_pp\n",
    "        \n",
    "    Y_train_score = np.array(Y_train_score)\n",
    "    Y_test_score = np.array(Y_test_score)\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    return model, Y_train_1D_predict, Y_test_1D_predict, Y_train_bin_pp, Y_test_bin_pp, Y_train_score, Y_test_score, dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9877856",
   "metadata": {},
   "source": [
    "## CNN !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaef08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_CNN(df_test2, X_cols, y_cols, img_dim): \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    # X_train: (batch_train, timesteps_train, feature_train)\n",
    "    # Y_train: (batch_train, timesteps_train, n_outputs)\n",
    "    # X_test: (batch_test, timesteps_test, feature_train)\n",
    "    # Y_test: (batch_test, timesteps_test, n_outputs)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "    X_train_img = Xbtf_2_Xbii3(X_train, img_dim, info['batch_train'])\n",
    "    X_test_img = Xbtf_2_Xbii3(X_test, img_dim, info['batch_test'])\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Y_train_1D =  [Y_train[i,0:1,0] for i in range(info['batch_train'])]\n",
    "    Y_test_1D =  [Y_test[i,0:1,0] for i in range(info['batch_test'])]\n",
    "    Y_train_1D = np.array(Y_train_1D)\n",
    "    Y_test_1D = np.array(Y_test_1D)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # shape of X_train_img :  (batch_train, img_dim, img_dim, 3)\n",
    "    # shape of Y_train_1D :  (batch_train,1)\n",
    "    # shape of X_test_img :  (batch_test, img_dim, img_dim, 3)\n",
    "    # shape of Y_test_1D :  (batch_test,)\n",
    "    \n",
    "    X_train_img = np.asarray(X_train_img, dtype = np.float16, order ='C')  # np.float16, np.float32, np.float64\n",
    "    Y_train_1D = np.asarray(Y_train_1D, dtype = np.float16, order ='C')\n",
    "    X_test_img = np.asarray(X_test_img, dtype = np.float16, order ='C')\n",
    "    Y_test_1D = np.asarray(Y_test_1D, dtype = np.float16, order ='C')\n",
    "    \n",
    "    print('X_train_img:' , X_train_img.shape)\n",
    "    print('Y_train_1D:' , Y_train_1D.shape)\n",
    "    print('X_test_img:' , X_test_img.shape)\n",
    "    print('Y_test_1D:' , Y_test_1D.shape)\n",
    "    \n",
    "    return X_train_img, X_test_img, Y_train_1D, Y_test_1D, info\n",
    "    \n",
    "    \n",
    "# Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "def Xbtf_2_Xbii3(X, img_dim, batch):\n",
    "    X_img = []\n",
    "    for i in range(batch):\n",
    "        X_1D = X[i,:,:].flatten()\n",
    "        \n",
    "        if i == 0:\n",
    "            n = int(np.floor(np.sqrt(len(X_1D))))\n",
    "    \n",
    "        # fold into a square\n",
    "        mat = np.reshape(X_1D[0:n*n], (n, n))\n",
    "    \n",
    "        image = imgORmat_resize_imgORmat_CNN(img_dim, mat, inpt='mat2D', outpt='img3D', norm='non', thresh='non')\n",
    "        \n",
    "        X_img.append(image)\n",
    "    \n",
    "    X_img = np.array(X_img)\n",
    "    \n",
    "    return X_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Though for the 1D signal classification CNNs are also suitable, like they are implemented in the article \n",
    "# [49] for the seismic signal classification, while dealing with 2D objects CNNs can perform significantly \n",
    "# better results. Thus, firstly, we convert the 1D accelerometer signal into the 2D images via applying \n",
    "# CWT in order to extract signal features and, at the same time, to make it possible to implement 2D CNNs.'\n",
    "\n",
    "def MPCNN_arch(n_outputs, img_dim, rgb_layers, ynum):\n",
    "    \n",
    "    # Typical architecture MPCNN architecture using alternating convolutional and max-pooling layers. \n",
    "    \n",
    "    model = Sequential()  # initialize Sequential model\n",
    "    \n",
    "    mod = 0\n",
    "    if mod == 0:\n",
    "        model.add(Conv2D(32, (5,5), strides=(1,1), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "        model.add(Conv2D(32 * 2, (5,5), strides=(1,1), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    elif mod == 1:\n",
    "        model.add(Conv2D(8,(4,4), strides=(1,1), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D((8,8), strides=(8,8), padding='same'))\n",
    "        model.add(Conv2D(16,(2,2), strides=(1,1), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D((4,4), strides=(4,4), padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcgan_arch(n_outputs, img_dim, ynum, den_activation):\n",
    "    \n",
    "    hidden_dim = 128\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_dim * 4, activation=den_activation, input_shape=(img_di*img_dim, )))\n",
    "    model.add(Dense(hidden_dim * 2, activation=den_activation))\n",
    "    model.add(Dense(hidden_dim, activation=den_activation))\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderdecoder_arch(n_outputs, img_dim, rgb_layers, ynum):\n",
    "    \n",
    "    base_dimension = 64          \n",
    "    \n",
    "    model = Sequential()\n",
    "    # 1ère valeur (filters) : le nombre de tranches \"(kernel_val,kernel_val)\" qui composent l'image de sortie\n",
    "    # 2eme valeur (kernel_size) : la taille de la carre/filtre que on glisse au dessous l'image \n",
    "    # 3eme valeur (stride): Le plus grande le stride valeur le plus petite l'image sortie : on prends z_dim/stride_num\n",
    "    \n",
    "    # --------\n",
    "    # Entrée = (img_dim, img_dim, 1)\n",
    "    model.add(Conv2D(base_dimension, (5,5), strides=(2,2), padding='same', input_shape=(img_dim, img_dim, rgb_layers)))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "    # taille_sortie = (28 + 2*p - 5)/2 + 1\n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    # Entrée = \n",
    "    model.add(Conv2D(base_dimension * 2, (5,5), strides=(2,2), padding='same'))\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # Sortie = \n",
    "\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    # --------\n",
    "\n",
    "    # --------\n",
    "    model.add(Flatten())\n",
    "\n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 4096)\n",
    "    # --------\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    # initializer = tf.keras.initializers.HeNormal()\n",
    "    # initializer = tf.keras.initializers.GlorotUniform()\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "    \n",
    "    print('model.output_shape :', model.output_shape)\n",
    "    # model.output_shape : (None, 1)\n",
    "    # --------\n",
    "    \n",
    "    # Compile the model for training\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    \n",
    "    # model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfa4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, img_dim):\n",
    "    if type(img) != 'PIL.Image.Image':\n",
    "        rgb_image = Image.fromarray(img , 'RGB')\n",
    "    else:\n",
    "        rgb_image = img.convert('RGB')\n",
    "\n",
    "    # Resize image into a 64, 64, 3\n",
    "    new_h, new_w = int(img_dim), int(img_dim)\n",
    "    img3 = rgb_image.resize((new_w, new_h), Image.ANTIALIAS)\n",
    "    w_resized, h_resized = img3.size[0], img3.size[1]\n",
    "    return img3\n",
    "\n",
    "def convert_img_a_mat(img, outpt):\n",
    "    mat = np.array(img)  # Convert image to an array\n",
    "    if outpt == 'mat2D':\n",
    "        # Transformer l'image de 3D à 2D\n",
    "        # Convert image back to a 2D array\n",
    "        matout = np.mean(mat, axis=2)\n",
    "    elif outpt == 'img3D': # techniquement c'est un image parce qu'il y a trois RGB channels \n",
    "        matout = mat\n",
    "    return matout\n",
    "\n",
    "def norm_mat(mat2Dor3D, norm):\n",
    "    if norm == 'zero2one':\n",
    "        # Normalizer l'image entre 0 et 1\n",
    "        norout = mat2Dor3D/255\n",
    "    elif norm == 'negone2posone':\n",
    "        # Normalize the images to [-1, 1]\n",
    "        norout = (mat2Dor3D - 127.5) / 127.5\n",
    "    elif norm == 'non':\n",
    "        norout = mat2Dor3D\n",
    "    return norout\n",
    "\n",
    "def threshold_mat(mat2D, thresh):\n",
    "    # Threshold image\n",
    "    val = 255/2\n",
    "    if thresh == 'zero_moins_que_val':\n",
    "        row, col = mat2D.shape\n",
    "        mat_thresh = mat2D\n",
    "        min_val = np.min(mat_thresh)\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if mat_thresh[i,j] < val:\n",
    "                    mat_thresh[i,j] = min_val\n",
    "    elif thresh == 'non':\n",
    "        mat_thresh = mat2D\n",
    "    return mat_thresh\n",
    "\n",
    "\n",
    "# Des choix\n",
    "# inpt='img3D', 'mat2D' \n",
    "# outpt='mat2D', 'img3D' \n",
    "# norm = 'zero2one', 'negone2posone', 'non', \n",
    "# thresh='non', 'zero_moins_que_val'\n",
    "\n",
    "def imgORmat_resize_imgORmat_CNN(img_dim, data_in, inpt='img3D', outpt='mat2D', norm='non', thresh='non'):\n",
    "    if inpt == 'img3D' and outpt=='mat2D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='mat2D':\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        mat2D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(mat2D, norm)\n",
    "    elif inpt == 'mat2D' and outpt=='img3D':\n",
    "        img = Image.fromarray(data_in , 'L')\n",
    "        img = resize_img(img, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "    elif inpt == 'img3D' and outpt=='img3D':\n",
    "        img = resize_img(data_in, img_dim)\n",
    "        img3D = convert_img_a_mat(img, outpt)\n",
    "        out = norm_mat(img3D, norm)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81258a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, LeakyReLU, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()  # This allows you to use placeholder in version 2.0 or higher\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def run_CNN(df_test2, X_cols, y_cols, ynum):\n",
    "\n",
    "    # ----------------\n",
    "    img_dim = 64\n",
    "    \n",
    "    # Folding data into CNN image format:\n",
    "    X_train_img, X_test_img, Y_train_1D, Y_test_1D, info  = initialize_CNN(df_test2, X_cols, y_cols, img_dim)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            model = MPCNN_arch(n_outputs, img_dim, ynum)\n",
    "        elif i == 1:\n",
    "            den_activation = LeakyReLU(alpha=0.2)\n",
    "            model = dcgan_arch(n_outputs, img_dim, ynum, den_activation)\n",
    "        elif i == 2:\n",
    "            model = encoderdecoder_arch(n_outputs, img_dim, ynum)\n",
    "    \n",
    "        patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "        \n",
    "        if i == 1:\n",
    "            X_train_1D = []\n",
    "            for lay in range(X_train_img.shape[0]):\n",
    "                # Transformez 3D image à 2D matrix\n",
    "                # Train\n",
    "                X_train_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_train_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_train_1D.append(X_train_2D.flatten())\n",
    "            \n",
    "            X_test_1D = []\n",
    "            for lay in range(X_test_img.shape[0]):\n",
    "                # Test\n",
    "                X_test_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_test_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_test_1D.append(X_test_2D.flatten())\n",
    "            \n",
    "            history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        else:\n",
    "            history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "    \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['mod_train'] = mod_type[a]\n",
    "    dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    cnn2D_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return cnn2D_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cb3ed",
   "metadata": {},
   "source": [
    "## LSTM !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    if stateful == True:\n",
    "        # Quand vous definez batch_input_shape, il faut que des entries de modele être le meme taille que le train dataset\n",
    "        model.add(LSTM(n_a, input_shape=(timesteps_train, feature), batch_input_shape=(batch, timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "    elif stateful == False:\n",
    "        model.add(LSTM(n_a, input_shape=(timesteps_train, feature), return_sequences=return_sequences, return_state=return_state, stateful=stateful))\n",
    "\n",
    "    # Types of W initializer :\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "\n",
    "    if ynum == 2:\n",
    "        model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "    else:\n",
    "        model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "    # Compile the model for training\n",
    "    # opt = keras.optimizers.Adam()\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    # opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "    # Si vous utilisez softmax activation, la taille de sortie est plus grand que deux donc il faut categorical_crossentropy\n",
    "    if ynum == 2:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "    else:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, info = df_2_XYtraintest_formatbatch_timestep_feature(df_test2, X_cols, y_cols)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_train = X_Y_a_tfobj(X_train, Y_train, info['batch_train'], return_sequences, stateless)\n",
    "    tf_test = X_Y_a_tfobj(X_test, Y_test, info['batch_test'], return_sequences, stateless)\n",
    "    \n",
    "    # side idea : if I give tf_test info['batch_train'] instead of info['batch_test'], could I run stateful=True\n",
    "    # for all data (train and test)?\n",
    "    # ----------------\n",
    "\n",
    "    return tf_train, tf_test, X_train, Y_train, X_test, Y_test, info\n",
    "\n",
    "\n",
    "\n",
    "def X_Y_a_tfobj(X, Y, batch_size, return_sequences, stateless):\n",
    "    \n",
    "    if return_sequences == False:  # one output per batch of samples\n",
    "        # Y shape : batch_size,      ie: Y.shape =  (104570,)\n",
    "        temp = [Y[i,0:1,:] for i in range(Y.shape[0])]\n",
    "        Y = np.array(temp)\n",
    "        Y = np.reshape(Y, (batch_size,))\n",
    "    elif return_sequences == True: # an output per each batch of samples\n",
    "        # Y shape : batch_size, timesteps, n_output\n",
    "        Y = np.array(Y)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    tf_data = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    \n",
    "    if stateless == True:  # sequence is non-causal\n",
    "        buffer_size = 1000\n",
    "        tf_data = tf_data.cache().shuffle(buffer_size).batch(batch_size)\n",
    "    elif stateless == False:  # sequence is causal\n",
    "        tf_data = tf_data.batch(batch_size)\n",
    "    \n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba908ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "def run_LSTM(df_test2, X_cols, y_cols, ynum):\n",
    "    \n",
    "    return_sequences = False # True=return a prediction at every batch sample, (default) False=return one prediction at the end of the batch\n",
    "    stateless = True # True=shuffle the batch samples/slices --samples are non-causal, False=do not shuffle slices---samples are causal\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Folding data into LSTM format:\n",
    "    tf_train, tf_test, X_train, Y_train, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # -------------------------------\n",
    "\n",
    "    # Model architecture\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    return_state = False # True=return a and c, (default) False=do not return hidden state (a) and cell state (c)\n",
    "    stateful = False #  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    liste_de_vals = [20, 40, 60, 70, 80, 100] # number of dimensions for the hidden state of each LSTM cell\n",
    "    \n",
    "    for n_a in liste_de_vals:\n",
    "\n",
    "        model = LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum)\n",
    "        # -------------------------------\n",
    "        \n",
    "        # 50 est trop, 20 est insuffisant \n",
    "        patience = 28 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "\n",
    "        if stateful == False:\n",
    "            history = model.fit(tf_train, epochs=epochs, validation_data=tf_test, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "            # OU\n",
    "            # history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "            # score = model.evaluate(tf_test, verbose=2)\n",
    "            # test_acc.append(score[1])\n",
    "            # test_loss.append(score[0])\n",
    "        elif stateful == True:\n",
    "            # On peut faire tf_train parce que le taille est fixer at batch_input_shape\n",
    "            history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "        \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['delay_train'] = liste_de_vals[a]\n",
    "    dict_out['delay_test'] = liste_de_vals[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    lstm_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return lstm_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c928478",
   "metadata": {},
   "source": [
    "# LSTM-CNN!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LSTMCNN(df_test2, X_cols, y_cols, ynum):\n",
    "    \n",
    "    return_sequences = False # True=return a prediction at every batch sample, (default) False=return one prediction at the end of the batch\n",
    "    stateless = True # True=shuffle the batch samples/slices --samples are non-causal, False=do not shuffle slices---samples are causal\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Folding data into LSTM format:\n",
    "    tf_train, tf_test, X_train, Y_train, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "\n",
    "    # Model architecture 1\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    return_state = False # True=return a and c, (default) False=do not return hidden state (a) and cell state (c)\n",
    "    stateful = False #  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "    \n",
    "    n_a = 32  # 20, 40\n",
    "    n_outputs = n_a\n",
    "    model = LSTM_arch(n_a, timesteps_train, feature, return_sequences, return_state, stateful, n_outputs, ynum)\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    # 50 est trop, 20 est insuffisant \n",
    "    patience = 28 # Number of epochs with no improvement after which training will be stopped.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    if stateful == False:\n",
    "        history = model.fit(tf_train, epochs=epochs, validation_data=tf_test, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    elif stateful == True:\n",
    "        # On peut faire tf_train parce que le taille est fixer at batch_input_shape\n",
    "        history = model.fit(tf_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "    # -------------------------------\n",
    "\n",
    "    lstm_model_best = model\n",
    "    \n",
    "    X_lstm_train = lstm_model_best.predict(X_train)\n",
    "    X_lstm_test = lstm_model_best.predict(X_test)\n",
    "    # should be (batch, n_a)\n",
    "    \n",
    "    X_lstm_train = np.array(X_lstm_train)\n",
    "    X_lstm_test = np.array(X_lstm_test)\n",
    "    \n",
    "    print('X_lstm_train : ', X_lstm_train.shape)\n",
    "    print('X_lstm_test : ', X_lstm_test.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # CNN\n",
    "    # -------------------------------\n",
    "    img_dim = 64\n",
    "    \n",
    "    # Folding data into CNN image format:\n",
    "    \n",
    "    # Tranformez X(batch, timestamps, feature) into X(batch, img_dim, img_dim, 3)\n",
    "    X_train_img = Xbtf_2_Xbii3(X_lstm_train, img_dim, X_lstm_train.shape[0])\n",
    "    X_test_img = Xbtf_2_Xbii3(X_lstm_test, img_dim, X_lstm_test.shape[0])\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Y_train_1D =  [Y_train[i,0:1,0] for i in range(Y_train.shape[0])]\n",
    "    Y_test_1D =  [Y_test[i,0:1,0] for i in range(Y_test.shape[0])]\n",
    "    Y_train_1D = np.array(Y_train_1D)\n",
    "    Y_test_1D = np.array(Y_test_1D)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # shape of X_train_img :  (batch_train, img_dim, img_dim, 3)\n",
    "    # shape of Y_train_1D :  (batch_train,1)\n",
    "    # shape of X_test_img :  (batch_test, img_dim, img_dim, 3)\n",
    "    # shape of Y_test_1D :  (batch_test,)\n",
    "    \n",
    "    X_train_img = np.asarray(X_train_img, dtype = np.float16, order ='C')  # np.float16, np.float32, np.float64\n",
    "    Y_train_1D = np.asarray(Y_train_1D, dtype = np.float16, order ='C')\n",
    "    X_test_img = np.asarray(X_test_img, dtype = np.float16, order ='C')\n",
    "    Y_test_1D = np.asarray(Y_test_1D, dtype = np.float16, order ='C')\n",
    "    \n",
    "    print('X_train_img:' , X_train_img.shape)\n",
    "    print('Y_train_1D:' , Y_train_1D.shape)\n",
    "    print('X_test_img:' , X_test_img.shape)\n",
    "    print('Y_test_1D:' , Y_test_1D.shape)\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Model architecture 2\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    tot = []\n",
    "    tot_mod = []\n",
    "    mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            model = MPCNN_arch(n_outputs, img_dim, ynum)\n",
    "        elif i == 1:\n",
    "            model = dcgan_arch(n_outputs, input_shape, ynum)\n",
    "        elif i == 2:\n",
    "            model = encoderdecoder_arch(n_outputs, img_dim, ynum)\n",
    "    \n",
    "        patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "        \n",
    "        # -------------------------------\n",
    "        \n",
    "        if i == 1:\n",
    "            X_train_1D = []\n",
    "            for lay in range(X_train_img.shape[0]):\n",
    "                # Transformez 3D image à 2D matrix\n",
    "                # Train\n",
    "                X_train_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_train_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_train_1D.append(X_train_2D.flatten())\n",
    "            \n",
    "            X_test_1D = []\n",
    "            for lay in range(X_test_img.shape[0]):\n",
    "                # Test\n",
    "                X_test_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_test_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "                X_test_1D.append(X_test_2D.flatten())\n",
    "            \n",
    "            history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        else:\n",
    "            history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
    "        \n",
    "        \n",
    "        history_df = pd.DataFrame(history.history)\n",
    "        out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "    \n",
    "        tot.append(out)\n",
    "        tot_mod.append(model)\n",
    "    \n",
    "    tot = np.array(tot)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    a = np.argmax(tot[:,1])  # train\n",
    "    b = np.argmax(tot[:,6])  # test\n",
    "    suf = ['train', 'test']\n",
    "    tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "    list2 = [j+i for i in suf for j in tr_noms]\n",
    "    list2\n",
    "\n",
    "    dict_out = {}\n",
    "    for i in range(len(list2)):\n",
    "        if i < len(list2)/2:\n",
    "            r = tot[a,i]\n",
    "        else:\n",
    "            r = tot[b,i]\n",
    "        dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "    # ajoutez au dictionaire\n",
    "    dict_out['mod_train'] = mod_type[a]\n",
    "    dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    cnn2D_model_best = tot_mod[b]\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return cnn2D_model_best, dict_out, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67eba11",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    # initialize a matrix angle_rads of all the angles\n",
    "    # Get the angles for the positional encoding\n",
    "    # pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "    # i --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]\n",
    "    pos = np.reshape(np.multiply(range(0, positions), 1), (positions,1))\n",
    "    i = np.reshape(np.multiply(range(0, d), 1), (1,d))\n",
    "    angle_rads = pos / (10000 ** ((2*(i//2))/d))\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # END CODE HERE\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45865137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Mask\n",
    "# Sequences longer than the maximum length of five will be truncated, and zeros will be added to the \n",
    "# truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum \n",
    "# length, they zeros will also be added for padding. \n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    # If x equals 0 it put a True, and if not puts a False\n",
    "    #out1 = tf.math.equal(x, 0)  \n",
    "    #print('out1 : ' + str(out1))\n",
    "\n",
    "    # Converts the boolean matrix to a float32\n",
    "    #out2 = tf.cast(out1, tf.float32)\n",
    "    #print('out2 : ' + str(out2))\n",
    "    \n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    # it takes the (3,5) sized seq matrix and makes it a (3, 1, 1, 5) matrix\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-ahead Mask\n",
    "# In training, you will have access to the complete correct output of your training example. \n",
    "# The look-ahead mask helps your model pretend that it correctly predicted a part of the output and \n",
    "# see if, without looking ahead, it can correctly predict the next output.\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    #Based on the lower and upper matrix diagonal keep values, it keeps \n",
    "    #the matrix entries and sets the other entries to zero\n",
    "    #If lower_mat_dia_keep=0 and upper_mat_dia_keep=1 it keeps the diagonal and one upper\n",
    "    #diagonal section, similarly lower_mat_dia_keep=-1 and upper_mat_dia_keep=0 it keeps\n",
    "    #one lower diagonal section and the diagonal\n",
    "    #out1 = tf.linalg.band_part(tf.ones((3, 3)), lower_mat_dia_keep, upper_mat_dia_keep)\n",
    "    #print('out1 : ' + str(out1))\n",
    "    \n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d04ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, embedding_dim) = (batch, timesteps, feature)\n",
    "    ])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim, fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "        # START CODE HERE\n",
    "        # calculate self-attention using mha(~1 line)\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to the self-attention output (~1 line)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer (~1 line)\n",
    "        out1 = self.layernorm1(tf.add(x, attn_output))  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output (~1 line)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer (~1 line)\n",
    "        out2 = self.layernorm2(tf.add(out1, ffn_output))  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        # END CODE HERE\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6, textORnot='timeseries'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.textORnot = textORnot\n",
    "        \n",
    "        if self.textORnot == 'text':\n",
    "            self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        if self.textORnot == 'text':\n",
    "            # Text ONLY : Pass input through the Embedding layer\n",
    "            x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim) = (batch, timesteps, feature)\n",
    "            # print('shape of embedding : ', x.shape)\n",
    "        \n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        # Remember to cast the embedding dimension to data type tf.float32 before computing the square root.\n",
    "        x *= tf.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        #print('shape of x : ' + str(x.shape))\n",
    "        #shape of x : (2, 3, 4) so loop over the first entry\n",
    "        \n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim) = (batch, timesteps, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80887fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeY3D_a_Y1D(Y):\n",
    "    # Y : (batch, timesteps, feature)\n",
    "    \n",
    "    batch = Y.shape[0]\n",
    "    \n",
    "    # Y shape : batch_size,      ie: Y.shape =  (104570,)\n",
    "    temp = [Y[i,0:1,0] for i in range(batch_size)]\n",
    "    Y = np.array(temp)\n",
    "    Y = np.reshape(Y, (batch,)) # Y : (batch, )\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "def run_Transformer(df_test2, X_cols, y_cols, ynum):\n",
    "    \n",
    "    tf_train, tf_test, X_train, Y_train, X_test, Y_test, info  = initialize_LSTM(df_test2, X_cols, y_cols, return_sequences, stateless)\n",
    "    batch = info['batch_train']\n",
    "    timesteps_train = info['timesteps_train'] \n",
    "    feature = info['feature_train']\n",
    "    n_outputs = info['n_outputs']\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    # Model architecture\n",
    "    \n",
    "    # Encoder (shuffling and transformation of the data)\n",
    "    tf.random.set_seed(10)\n",
    "    \n",
    "    # unique words, or unique data points to a certain precision\n",
    "    precision = 0.1\n",
    "    input_vocab_size=len(np.arange(0,1,precision))\n",
    "    \n",
    "    # the maximum number of words in each sentence, OR timesteps\n",
    "    maximum_position_encoding = timesteps_train\n",
    "    \n",
    "    encoderq = Encoder(num_layers=6, embedding_dim=feature, num_heads=1, fully_connected_dim=2*feature, \n",
    "                       input_vocab_size=input_vocab_size,\n",
    "                       maximum_position_encoding=maximum_position_encoding, dropout_rate=0.1, \n",
    "                       layernorm_eps=1e-6, textORnot='timeseries')\n",
    "    \n",
    "    training = True  # training for Dropout layers\n",
    "    mask = create_padding_mask(x)  # create_look_ahead_mask(x.shape[1]) # None\n",
    "    encoder_X_train = encoderq(X_train, training, mask)   # output is shape=(batch, timesteps, feature)\n",
    "    encoder_X_test = encoderq(X_test, training, mask)   # output is shape=(batch, timesteps, feature)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    # Final Fully Connected\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    \n",
    "    whichmod = 0\n",
    "    if whichmod == 0:\n",
    "        den_activation = 'relu'\n",
    "        model = dcgan_arch(n_outputs, img_dim, ynum, den_activation)\n",
    "    elif whichmod == 1:\n",
    "        model = Sequential()\n",
    "        initializer = tf.keras.initializers.HeUniform()\n",
    "        if ynum == 2:\n",
    "            model.add(Dense(n_outputs, activation='softmax', kernel_initializer=initializer))\n",
    "        else:\n",
    "            model.add(Dense(n_outputs, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "        # --------\n",
    "\n",
    "        # Compile the model for training\n",
    "        opt = keras.optimizers.Adam()\n",
    "        if ynum == 2:\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "        else:\n",
    "            model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])  # optimizer='adam'\n",
    "\n",
    "    \n",
    "    patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "\n",
    "    # -------------------------------\n",
    "    \n",
    "    X_train_1D = np.reshape(encoder_X_train, (batch, timesteps_train*feature))\n",
    "    X_test_1D = np.reshape(encoder_X_test, (batch, timesteps_train*feature))\n",
    "    Y_train_1D = reshapeY3D_a_Y1D(Y_train)\n",
    "    Y_test_1D = reshapeY3D_a_Y1D(Y_test)\n",
    "    \n",
    "    verbose = 2\n",
    "    history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=verbose)\n",
    "    # history_df = pd.DataFrame(history.history)\n",
    "    \n",
    "    # -------------------------------\n",
    "    \n",
    "    return model, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df83623",
   "metadata": {},
   "source": [
    "# Batch Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_script(exp, ax_val, ss_val):\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    df = get_df(ax_val, ss_val)\n",
    "\n",
    "    feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "    # Elasped time for feature processing :  715.9615476131439\n",
    "\n",
    "    del df\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    if exists(\"df_feat.pkl\") == False:\n",
    "        print('Creation des marquers')\n",
    "        df_feat = feature_creation_preprocessing(feat0, t_feat0)\n",
    "        del feat0, t_feat0\n",
    "        save_dat_pickle(df_feat, file_name=\"df_feat.pkl\")\n",
    "    else:\n",
    "        print('Load des marquers')\n",
    "        df_feat = load_dat_pickle(file_name=\"df_feat.pkl\")\n",
    "        \n",
    "    df_org = df_feat\n",
    "    \n",
    "    del df_feat\n",
    "    # ----------------\n",
    "\n",
    "    col_list = ['num', 'pos', 'vel', 'acc', 'pos_nc', 'vel_nc', 'acc_nc', 'pos_sl0', 'pos_sl1', 'pos_sl2', \n",
    "                'pos_sl3', 'pos_sl4', 'vel_sl0', 'vel_sl1', 'vel_sl2', 'vel_sl3', 'vel_sl4', 'acc_sl0', \n",
    "                'acc_sl1', 'acc_sl2', 'acc_sl3', 'acc_sl4', 'pos_spec', 'vel_spec', 'acc_spec', 'pos_cwt', \n",
    "                'vel_cwt', 'acc_cwt', 'kmeans']\n",
    "    df_org = pandas_rename_columns(df_org, col_list)\n",
    "    df_org = df_org.reset_index(drop=True)  # reset index : delete the old index column\n",
    "    # Gardez df_org\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    y_alllabel = [y1_feat0, y2_feat0, y3_feat0]\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Loop over the different y labels\n",
    "    for ynum in range(1):    #range(len(y_alllabel)):\n",
    "        print('ynum : ', ynum)\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # Select y\n",
    "        y_label = y_alllabel[ynum]\n",
    "        \n",
    "        # ----------------\n",
    "\n",
    "        # Ajoutez le column de label à la fin\n",
    "        y_pd = pd.Series(make_a_properlist(y_label))\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        df_feat = pd.concat([df_org, y_pd], axis=1)\n",
    "        df_feat = df_feat.rename({0: 'y'}, axis=1)\n",
    "\n",
    "        # ----------------\n",
    "\n",
    "        # Balancez des class/labels: pad\n",
    "        # Pad data 2 Make Classes Equivalent\n",
    "        df_test2 = pad_data_2makeclasses_equivalent(df_feat)\n",
    "        del df_feat\n",
    "        \n",
    "        # ----------------\n",
    "        \n",
    "        # classification for 3 partitions of feature data per 8 classifiers\n",
    "        # classify(df_test2, ynum)\n",
    "        \n",
    "        # del df_test2\n",
    "\n",
    "    return df_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae227ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "df_test2 = run_main_script(exp, ax_val, ss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed5608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2331a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5f754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fe411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Manual runs\n",
    "# Justification for not doing a loop : Can not do a loop because the computer stops or it stops \n",
    "# for weird reasons.  Have to run each manually.\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "# -------------\n",
    "# DONE\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sub'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'all'\n",
    "ss_val = 'sup'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'rot'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "# -------------\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax0'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax1'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n",
    "\n",
    "exp = 'trans'\n",
    "ax_val = 'ax2'\n",
    "ss_val = 'all'\n",
    "run_main_script(exp, ax_val, ss_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e6f8c",
   "metadata": {},
   "source": [
    "# Load saved data to see if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/oem2/Documents/9_Motor_classification_2018-22/Coding_version3_python_FINAL/e_classification/Resultats_j02m06a22/'\n",
    "file = 'res_exp_rot_all_all_ynum0_RF.pkl'\n",
    "file_name = path+file\n",
    "out = load_dat_pickle(file_name=file_name)\n",
    "dff = pd.DataFrame(out)\n",
    "dff.columns= ['ynum', 'm_name', 'fea', 'X_cols', 'acc_train', 'prec_train', 'recall_train', 'roc_auc_train', 'acc_test', 'prec_test', 'recall_test', 'roc_auc_test', 'marquers_important', 'extra']\n",
    "dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/oem2/Documents/9_Motor_classification_2018-22/Coding_version3_python_FINAL/e_classification/'\n",
    "file = 'res_exp_rot_all_all_ynum0_RF.pkl'\n",
    "file_name = path+file\n",
    "out = load_dat_pickle(file_name=file_name)\n",
    "dff = pd.DataFrame(out)\n",
    "dff.columns= ['ynum', 'm_name', 'fea', 'X_cols', 'acc_train', 'prec_train', 'recall_train', 'roc_auc_train', 'acc_test', 'prec_test', 'recall_test', 'roc_auc_test', 'marquers_important', 'extra']\n",
    "dff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e551c",
   "metadata": {},
   "source": [
    "# Individual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fea = 0\n",
    "\n",
    "# Order of which features to use in a model\n",
    "if fea == 0:\n",
    "    # 1) All features\n",
    "    X_cols = list(np.arange(1, df_test2.shape[1]-1, 1))\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 1:\n",
    "    # 2) first 3 from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[oo][0] for oo in range(3)]\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 2:\n",
    "    # 3) first 2 from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[oo][0] for oo in range(2)]\n",
    "    y_cols = [df_test2.shape[1]-1]\n",
    "elif fea == 3:\n",
    "    # 4) first feature from permutation_importance\n",
    "    X_cols = [list(marquers_important.items())[0][0]]\n",
    "    y_cols = [df_test2.shape[1]-1] \n",
    "\n",
    "\n",
    "# Spatial (find global trends in the feature) : CNN\n",
    "m_name = 'CNN'\n",
    "#dict_out, X_test, Y_test, desired_col = run_CNN(df, ynum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3aaf51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dict_out, X_test, Y_test, desired_col = run_CNN(df, ynum)\n",
    "\n",
    "# ----------------\n",
    "img_dim = 64\n",
    "\n",
    "# Folding data into CNN image format:\n",
    "X_train_img, X_test_img, Y_train_1D, Y_test_1D, info  = initialize_CNN(df_test2, X_cols, y_cols, img_dim)\n",
    "batch = info['batch_train']\n",
    "timesteps_train = info['timesteps_train'] \n",
    "feature = info['feature_train']\n",
    "n_outputs = info['n_outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f9c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------\n",
    "\n",
    "# Model architecture\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "\n",
    "tot = []\n",
    "tot_mod = []\n",
    "mod_type = ['mpcnn', 'dcgan', 'encdec'] # CNN model architecture type\n",
    "\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = MPCNN_arch(n_outputs, img_dim, X_train_img.shape[3], ynum)\n",
    "    elif i == 1:\n",
    "        model = dcgan_arch(n_outputs, img_dim, ynum)\n",
    "    elif i == 2:\n",
    "        model = encoderdecoder_arch(n_outputs, img_dim, X_train_img.shape[3], ynum)\n",
    "\n",
    "    patience = 5 # Number of epochs with no improvement after which training will be stopped.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
    "\n",
    "    # -------------------------------\n",
    "\n",
    "    if i == 1:\n",
    "        # Transformez 3D image à 2D matrix\n",
    "        X_train_1D = []\n",
    "        for lay in range(X_train_img.shape[0]):\n",
    "            X_train_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_train_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "            X_train_1D.append(X_train_2D.flatten())\n",
    "        \n",
    "        X_test_1D = []\n",
    "        for lay in range(X_test_img.shape[0]):\n",
    "            X_test_2D = imgORmat_resize_imgORmat_CNN(img_dim, X_test_img[lay,:,:,:], inpt='img3D', outpt='mat2D', norm='non', thresh='non')\n",
    "            X_test_1D.append(X_test_2D.flatten())\n",
    "        \n",
    "        X_train_1D = np.array(X_train_1D)\n",
    "        X_test_1D = np.array(X_test_1D)\n",
    "        print('X_train_1D : ', X_train_1D.shape)\n",
    "        print('X_test_1D : ', X_test_1D.shape)\n",
    "\n",
    "        history = model.fit(X_train_1D, Y_train_1D, epochs=epochs, validation_data=(X_test_1D, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    else:\n",
    "        history = model.fit(X_train_img, Y_train_1D, epochs=epochs, validation_data=(X_test_img, Y_test_1D), batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    out = [history_df.iloc[:,i].mean() for i in range(len(history_df.columns))]\n",
    "\n",
    "    tot.append(out)\n",
    "    tot_mod.append(model)\n",
    "\n",
    "tot = np.array(tot)\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "a = np.argmax(tot[:,1])  # train\n",
    "b = np.argmax(tot[:,6])  # test\n",
    "suf = ['train', 'test']\n",
    "tr_noms = ['loss_', 'acc_', 'prec_', 'recall_', 'roc_auc_']\n",
    "\n",
    "list2 = [j+i for i in suf for j in tr_noms]\n",
    "list2\n",
    "\n",
    "dict_out = {}\n",
    "for i in range(len(list2)):\n",
    "    if i < len(list2)/2:\n",
    "        r = tot[a,i]\n",
    "    else:\n",
    "        r = tot[b,i]\n",
    "    dict_out[list2[i]] = r\n",
    "\n",
    "\n",
    "# ajoutez au dictionaire\n",
    "dict_out['mod_train'] = mod_type[a]\n",
    "dict_out['mod_test'] = mod_type[b]\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "cnn2D_model_best = tot_mod[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd3e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93765f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ac84cbd",
   "metadata": {},
   "source": [
    "# Checking kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3a9838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmation : exp= rot , ax_val= all , ss_val= all\n"
     ]
    }
   ],
   "source": [
    "exp = 'rot'\n",
    "ax_val = 'all'\n",
    "ss_val = 'all'\n",
    "\n",
    "df = get_df(ax_val, ss_val)\n",
    "\n",
    "feat0, t_feat0, y1_feat0, y2_feat0, y3_feat0 = create_labels_and_initial_feature(df) \n",
    "# Elasped time for feature processing :  715.9615476131439\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77a4a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_vec_taille_row(vec):\n",
    "    # Does the same as np.reshape, but oddly np.reshape could not make a column vector a row vector\n",
    "    vec = np.array(vec)\n",
    "    if len(vec.shape) == 2:\n",
    "        if vec.shape[0] > vec.shape[1]:\n",
    "            vec = vec.T\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18b2443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num de samples avant dropna :  142\n",
      "num de samples apres dropna :  142\n",
      "part1 :  (471, 3)\n",
      "part2 :  (471, 3)\n",
      "cols6 :  (471, 5)\n",
      "cols7 :  (471, 5)\n",
      "cols8 :  (471, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9986/3488110942.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vec = np.array(vec)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ----------------\n",
    "# Make your features\n",
    "# ----------------\n",
    "df_feat = pd.DataFrame()\n",
    "\n",
    "n = 4   # filter order\n",
    "fs = 250 # data sampling frequency (Hz)\n",
    "fcc = 10  # Cut-off frequency of the filter\n",
    "w = fcc / (fs / 2) # Normalize the frequency\n",
    "b, a = signal.butter(n, w, 'low')  # 3rd order\n",
    "\n",
    "scales = np.arange(1, 128)\n",
    "\n",
    "print('num de samples avant dropna : ', len(feat0))\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Drop nan values from feat0\n",
    "temp = pd.DataFrame(feat0)\n",
    "temp0 = my_dropna_python(temp)\n",
    "feat0 = temp0.to_numpy()\n",
    "print('num de samples apres dropna : ', len(feat0))\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Drop all zero samples from feat0\n",
    "temp = []\n",
    "for i in range(len(feat0)):\n",
    "    if int(np.mean(feat0[i])) != 0:\n",
    "        temp.append(feat0[i])\n",
    "del feat0\n",
    "feat0 = temp\n",
    "del temp\n",
    "\n",
    "# ----------------\n",
    "\n",
    "\n",
    "# Need to find when one trial starts and end - take derivative from start-stop periods\n",
    "for i in range(0,1):   # range(len(feat0)):\n",
    "    \n",
    "    if i == 0:\n",
    "        plotORnot = 0 #1\n",
    "    else:\n",
    "        plotORnot = 0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Causale ordre\n",
    "    # ----------------------------\n",
    "    # (0) position\n",
    "    col0 = scale_feature_data(feat0[i], plotORnot)\n",
    "    col0 = np.ravel(col0)\n",
    "    col0 = force_vec_taille_row(col0)\n",
    "    \n",
    "    # (1) velocity\n",
    "    vel = numderiv(feat0[i], t_feat0[i])\n",
    "    col1 = scale_feature_data(vel, plotORnot)\n",
    "    col1 = make_a_properlist(col1)\n",
    "    col1 = force_vec_taille_row(col1)\n",
    "\n",
    "    # (2) acceleration\n",
    "    acc = numderiv(vel, t_feat0[i])\n",
    "    filtacc = signal.filtfilt(b, a, acc) # the signal is noisy\n",
    "    col2 = scale_feature_data(filtacc, plotORnot)\n",
    "    col2 = make_a_properlist(col2)\n",
    "    col2 = force_vec_taille_row(col2)\n",
    "    # ----------------------------\n",
    "    \n",
    "    part1 = pd.DataFrame(data=[col0, col1, col2]).T\n",
    "    print('part1 : ', part1.shape)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Non-causale ordre\n",
    "    # ----------------------------\n",
    "    # (4) position\n",
    "    col3 = normal_distribution_feature_data(col0, plotORnot)\n",
    "    col3 = make_a_properlist(col3)\n",
    "    col3 = force_vec_taille_row(col3)\n",
    "    \n",
    "    # (5) velocity\n",
    "    col4 = normal_distribution_feature_data(col1, plotORnot)\n",
    "    col4 = np.ravel(col4)\n",
    "    col4 = force_vec_taille_row(col4)\n",
    "\n",
    "    # (6) acceleration\n",
    "    col5 = normal_distribution_feature_data(col2, plotORnot)\n",
    "    col5 = np.ravel(col5)\n",
    "    col5 = force_vec_taille_row(col5)\n",
    "    # ----------------------------\n",
    "\n",
    "    part2 = pd.DataFrame(data=[col3, col4, col5]).T\n",
    "    print('part2 : ', part2.shape)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Frequence marquers : sublevels of frequency pattern\n",
    "    # ----------------------------\n",
    "    # (7-22) une transformation de fréquence (ondelettes)\n",
    "    coeff = tsig_2_discrete_wavelet_transform(col0, waveletname='sym5', level=5, plotORnot=0)\n",
    "    cols6 = pd.DataFrame(coeff).T\n",
    "    print('cols6 : ', cols6.shape)# col6 :  (471, 5)\n",
    "\n",
    "    coeff = tsig_2_discrete_wavelet_transform(col1, waveletname='sym5', level=5, plotORnot=0)\n",
    "    cols7 = pd.DataFrame(coeff).T\n",
    "    print('cols7 : ', cols7.shape)  # col7 :  (471, 5)\n",
    "\n",
    "    coeff = tsig_2_discrete_wavelet_transform(col2, waveletname='sym5', level=5, plotORnot=0)\n",
    "    cols8 = pd.DataFrame(coeff).T\n",
    "    print('cols8 : ', cols8.shape)   # col8 :  (471, 5)\n",
    "    # ----------------------------\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Hybrid marquers : temporalle et frequence information\n",
    "    # ----------------------------\n",
    "    # (8) spectrogram flatten - periodogram (fft)\n",
    "    col9 = tsig_2_spectrogram(col0, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "    col9 = np.ravel(col9)\n",
    "    col9 = force_vec_taille_row(col9)\n",
    "    \n",
    "    col10 = tsig_2_spectrogram(col1, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "    col10 = np.ravel(col10)\n",
    "    col10 = force_vec_taille_row(col10)\n",
    "\n",
    "    col11 = tsig_2_spectrogram(col2, fs=10, nfft=20, noverlap=0, plotORnot=0)\n",
    "    col11 = np.ravel(col11)\n",
    "    col11 = force_vec_taille_row(col11)\n",
    "    # ----------------------------\n",
    "    \n",
    "    part3 = pd.DataFrame(data=[col9, col10, col11]).T\n",
    "    print('part3 : ', part3.shape)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 2D continuous wavelet transform flattened\n",
    "    # ----------------------------\n",
    "    # continuous_wavelets = ['mexh', 'morl', 'cgau5', 'gaus5']\n",
    "    col12 = tsig_2_continuous_wavelet_transform(t_feat0[i], col0, scales, waveletname='mexh', plotORnot=0)\n",
    "    col12 = np.ravel(col12)\n",
    "    col12 = force_vec_taille_row(col12)\n",
    "    # print('col12 : ', col12.shape)   # col12 :  (471,)\n",
    "    \n",
    "    col13 = tsig_2_continuous_wavelet_transform(t_feat0[i], col1, scales, waveletname='mexh', plotORnot=0)\n",
    "    col13 = np.ravel(col13)\n",
    "    col13 = force_vec_taille_row(col13)\n",
    "    # print('col13 : ', col13.shape)   # col13 :  (471,)\n",
    "    \n",
    "    col14 = tsig_2_continuous_wavelet_transform(t_feat0[i], col2, scales, waveletname='mexh', plotORnot=0)\n",
    "    col14 = np.ravel(col14)\n",
    "    col14 = force_vec_taille_row(col14)\n",
    "    # print('col14 : ', col14.shape)   # col14 :  (471,)\n",
    "    # ----------------------------\n",
    "    \n",
    "    part4 = pd.DataFrame(data=[col12, col13, col14]).T\n",
    "    print('part4 : ', part4.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cd36184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.555250</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.523396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.567738</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.022841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.580315</td>\n",
       "      <td>2.631110</td>\n",
       "      <td>0.718676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.592985</td>\n",
       "      <td>2.490232</td>\n",
       "      <td>0.470080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.605750</td>\n",
       "      <td>2.386248</td>\n",
       "      <td>0.247844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>-2.386248</td>\n",
       "      <td>-1.013884</td>\n",
       "      <td>1.770789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>-2.490232</td>\n",
       "      <td>-0.996208</td>\n",
       "      <td>1.676619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>-2.631110</td>\n",
       "      <td>-0.970264</td>\n",
       "      <td>1.558356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>-2.858587</td>\n",
       "      <td>-0.957534</td>\n",
       "      <td>1.443299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-0.957534</td>\n",
       "      <td>1.357986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2\n",
       "0    0.555250  5.199338  1.523396\n",
       "1    0.567738  5.199338  1.022841\n",
       "2    0.580315  2.631110  0.718676\n",
       "3    0.592985  2.490232  0.470080\n",
       "4    0.605750  2.386248  0.247844\n",
       "..        ...       ...       ...\n",
       "466 -2.386248 -1.013884  1.770789\n",
       "467 -2.490232 -0.996208  1.676619\n",
       "468 -2.631110 -0.970264  1.558356\n",
       "469 -2.858587 -0.957534  1.443299\n",
       "470 -5.199338 -0.957534  1.357986\n",
       "\n",
       "[471 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d76e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col1 = make_a_properlist(col1)\n",
    "col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e78341c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963067</td>\n",
       "      <td>[0.14841776649745575]</td>\n",
       "      <td>[0.002092661540859374]</td>\n",
       "      <td>[0.5552503935714771]</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.523396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>[0.14841776649745542]</td>\n",
       "      <td>[-0.006755454789378098]</td>\n",
       "      <td>[0.5677381747778057]</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.022841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965185</td>\n",
       "      <td>[0.14534720812182894]</td>\n",
       "      <td>[-0.015430532039416918]</td>\n",
       "      <td>[0.5803151278188501]</td>\n",
       "      <td>2.63111</td>\n",
       "      <td>0.718676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966223</td>\n",
       "      <td>[0.14321116751269505]</td>\n",
       "      <td>[-0.023695450303675408]</td>\n",
       "      <td>[0.5929845539816321]</td>\n",
       "      <td>2.490232</td>\n",
       "      <td>0.47008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967245</td>\n",
       "      <td>[0.14227309644669214]</td>\n",
       "      <td>[-0.03134344740029099]</td>\n",
       "      <td>[0.6057498875430153]</td>\n",
       "      <td>2.386248</td>\n",
       "      <td>0.247844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>[-0.5116823148646104]</td>\n",
       "      <td>[0.004280192966172175]</td>\n",
       "      <td>[-2.386247821611854]</td>\n",
       "      <td>-1.013884</td>\n",
       "      <td>1.770789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010939</td>\n",
       "      <td>[-0.5113109074929155]</td>\n",
       "      <td>[0.003328674363325968]</td>\n",
       "      <td>[-2.4902324422289266]</td>\n",
       "      <td>-0.996208</td>\n",
       "      <td>1.676619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00729</td>\n",
       "      <td>[-0.5107761105130509]</td>\n",
       "      <td>[0.0023249650381623344]</td>\n",
       "      <td>[-2.631110406252946]</td>\n",
       "      <td>-0.970264</td>\n",
       "      <td>1.558356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>[-0.5107761105129706]</td>\n",
       "      <td>[0.0012999463359109945]</td>\n",
       "      <td>[-2.858587364286917]</td>\n",
       "      <td>-0.957534</td>\n",
       "      <td>1.443299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.5107761105129706]</td>\n",
       "      <td>[0.00028555961312242133]</td>\n",
       "      <td>[-5.199337582605575]</td>\n",
       "      <td>-0.957534</td>\n",
       "      <td>1.357986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1                      2                         3  \\\n",
       "0    0.0  0.963067  [0.14841776649745575]    [0.002092661540859374]   \n",
       "1    0.0  0.964126  [0.14841776649745542]   [-0.006755454789378098]   \n",
       "2    0.0  0.965185  [0.14534720812182894]   [-0.015430532039416918]   \n",
       "3    0.0  0.966223  [0.14321116751269505]   [-0.023695450303675408]   \n",
       "4    0.0  0.967245  [0.14227309644669214]    [-0.03134344740029099]   \n",
       "..   ...       ...                    ...                       ...   \n",
       "466  0.0  0.014591  [-0.5116823148646104]    [0.004280192966172175]   \n",
       "467  0.0  0.010939  [-0.5113109074929155]    [0.003328674363325968]   \n",
       "468  0.0   0.00729  [-0.5107761105130509]   [0.0023249650381623344]   \n",
       "469  0.0  0.003645  [-0.5107761105129706]   [0.0012999463359109945]   \n",
       "470  0.0       0.0  [-0.5107761105129706]  [0.00028555961312242133]   \n",
       "\n",
       "                         4         5         6  \n",
       "0     [0.5552503935714771]  5.199338  1.523396  \n",
       "1     [0.5677381747778057]  5.199338  1.022841  \n",
       "2     [0.5803151278188501]   2.63111  0.718676  \n",
       "3     [0.5929845539816321]  2.490232   0.47008  \n",
       "4     [0.6057498875430153]  2.386248  0.247844  \n",
       "..                     ...       ...       ...  \n",
       "466   [-2.386247821611854] -1.013884  1.770789  \n",
       "467  [-2.4902324422289266] -0.996208  1.676619  \n",
       "468   [-2.631110406252946] -0.970264  1.558356  \n",
       "469   [-2.858587364286917] -0.957534  1.443299  \n",
       "470   [-5.199337582605575] -0.957534  1.357986  \n",
       "\n",
       "[471 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1 = pd.DataFrame(data=[num, col0, col1, col2, col3, col4, col5]).T\n",
    "part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31f947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0c82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e56ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43a4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_org = df_feat\n",
    "\n",
    "del df_feat\n",
    "# ----------------\n",
    "\n",
    "col_list = ['num', 'pos', 'vel', 'acc', 'pos_nc', 'vel_nc', 'acc_nc', 'pos_sl0', 'pos_sl1', 'pos_sl2', \n",
    "            'pos_sl3', 'pos_sl4', 'vel_sl0', 'vel_sl1', 'vel_sl2', 'vel_sl3', 'vel_sl4', 'acc_sl0', \n",
    "            'acc_sl1', 'acc_sl2', 'acc_sl3', 'acc_sl4', 'pos_spec', 'vel_spec', 'acc_spec', 'pos_cwt', \n",
    "            'vel_cwt', 'acc_cwt', 'kmeans']\n",
    "df_org = pandas_rename_columns(df_org, col_list)\n",
    "df_org = df_org.reset_index(drop=True)  # reset index : delete the old index column\n",
    "# Gardez df_org\n",
    "\n",
    "# ----------------\n",
    "\n",
    "y_alllabel = [y1_feat0, y2_feat0, y3_feat0]\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Loop over the different y labels\n",
    "for ynum in range(1):    #range(len(y_alllabel)):\n",
    "    print('ynum : ', ynum)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Select y\n",
    "    y_label = y_alllabel[ynum]\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Ajoutez le column de label à la fin\n",
    "    y_pd = pd.Series(make_a_properlist(y_label))\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    df_feat = pd.concat([df_org, y_pd], axis=1)\n",
    "    df_feat = df_feat.rename({0: 'y'}, axis=1)\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Balancez des class/labels: pad\n",
    "    # Pad data 2 Make Classes Equivalent\n",
    "    df_test2 = pad_data_2makeclasses_equivalent(df_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f49d41",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a757d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametre tuning : NuSVC\n",
    "acc_test = 0.6136727078891258\n",
    "acc_train = 0.929637526652452\n",
    "gamma_l[8] = 0.2222222222222222\n",
    "\n",
    "# Adaptive : NuSVC\n",
    "acc_test = 0.8214285714285714\n",
    "acc_train = 0.9360341151385928\n",
    "5 \t5.0 \t0.936034 \t0.936034 \t0.936034 \t0.447889 \t0.821429 \t0.821429 \t0.821429 \t0.310647\n",
    "gamma\n",
    "\n",
    "# Hyperparametre tuning : SVC\n",
    "acc_test = 0.7193496801705757\n",
    "acc_train = 0.9786780383795309\n",
    "C_l[12] = 1.56\n",
    "12 \t12.0 \t0.978678 \t0.978678 \t0.978678 \t0.668755 \t0.719350 \t0.719350 \t0.719350 \t0.866848\n",
    "\n",
    "# Adaptive : SVC - batch_size = 16\n",
    "acc_test = 0.7214818763326226  (test = batch)\n",
    "acc_test = 0.5898623764295406  (test = all data)\n",
    "acc_train = 0.9898720682302772\n",
    "C =  1.18\n",
    "\n",
    "12 \t12.0 \t1.18 \t0.968950 \t0.968950 \t0.968950 \t0.666856 \t0.589862 \t0.589862 \t0.589862 \t0.886586\n",
    "\n",
    "# Adaptive : SVC - batch_size = 24\n",
    "acc_test = 0.7214818763326226  (test = batch)\n",
    "acc_test = 0.5768753634425277  (test = all data)\n",
    "acc_train = 0.9349680170575693\n",
    "C =  1.18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
