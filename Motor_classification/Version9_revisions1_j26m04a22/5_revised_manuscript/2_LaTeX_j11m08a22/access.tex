\documentclass{ieeeaccess}

% --------------------------------------------
% Preamble
% --------------------------------------------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

% Additional packages
\usepackage[english]{babel}
\usepackage[margins]{trackchanges}

\usepackage{xcolor}
\usepackage{soul}

% --------------------------------------------

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% --------------------------------------------
% To print bibliography in order of citation:
\bibliographystyle{unsrt}		% Style of bibliography presentation
% \bibliographystyle{ieeetr}
% --------------------------------------------

\addeditor{JF}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{document}

% header on first page
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}

\title{Simulation and Classification of Spatial Disorientation in a Flight use-case using Vestibular Stimulation}
\author{\uppercase{Jamilah Foucher}\authorrefmark{1}, \IEEEmembership{Member, IEEE},
\uppercase{Anne-Claire Collet}\authorrefmark{5}, \uppercase{Kevin Le Goff}\authorrefmark{3}, \uppercase{Thomas Rakotomamonjy}\authorrefmark{2}, \uppercase{Valerie Juppet}\authorrefmark{4}, \uppercase{Thomas Descatoire}\authorrefmark{4}, \uppercase{Jerémie Landrieu}\authorrefmark{1}, \uppercase{Marielle Plat-Robain}\authorrefmark{3}, \uppercase{François Denquin}\authorrefmark{1,2}, \uppercase{Arthur Grunwald}\authorrefmark{6}, \uppercase{Jean-Christophe Sarrazin}\authorrefmark{2}, and \uppercase{Benoît G. Bardy}.\authorrefmark{1}}
\address[1]{EuroMov Digital Health in Motion, Univ Montpellier and IMT Mines Ales, Montpellier 34090 France (e-mail: benoit.bardy@umontpellier.fr)}
\address[2]{DTIS, ONERA, Salon de Provence 13300 France}
\address[3]{AIRBUS, Toulouse 31000 France}
\address[4]{AIRBUS Helicopters, Marignane 13700 France}
\address[5]{Human Design Group, Toulouse 31000 France}
\address[6]{Technion, Israel Institute of Technology 32000 Israel}
\tfootnote{This project was supported by the French Department of Civil Aviation (DGAC) and by the European Union (FEDER iMOSE).}

\markboth
{Jamilah Foucher \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Benoît G. Bardy \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding authors: Jamilah Foucher (e-mail: j622amilah@gmail.com).}.

\begin{abstract}
A commonly used definition of spatial disorientation (SD) in aeronautics is "an erroneous sense of one’s position and motion relative to the plane of the earth’s surface". SD has a wide range of situations and factors, it has been predominantly studied using reduced experimental contexts such as motion detection experimentation in isolation. Due to the fact that many SD use-cases are studied in isolation in a reduced manner, it is difficult to develop a generalized and fundamental understanding of the occurrence of SD and provide viable solutions. To investigate SD in a generalized manner, a two-part Human Activity Recognition (HAR) study was perform; Part I consisted of dataset creation using an in-flight piloting use-case experiment and Part II employed Machine Learning (ML), Deep Learning (DL), and unsupervised modeling methods on the SD dataset to predict the occurrence of SD. Specifically, in Part I, a generalized SD perception dataset was created using whole-body experimental motion detection methods in a naturalistic flight context; participant perceptual joystick response was measured during rotational or translational vestibular stimulation. Orientation, speed, and axis experimental condition differences were evaluated. It was found that SD occurred less for faster speeds than slower speeds, and specific orientations and axes were more difficult to detect motion. Physical disorientation was quantified using the simulator sickness questionnaire (SSQ) disorientation sub-scale and was statistically compared with motion detection performance; no statistically significant relationship between SD and physical disorientation was found. Part II evaluated ML, DL, and unsupervised clustering model parameters, including model architecture, data use-case, feature-type, feature quantity, ground-truth labeling, and unsupervised labeling. Long-Short Term Memory (LSTM), Random Forest (RF), and Transformer model architectures had the highest mean prediction accuracy of 0.84, 0.82, and 0.77 respectively, across experiments, data use-cases, ground-truth labels, feature-types, and feature quantities; the LSTM model required at least 4 seconds of data and at least three features were required. Using permutation importance a dependency score for time, frequency, and time \& frequency feature-types were calculated for each model architecture, demonstrating that model architecture dictated a preference for feature-type. Support Vector Machine (SVM), RF, and Multilayer Perceptron (MLP) models depended mostly on frequency features, whereas LSTM, Convolutional Neural Networks (CNN), and LSTM-CNN models strongly depended on time features; the Transformer architecture depended on time \& frequency features. The lenient ground-truth label characterized the feature data better than the strict and complex labels; the lenient label definition is in alignment with the standard definition of SD where successive errors signified SD and occasional errors did not. K-medoids unsupervised clustering using position and velocity features most accurately replicated all ground-truth labels, indicating that SD occurrence can be reliably predicted using general flight joystick data (e.g.: rand score of $\sim0.8$)

\end{abstract}

\begin{keywords}
Aircraft navigation, human computer interaction, joystick perceptual response, machine learning algorithms, motion detection, spatial disorientation, vestibular dead reckoning, human activity recognition.
\end{keywords}

% \titlepgskip=-15pt

\maketitle

\section{INTRODUCTION}
\label{INTRODUCTION}
% -------------------
% Description of SD
% -------------------
% - Define SD
% - State the problems of SD
%   1) SD happens often and result in serious accidents
%   2) it is difficult to recover from SD, the pilot must be able to think clearly and quickly when disorientation occurs
%   3) Current measures used in SD are questionnaires
%   4) Current solutions to treat SD
% - What exactly needs to be solved regarding SD
% - Last sentence : needs to talk about SD, motion detection, and HAR
\PARstart{S}{patial} Disorientation (SD), in aviation, is the failure to perceive orientation, position, or movement. It is caused by multiple factors including environmental references and conditions, experience, and stress. There are diverse types of SD symptoms, ranging from confusion to physical sickness, and currently there is no proven method or solution to prevent it \cite{Bles_2008_SD}, \cite{Gibb_2010_Aviation}, \cite{Perdriel_1980_SD}, \cite{Gillingham_1993_Spatial}, \cite{Previc_2004_Spatial}, \cite{Newman_2007_SD}.  International studies on the frequency and severity of SD accidents show that the cause of 6-32\% of major accidents are due to SD, similarly 15-26\% of fatal accidents are a result of SD \cite{Newman_2007_SD}. Recovery from SD is strongly connected to the pilot's awareness of the situation, and his/her ability to perform corrective control, despite the disorientation, to maintain aerodynamic stability; 80\% and 20\% of SD incidents are caused by unrecognized and recognized situations respectively \cite{Bles_2008_SD}. Current measures to document the occurrence of SD are personal reports of physical and perceptual experiences using questionnaire-based methods; measured human activity is rarely used to monitor the occurrence of SD. Current solutions to minimize and/or prevent SD are : 1) detailed categorization of physiological and environmental situations of when SD is likely to occur, we will call SD use-cases, 2) pilot education about the signs and symptoms of SD per SD use-case and pilot training to fly below physiological thresholds of the human vestibular system  \cite{Gillingham_1993_Spatial}, \cite{Newman_2007_SD}, \cite{Previc_2004_Spatial}. In summary, the current measures and solutions to treat SD do not significantly prevent or minimize the occurrence of SD. Additionally, there lacks general understanding of the onset of SD with respect to orientation, position, and speed using environmental references because it is difficult to label SD and non-SD time periods for time-series human activity measurements during "real-world" applications.

% -------------------
% Human activity recognition definition
% -------------------
Human Activity Recognition (HAR) is the research field in which time-series and/or video data are used with Machine Learning (ML) \& Deep Learning (DL) algorithms to predict human activity in unconstrained real-world situations. HAR encompasses three main fields of study: gait monitoring, human pose estimation, human activity recognition. Gait monitoring is the study of the human stride, often using accelerometer and/or gyroscope measures, such that health care rehabilitation and athletic performance performance can be quantified and monitored. Human pose estimation is the quantification and recognition of human action using 2D or 3D cameras, to detect life threatening, abnormal, and/or suspicious behavior. Finally, human activity recognition is the study of human behavior, including physical and long-term habits, using a wide variety of sensors such as accelerometers/gyroscopes, cameras, RFIDs, and environmental measures \cite{Anguita_2012_SVMHumanActivity}, \cite{Dirgova_2022_Wearable}, \cite{Fu_2020_Sensing}, \cite{Von_2017_Sparse}, \cite{Nedorubova_2021_CWT_CNN_HumanActivity}, \cite{An_2021_Mgait} \cite{Xiao_2003_DeepLearning}. We hypothesize that the occurrence of SD can be predicted from human activity measurements as has been proven in the field of HAR. Therefore, in this study we propose a motion detection experiment where SD orientation, position, and speed situations are induced, similar to typical walking and running scenarios in HAR, and human activity of one's perceived position and orientation are measured.

% -------------------
% Current situation of SD research
% -------------------
SD has been investigated in various scientific fields, including aviation, psychophysical human motion detection, control theory, neuroscience, and Neuroergonomics. Depending on the scientific field, the approach to quantify human behavior during the occurrence of SD has been recasted in terms of the field's speciality. From an aviation approach, SD is investigated in terms of use-case where factors (e.g.: environmental, psychological, etc) that give rise to each use-case are studied individually and with respect to each other, with the goal of creating an instruction map of how to behave in a certain manner such that SD is not induced. 22+ SD use-cases have been categorized, including the well-known somatogravic and black-hole illusions \cite{Newman_2007_SD}, \cite{Gillingham_1993_Spatial}.  Identified human physicological vestibular thresholds were typically used to restrict certain flight maneuvers, to reduce the likelihood of SD \cite{Gillingham_1993_Spatial}, \cite{Previc_2004_Spatial}. 

The psychophysical human motion detection approach is to investigate behavioral response during varied situational stimuli for specific SD use-cases, with the goal of understanding the range of human response during isolated or mixed stimulus situations. Results assist with clarifying the instruction map of how to behave to prevent SD, or sensorial solutions are developed to modify human response during exposure to use-case stimuli. For instance, directional perception error was investigated in a realistic helicopter task in which participants were asked to point towards the sky to demonstrate a non-SD state \cite{Cheung_2000_Disorientation}. Similarly, continuous heading detection was investigated using a compensatory task in which perceived heading was measured with respect to a remembered target \cite{Sargent_2008_Disorientation}. Most recently, the individual and interactive influences of optical and gravito-inertial stimuli during simulated low-altitude flight demonstrated the importance of sensory integration effects on height perception using joystick response \cite{Denquin_2021_LAF}. 

The control theory approach to motion detection is to model typical human response during use-case stimuli and compare error between predicted and actual human response. Results assist in real-time monitoring of motion detection \cite{Soyka_2011_Predicting}. Neuroscience and Neuroergonomics approaches measure central nervous system mechanisms, including the brain and electrodermal activity, such that changes in physiological signals can assist in understanding neural mechanisms involved during SD. Insensate, unperceived, and perceived SD occurrence use different neuro mechanisms, thus measuring these neuro mechanisms would allow for detection of SD \cite{Hao_2020_Classification}. 

These different perspectives of studying SD are useful and provide insightful information regarding human response in realistic contexts. However many of the mentioned studies, research SD per use-case instead of trying to identify SD in a generalized manner. We believe that it is possible to measure general human flight activity, like a tracking task signal, and predict the occurrence of SD regardless of an SD use-case context, using HAR modeling methods. HAR has not been applied to the problem of SD in aviation such that SD is recognized and detected using sensor information. However, HAR has been investigated in the field of aviation. Moreover, human activity was the quantification of frequency and location interactions between agents \cite{Fusier_2007_ComplxexActivity}. Pilot activity was measured using hand, arm, and body positional movements' in 3D video images \cite{Ding_2015_Surveillance}.

\indent In this study, we investigated human activity during SD in two-steps: 1) motion perception experimental methods to create a generalized SD occurrence dataset containing a perceptual feedback measure, and 2) ML, DL, and unsupervised clustering methods were used to identify optimal modeling parameters for predicting SD. From a HAR perspective SD can be modeled and predicted in a generalized manner, by measuring a piloting activity measurement like joystick motion or body movement during labeled moments of SD and non-SD; where labeled moments of non-SD are determined based on correct task completed. During the dataset creation phase, we used existing motion detection experimental design methodologies and designed a generalized motion detection experiment. A vestibular whole-body compensatory task in darkness was used to produce realistic motion cues that a pilot might experience, and motion detection behavior was recorded via joystick movements. Two experiments were conducted: rotational and translational motion detection tasks. The rotational and translational experiments administered angular and linear whole-body stimulation, around and along the three Cartesian coordinate frame axes respectively, using a motion simulation system. Participants were given randomized combinations of three parameters that created angular or linear motion stimuli: axis, direction along the axis, and speed. The goal of the dataset creation phase was to create a realistic and diverse dataset of the perceptual joystick motion with respect to the occurrence of SD. The motivation was not to identify vestibular thresholds and report corresponding behavior, but to recreate realistic flight response data in a controlled manner such that states of disorientation could be modeled. Participant responses were into four performance categories, where orientation, speed, and axis experimental conditions were evaluated with respect to motion detection performance. Finally during the dataset creation step, the relationship between physical sickness symptoms and motion disorientation was statistically investigated with respect to motion detection performance in order to identify potential physiological markers for SD prediction; physical sickness was quantified using a generalized disorientation test for humans called the simulator sickness questionnaire (SSQ) disorientation sub-scale \cite{Kennedy_1993_Simulator}, \cite{Bouchard_2007_SimulatorSickness}. We hypothesized that participants who correctly detected motion, implying that they do not have SD, would not have physical sickness. The feasibility of other measures for capturing abnormal piloting activity, signalling the occurrence of SD, are discussed. In Part II, using the generalized SD dataset, DL methods were chosen for SD modeling because of their reliable and effective predictive capabilities in real-world situations \cite{Dirgova_2022_Wearable, Xiao_2003_DeepLearning}. SD classification analysis included; 1. creation of ground-truth SD labels from identified performance categories in Part I, 2. creation of 27 unique features from the perceptual joystick feedback measure, 3. comparison of metrics for five main modeling parameters: model architecture, data use-case, feature-type, feature quantity, and ground-truth labeling. Finally, unsupervised clustering labels were compared with ground-truth SD labels using the rand score; clustering models and features that most accurately replicated ground-truth labels were reported.


\section{RELATED WORKS}

\subsection{EXPERIMENTAL MOTION DETECTION}
Vibration or motion, measured by the human vestibular system, contains important information about the environment and our orientation and position with respect to the environment. Motion detection is the act of discerning self-motion with respect to a reference in the environment \cite{Chaudhuri_2013_Wholebody}. Human motion detection and perception are quantified by stimulating the vestibular system systematically via different vibrational and motion experimental paradigms \cite{Angelaki_2008_Vestibular}. Initially, motion detection was quantified by observing at which directions and speeds/accelerations, angular or linear, humans could perceive self-motion. The observed values where humans could not perceive correct self-motion were called vestibular thresholds or motion detection thresholds. Movement speed and acceleration influence motion perception. Earlier motion detection studies targeted aviation applications, where thresholds were often reported in terms of acceleration because flight instrumentation and behavioral interpretation was more accessible in terms of acceleration than speed \cite{Melvill_1978_Vertical}. Whereas recent motion detection studies use robotic motion simulation and often report thresholds in terms of speed because robotic motion planning is more reliable in terms of speed than acceleration \cite{BermudezRey_2016_Vestibular}, \cite{Hartmann_2014_Direction}, \cite{Karmali_2017_Multivariate}, \cite{Valko_2012_Vestibular}. Both speed and acceleration motion detection thresholds are comparable because they are directly related with the derivative or integral function. Earlier experimental paradigms included the usage of different experimental conditions such as magnitude and frequency of speed or acceleration motion stimuli trajectory, sequence and exposure time of movement and non-movement events, movement direction with respect to the orientation of the head, and whole-body stimulation \cite{Melvill_1978_Vertical}. Recent motion perception research has adopted robotic simulation tools and standardized experimental paradigms, including a greater range of test motion frequencies, allowing for more precise and consistent motion detection boundaries for a large variety of perceptual situations. Additionally, vestibular motion perception studies investigate context-driven parameters, such as 1) position and acceleration stimuli trajectory, direction, and rate; 2) vestibular dysfunction vs control detection; 3) orientation and/or movement of the user's body during exposure to stimuli; 4) expertise vs novice detection; and 5) age. Depending on the context parameters and the stimuli trajectory, the vestibular-proprioceptive system detects motion differently, and thus, behavioral responses are different \cite{Soyka_2011_Predicting}, \cite{Valko_2012_Vestibular}, \cite{Hartmann_2014_Direction}, \cite{BermudezRey_2016_Vestibular}, \cite{Karmali_2017_Multivariate}. For SD applications, motion detection thresholds were used as an indicator of SD awareness \cite{Gillingham_1993_Spatial}, \cite{Previc_2004_Spatial}. However, it remains uncertain how to reliably use thresholds to assist with SD in a functional aviation context. Regulating flight behavior with respect to thresholds does not directly measure or give feedback about the occurrence of SD, therefore we focus on sampling/collecting human activity data at various orientations or positions and speeds below and above known thresholds. Creating a predictive model from a wide variety of SD labeled data will allow for functional feedback with respect to SD.

\subsection{HUMAN ACTIVITY MEASUREMENTS}
% Past human activity measures : history of joysticks in human activity measures
The force sensor, such as a joystick, is one of the first human activity measurements. A joystick is a stick-like input device that is omni-directional with respect to its supporting base, such that the angle and direction corresponds to motion control of an object. Joysticks have been and/or are currently used in aviation, space, industry, military settings, and video gaming. Since the mid to late 1900s, human control using joysticks have been investigated in fields of human movement science in psychology and human-in-the-loop in automated control. Non-engineering and computational fields like psychology and neuroscience were included in early HAR-like endeavors because the goal was to control a machine using a human activity measure like a joystick, thus the underlying mechanisms of human movement needed to be investigated with and without the usage of human activity sensor measurements. Psychophysical tracking experimentations, both pursuit and compensatory tracking, were proven ways to quantify human control performance using force sensors. Statistical analysis and automated control modeling of tracking signals revealed that humans move in a consistent manner, such that velocity and/or acceleration of movement are modulated to perform a smooth position-based movement trajectory. Therefore, position, velocity, acceleration, and even the derivative of acceleration called jerk of human motion response were investigated to understand optimal human control of machines using joysticks. For example, a key realisation for joystick usage was that human operators could control positional outputs more smoothly and precisely when the velocity or acceleration of their angular and directional inputs were used.

% Modern human activity measures
The intention of human activity measurements has changed with the advent of many types of affordable and portable sensors, that can be easily put in the environment and on humans \cite{Fu_2020_Sensing}. Therefore, instead of studying human movement with respect to the sensor under different experimentally controlled scenarios, as was done in human movement science and human-in-the-loop control, researchers can investigate more "real-world" problems without sacrificing measurement accuracy. Similarly, in non-engineering and computational fields human movement has been sufficiently explored under controlled experimental contexts, thus naturalistic uncontrolled studies from an ecological perspective are of interest. Thus, newly developed domains such as HAR and Neuroergonomics, stemming from traditional engineering \& computer science and neuroscience fields respectively, have developed with purpose of quantifying and predicting human behavior in "real-world" settings using sensor fusion. 

% Pros and cons of commonly used HAR measure
Commonly used sensors in HAR are cameras, accelerometers, and gyroscopes; Inertial Measurement Units (IMUs), smartphones, video gaming technologies, and questionnaires are often used for data collection. Eventhough joysticks have been rigorously used and investigated as a human activity measure because they capture fine motor movements, IMUs are preferable in HAR because joystick devices capture 3D motion in a limited area bounded to the device base. Coupled IMU joystick devices used for gaming consoles capture both whole-body and small range limb/hand movements. IMUs can capture unbounded 3D motion, allowing them to be more suitable to capture movements over large distances and in areas with poor visibility and/or lighting. Activities such as walking, running, and stair climbing are often monitored using IMU devices. Despite the benefits of IMUs, IMUs can produce erroneous trajectories amplitudes caused by sensor drift, the estimation of the Euler angles from the raw inertial data, therefore sensor calibration before usage is necessary to minimize sensor error.

% Discussion of why we choose to use the joystick measure, eventhough IMUs and camera sensors capture a wider range of human activity
In this study, we selected a joystick to measure human activity instead of IMU sensors and/or a camera because the joystick is an existing cockpit instrument that is an extension of the pilot; no addition sensors or tools would need to be installed or approved in real-world settings. Joystick deflection were similarly used in a recent DL study to predict SD for space applications \cite{Wang_2022_Crash}.  

\subsection{ARTIFICIAL INTELLIGENCE METHODS FOR HAR}
\label{ARTIFICIAL_INTELLIGENCE_METHODS_FOR_HAR}
HAR data is typically in the form of time-series or images. Regarding time-series data, ML and DL models that accurately and sufficiently predict human activity are those that capture short to long range temporal dependencies.  One of the best ML models proven to capture temporal causality, using raw time-series data, was Support Vector Machine (SVM) \cite{Anguita_2012_SVMHumanActivity, Xiao_2003_DeepLearning}. SVM considers the entire feature space of the temporal data, thus temporal relationships are more likely to be found due to similar amplitudes. Depending on the feature transformation, subspace models like Random Forest (RF) maybe able to capture short range temporal dependencies. DL  algorithms improve prediction accuracy because they do not consider all of the data at one time, but they look at a window of temporal data only. Using precise windows of data these algorithms are able to better capture trends, both due to sequential order and amplitude, with respect to the given label.  The best DL algorithms that capture causal information for HAR data, both raw time-series and transformed time-series data, are Recurrent Neural Network (RNN) such as Long-Short Term Memory (LSTM), Convolutional Neural Networks (CNN) (1D and 2D), and Transformer \cite{Xiao_2003_DeepLearning, Xia_2020_LSTMCNN, Dirgova_2022_Wearable}. LSTM models are more effective than RNN because the cell architecture allows for past information within the specified window to be used for prediction, called a gating mechanism \cite{Sarang_2021_Tensorflow2}.  Moreover, 2D CNN has proven to have more predictive ability than the 1D CNN, due to the second dimensional space with respect to the neural network \cite{Nedorubova_2021_CWT_CNN_HumanActivity}. Depending on the feature, LSTM maybe more effective at prediction than CNN, and vice versa.  However, it remains unclear as to what temporal feature aspects render LSTM or CNN more or less effective. Finally, and most recently, Transformer models have been shown to reliably predict HAR activities.  Transformers, like LSTM, window the time-series data thus produce predictions based on specific sequentially transformed pieces of data.  Temporal, amplitude, and context similarity data aspects with respect to other features are evaluated, thus distinguishing data with respect to the corresponding label. Regarding image data, it has been shown that 2D CNN and Transformer architectures are more accurate than other architectures. Specifically for human pose estimation the Transformer model is able to decipher activity context with respect to previous frames better than 2D CNN \cite{Mazzia_2022_ActionTransformer}. 

Finally, hybrid architectures such as Transformer-CNN, CNN-Transformer, LSTM-CNN, and CNN-LSTM exploit both sequential and spatial aspects of the data. Regarding HAR accelerometer data, LSTM-CNN was shown to predict better than an LSTM \cite{Xia_2020_LSTMCNN}. As the Transformer architecture becomes more popular, hybrid architectures are less desirable due to the unnecessary complexity of steps is superior to hybrid models \cite{Dirgova_2022_Wearable}.

Previous works on prediction of human activity have efficiently compared and reported model architectures and features.  However, for certain HAR data, it is uncertain as to which underlying data feature properties cause accurate predictions, using specific models.  For example, it is uncertain to what degree time and/or frequency signal dynamics contribute to each models' ability to predict. Therefore, as previously mentioned, in this work it is of interest to compare prediction accuracy for different feature dynamics and model architectures.




\section{PART I: SD DATASET CREATION USING MOTION DETECTION EXPERIMENTATION}
The rotational and translational SD motion detection experiments were designed identically such that the resulting SD dataset would be in a standard format. The following experimental parameters were the same for both experiments: experimental stimulus conditions, number of randomized trials per experiment, timeline of experimental events per trial, experimental protocol, and motion simulation system. In order to validate our SD dataset using our motion simulator platform, we followed recent motion detection protocols that also used motion simulator platforms such as the Moog 6-degree-of-freedom (DOF) motion platform \cite{BermudezRey_2016_Vestibular}, \cite{Hartmann_2014_Direction}, \cite{Karmali_2017_Multivariate}, \cite{Valko_2012_Vestibular}. Recent motion simulator-based motion detection protocols report motion detection thresholds in terms of speed, thus manipulating speed as an experimental condition instead of acceleration. Robotic motion planning is easier and more accurate for speed control than acceleration control. Finally, to create a diverse dataset of vestibular and proprioceptive SD responses, perceptual responses were measured using a 3 x 3 block design testing a randomized combination of angular or linear axis motion, axial direction, and speed. A total of 32 participants, for both experiments, received the same experimental instructions and protocol while using the motion simulator.

\subsection{EXPERIMENTAL DESIGN}
\label{EXPERIMENTAL_DESIGN}
% 1st paragraph
The axis experimental condition had three parameters, cabin movements for rotation were roll (RO), pitch (PI), and yaw (YA), and translation included left/right (LR), forward/backward (FB), and up/down (UD). In addition, minuscule sinusoidal vibrational noise of, 1-2 cm in amplitude and frequency greater than 10Hz was added to the non-stimulated axes to mask the sound of the motor for the selected stimulus. Because vibrational noise was present, the participants were exposed to a more realistic aviation environment. Furthermore, the additional vibration helped reduce movement detection thresholds, such that the task was realistically challenging \cite{Chaudhuri_2013_Wholebody}. The axial direction experimental condition had two parameters: positive or negative direction. Figure \ref{fig1} A depicts both the axis and axial direction conventions for both the rotational and translational experiments; the grey Cartesian coordinate frame represents the simulator cabin. The cabin could move in both rotation (RO, PI, YA) and translation (LR, FB, UD) via the input stimulus and/or participant control. The black outlined squares and circles in Figure~\ref{fig1}A denote positive directional movement (RollP, PitchP, YawP, Right, Forward, Down), where squares and circles correspond to rotational and translation movement respectively. Non-outlined squares and circles indicate negative directional movement (RollN, PitchN, YawN, Left, Backward, Up). Figure~\ref{fig1}B shows the mapping of participants’ joystick movements to the cabin movement.

% -------------------- Figure 1 --------------------
\begin{figure}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure1.eps}
\end{center}
\caption{Axial and directional motion convention for cabin (A) and joystick (B).}
\label{fig1}
\end{figure}
% --------------------

\noindent The speed experimental condition had two parameters; a slow near below-threshold (sub) speed where motion was difficult to detect and a fast above-threshold (sup) speed where motion detection was easier to detect. Some talented participants could detect sub speed movement, therefore this lower limit perceptual stimulation was emphasized to be at ``near below-threshold" instead of at a below-threshold speed. In the motion detection literature, our speed parameters are known as motion detection thresholds measured in terms of Hz, which is a frequency measure of deg/s or cm/s depending on whether the stimulus motion is in rotation or translation respectively. Rotational and translational, sub and sup speed selection was based on reported experimental design thresholds from motion detection literature that accommodated the motion constraints of the simulation system \cite{Hartmann_2014_Direction}, \cite{BermudezRey_2016_Vestibular}, \cite{Karmali_2017_Multivariate}, \cite{Valko_2012_Vestibular}, \cite{Melvill_1978_Vertical}. The Rotational and translation task sub \& sup speeds were 0.5 Hz (deg/s) \& 1.25 Hz (deg/s) and 3.75 Hz (cm/s) \& 15 Hz (cm/s) respectively; implying that acceleration was constant at 0.5 deg/$\textrm{s}^{2}$ \& 1.25 deg/$\textrm{s}^{2}$ and 3.75 cm/$\textrm{s}^{2}$ \& 15 cm/$\textrm{s}^{2}$ respectively.

\indent Figure \ref{fig2}A and \ref{fig2}B show a typical position trajectory when the participant did not respond and when the participant responded during phase A respectively, demonstrating that the experimental phases and trial length were dependent upon the participant’s initial response. A single trial was composed of four different phases, as denoted by the timeline in Figure \ref{fig2}, in which participants were tasked to give feedback to specific visual and vestibular stimuli per phase. During phases a and b, participants could move the simulator using the joystick in any of the rotational or translational axes to counteract the perturbation. Joystick control was in terms of velocity control because it allowed for fast and smooth responses. Timeline A occurred when the participant did not respond in phase A, it consisted of three phases: (a Detection) motion stimulation of the cabin using a smoothed ramp forcing function, (c Reinitialization) cabin reinitialization to the initial orientation or position, (d Rest) cabin and participant at rest. Timeline B occurred when the participant responded in phase a, the four phases consisted of: a Detection, (b Active control) participant active control, c Reinitialization, d Rest. For both timeline A and B, visual and vestibular stimulation was given during each phase. The blue and red lines are position-based trajectories. The blue line denotes automatic robotic movement of the simulator cabin along one axis per trial, and the red line denotes the stimulus plus the participant’s movements to compensate for the perturbation. T1 denotes the maximum allowed stimulation time per trial with respect to each axis and speed, if initial detection was not made within T1s the experimental phases followed as depicted in timeline A. If the joystick was moved within T1s, an initial response was registered and experimental phases occurred as depicted in timeline B.

% --------------------
% Single figure: as in previous version
% -------------------- Figure 2 --------------------
%\begin{figure}[htp]
%\begin{center}
%\includegraphics[width=0.9\linewidth]{figures/figure2.eps}
%\end{center}
%\caption{Experimental event timeline examples. Timeline A occurred when the participant did not respond in phase A, it consisted of three phases: (A Detection) motion stimulation of the cabin using a smoothed ramp forcing function, (C Reinitialization) cabin reinitialization to the initial orientation or position, (D Rest) cabin and participant at rest. Timeline B occurred when the participant responded in phase A, the four phases consisted of: A Detection, (B Active control) participant active control, C Reinitialization, D Rest. For both timeline A and B, visual and vestibular stimulation was given during each phase. The blue and red lines are position-based trajectories. The blue line denotes automatic robotic movement of the simulator cabin along one axis per trial, and the red line denotes the stimulus plus the participant’s movements to compensate for the perturbation. T1 denotes the maximum allowed stimulation time per trial with respect to each axis and speed, if initial detection was not made within T1s the experimental phases followed as depicted in timeline A. If the joystick was moved within T1s, an initial response was registered and experimental phases occurred as depicted in timeline B.}
%\label{fig2}
%\end{figure}
% --------------------

% --------------------
% Two figures stacked to make the figures larger and easier to see
% --------------------
\begin{figure}[htp]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/figure2A.eps}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure2B.eps}
\end{center}
\caption{Experimental event timelines for when participants did not respond during phase a (Timeline A) and when participants did respond (Timeline B).}
\label{fig2}
\end{figure}
% --------------------

\begin{itemize}
\item Phase a detection: A smoothed ramp-forcing function, where the rate of displacement was unknown to the participants, slowly and continuously perturbed one of the three rotational or translational axes of the simulator cabin at a sub or sup rate. The acceleration profile was the second derivative of the position trajectory. Position trajectories are shown by the blue and red lines in Figure \ref{fig2}. During phase a participants were tasked to perform ``initial detection", which consisted of identifying the axis and direction of the felt perturbation and manipulating a joystick replicating actual aircraft controls (Thrustmaster Hotas Warthog joystick), shown in Figure \ref{fig1}B, in the opposite direction of the stimulus. Participants had 15-20s to detect motion depending on the condition, denoted by T1 in Figure \ref{fig2}, which corresponded to the cabin reaching the maximum allowed cabin displacement for a particular axis and direction. T1 was different for every axis and experiment because sub and sup rates were different for each experiment and the physical cabin displacement range was different for each axis. In particular, the rotational experiment had slightly longer stimulation times than the translational experiment because the sub and sup rates were slower and the available cabin displacements in the RO, PI, and YA orientations were larger than the available translational displacement ranges. If the participants did not respond within T1s during phase A, the cabin automatically displaced along one of the three axes as the ramp function increased until it reached T1s, where the ramp function maintained a zero slope causing the cabin to remain stationary for 2s.
\item Phase b active control: If participants responded within T1s during phase A, phase B active control began and they had 15s to maintain the simulator orientation or position stably at the initial location by counteracting the perturbation; phase B was a vestibular dead-reckoning task. No visual stimulation was present; thus, the participants could rely only on vestibular and proprioceptive cues.
\item Phase c reinitialization: A red dot appeared on the screen instructing participants to release the joystick and rest, while the cabin automatically returned to the initial starting location within 10s.
\item Phase d rest: The cabin remained stationary at the starting location for 5s in order to avoid possible over-stimulation or after-effects.
\end{itemize}

\indent In summary, the shortest and longest trials were approximately 32s and 50s respectively. The shortest trial length occurred when the participant immediately responded within 1-2s (2s+15s+10s+5s) or did not respond with T1 equaling 15s ((15s+2s)+10s+5s), the longest trial length occurred when the participant responded just before T1 with T1 equaling 20s (19.9s+15s+10s+5s). Both experiments administered 42 trials: 12 familiarization practice trials and 30 experimental trials. During the familiarization practice phase, unique experimental condition combinations were given, where each of the three axes was stimulated in negative or positive directions at sub or sup speeds. Similarly, the experimental phase consisted of 30 randomized trials, in which 15 trials with unique experimental conditions were repeated twice: five direction-speed conditions (negative sup, negative sub, no-movement, positive sup, positive sub) for each of the three axes (RO/LR, PI/FB, YA/UD). No-movement trials were included as sham trials to encourage the participants to remain active.

\subsection{PARTICIPANTS}
The EuroMov Institutional Review Board (IRB) at the University of Montpellier approved that the scientific objectives and organization of both experiments (IRB-EM rotational: 1703B, IRB-EM translational: 1704B) were safe and appropriate for human participation. The EuroMov IRB committee rules and regulations are in accordance with the 1964 Declaration of Helsinki and its later amendments. Eighteen and 14 healthy volunteers with normal or corrected vision gave informed consent before participating in the rotational and translational tasks respectively (males and females, $32\pm10$ years old); four of the 32 participants reported having novice time-limited piloting experiences lasting less than 40 hours. Four of the 18 rotational and four of the 14 translational participants were over the age of 40 years. The participants who performed the rotational experiment were not the same than those who performed the translational experiment. Therefore, there was no confounds due to experimental ordering, learning, carryover, or fatigue. The same participant population, university students, and staff, was used for both experiments; therefore, it is likely that both experimental populations were similar.

\subsection{EXPERIMENTAL PROTOCOL AND MOTION SIMULATION SYSTEM}
The experiment took approximately 90 min and consisted of four sections (1) arrival, questionnaires, and instruction; (2) familiarization; (3) active control of rotational or translational stimulation; and (4) questionnaire and debriefing. After describing the experimental task and completing the questionnaires, participants were securely installed using the safety harness and communication headphones, as shown in Figure \ref{fig3}A. They were asked to moderately move the joystick in one axis direction at a time while compensating the unknown perturbation. Participants were reminded to maintain the cabin at the initial trial position or orientation, fixed at a steady centered pose, by compensating the motion stimulus. Participants were free to adopt their own strategy to perform the task, both in terms of response speed and of exploration behavior. In order to replicate a realistic flight scenario, the participants were free to move their head and body, looking and/or fixating where they wished, as long as it did not interfere with the task. The fact that the head was left unrestrained is considered undesirable, causing erroneous motion detection due to conflicting self-generated sensory information, and thus rarely performed in traditional motion perception experiments. However we considered it ecologically innovative because it replicated human response under realistic flight circumstances, allowing for a more realistic SD dataset. Once the participant was installed in the cabin, the cabin door was closed and all communication between the participant and experimenter was performed via a camera interface system that facilitated two-way auditory communication. The camera system also provided the experimenter visual feedback of the participant's upper body. The experimenter visually monitored the well-being of the participants, and confirmed participant's feelings of illness auditorily; the experiment ended if the participants reported physical illness.

% -------------------- Figure 3 --------------------
\begin{figure*}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure3.eps}
\end{center}
\caption{Motion simulator apparatus and installation; A and B show the experimental simulator cabin with and without a seated participant respectively. C shows an exterior view of the six-axis iMose motion simulator, consisting of the participant cabin and the robotic arm.}
\label{fig3}
\end{figure*}
% --------------------

\indent The motion simulation system that provided sensory stimulation, iMose, consisted of a 6DOF position-controlled KUKA-based motion simulator system (KR 500-3 MT adapted by BEC GmbH motion simulators, KUKA Roboter GmbH, Germany) and a local area network of three 1independent workstations \cite{Denquin_2021_LAF}, \cite{Landrieu_2017_Timetocollision}, \cite{Bellmann_2011_DLR}. Figures \ref{fig3}B and 3\ref{fig3}C show the interior and exterior of the simulation system, data was transferred between the simulator and workstations at 250 Hz on a private network using UDP. Workstations 1 and 3 were located in the experimenter control room; workstation 1 generated motion for the robot using a MATLAB/Simulink control interface program (MATLAB and Simulink Toolbox Release 2009, The MathWorks, Inc., Natick, Massachusetts, USA). Workstation 2 was fixed to the simulator cabin, and it administered the red dot or black visual screen and recorded the streamed user-controlled joystick signal. Workstation 3, using Labview, served as the experimenter’s user control interface to start and stop the experiment and collect experimental data without causing information delays between the workstations.

\indent Two questionnaires were administered before the experimental phase: a claustrophobia assessment \cite{Radomsky_2001_Claustrophobia}, \cite{Radomsky_2006_Claustrophobia_CLQ} and SSQ \cite{Kennedy_1993_Simulator}, \cite{Bouchard_2007_SimulatorSickness}. All questionnaires were administered in the native fluently spoken language of each participant (French or English). The claustrophobia questionnaire consisted of two sections: the first section measured fear of suffocation (14 questions) and the second section assessed fear of restriction (12 questions). The claustrophobia questionnaire was used as a screening method to assess whether participants could enter the simulator and perform the task relatively stress-free; participants who scored 40 points or lower, indicating that they were not claustrophobic, were initially recruited, and participants scoring higher than 40 were recruited last. For both the rotational and translational experiments all participants scored “non-claustrophobic”, rotational results were mean=10.94, max=38, min=0 and translational results were mean=8.77, max=9, min=0. The SSQ consisted of 16 questions and measured the participant’s general physical state, evaluating nausea, ocular motor, and disorientation sub-scales. The SSQ was administered before and after the experiment to measure the effects of the experiment in terms of disorientation.


\subsection{ANALYSIS}
The goal of the part 1 study was to create a realistic flight dataset of disorientation and non-disorientation occurrence measured by joystick motion. The analysis methodologies for dataset creation were to: 1) verify the correctness and authenticity of the dataset, and 2) statistically correlate physical with perceptual disorientation to confirm whether other possible measures besides joystick could convey markers for the occurrence of human SD-state. Python was used for all analyses, using numpy, pandas, scipy, seaborn, plotly, and matplotlib (Python 3.9, Python Software Foundation, Fredericksburg, Virginia, USA).

\subsubsection{VERIFICATION OF SIMULATION DATASET}
\label{VERIFICATION_OF_SIMULATION_DATASET}
All trials, familiarization and experimental trials, were used in data analysis to maximize data usage. The simulator system motion and participant joystick responses were down-sampled from 250 Hz to 10 Hz for data analyses, such that only relevant human motor movements were considered; literature has shown that human hand and arm movements do not exceed frequencies of 10 Hz \cite{Shadmehr_2004_Computational}.

\indent Data standardization pre-processing analysis was performed to ensure that the data was collected properly. Data standardization consisted of two-steps: 1) numerical confirmation of experimental settings, and 2) numerical confirmation of the experimental design. In the first step, four items were checked for correctness using both joystick and cabin motion data: experimental event matrix per trial, joystick and cabin directional control convention, joystick margin needed to command cabin motion, and start-stop time of phases A and B per trial. In the second step, the motion and timing of the cabin with respect to joystick motion was checked for correctness. The robotic simulator performed motion stimulation in real time using a real-time Linux kernel, with a MATLAB/Simulink input layer, to capture responses with minimal delay. Despite the advantage of rapid response synchrony, real-time systems are prone to having system delays that can influence functional timing and communication between tasks; real-time functioning refers to the order in which numerical tasks are executed using the available computer resources. Therefore, the rotational and translational experiments had trials where system delays imperceptibly influenced the robotic trajectory and/or the participant's ability to respond correctly via the joystick. Due to these slight processing and thus execution errors that are due to the real-time functionality of the motion simulator, it was necessary to remove all trials that had frequency or joystick-cabin related defects such that experimental defects were not confounded with participant response. The following defects were checked in the second step of data standardization:
\begin{itemize}
\item temporal gaps in data,
\item trials where phases A and/or B were shorter than the minimal expected trial length of 17s, denoting the system sampling frequency was faster than desired,
\item trials where joystick motion was sufficient but the cabin insufficiently moved,
\item delays longer than 5s between joystick and cabin movement.
\end{itemize}

\indent The majority of trials that were removed were due to fast or slow system frequency sampling rates, thus discarding trials that had recorded timestamps less or more than 17 s or 50s respectively. The second reason for discarding trials was due to the fact that the cabin did not respond within a few seconds after joystick movement, or the cabin motion axes and direction was incorrect with respect to joystick motion. In total, 40\% of rotational and 50\% of translational trial data was removed from the analysis. Errors were expected as the system was a new experimental test platform where many computers needed to operate in synchrony. Data standardization was the only step that removed trial data, trials that passed data standardization were used in data analysis.

\subsubsection{RESPONSE CATEGORIZATION}
Detection of correct stimuli was categorized into ten possible categories based on the selection of axis and axial direction. Figure \ref{fig4} depicts a flowchart and possible participant choices based on response movements. The blue squares indicate the experimental trial type: the presence of motion stimuli denoted by ``Movement" and no presence of motion stimuli denoted by “Sham". For “Movement" activity, the green squares indicate participant response activity such that “1-3 axes” means that the participant moved the joystick on one or more axes and “No axes" means that the participant did not move the joystick. The yellow diamonds denote the decision process based on the question asked within the diamond. For example, for “Movement" activity where the participant responded using one or more joystick movements, the following question is posed: “Is the stimulus axis the same as the axis in which the participant initially moved the joystick?”. If yes, the axis was noted as correct, and the initial direction was confirmed in a similar manner. For example, “Did the participant initially move in the opposite direction of the stimulus direction?”. The red numbers indicate the total number of possible categories based on the logical progression of performing the task correctly, first finding the correct axis and then finding the correct direction to counteract the vestibular stimulus.

% -------------------- Figure 4 --------------------
\begin{figure}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure4_2.eps}
\end{center}
\caption{Flowchart of selection process for detection performance categories, where correct response categories 1, 2, 4, and 5 denote non-SD occurrence and wrong response categories 3, 6, 7, and 9 denote SD occurrence.}
\label{fig4}
\end{figure}
% --------------------
The ten detection performance categories were reduced to four categories:
\begin{itemize}
\item Initially Correct axis and direction: trials in which the first response was with the correct axis and direction (IC: Category 1)
\item Eventually Correct axis or direction: trials where the first response was with an incorrect axis or direction but the correct axis and direction was found (EC: Category 2, 4, and 5)
\item Never Correct: trials where participants acted on the joystick but never found the correct axis and/or direction (NC: Category 3, 6, and 7)
\item No response: trials in which participants did not respond (NR: Category 9). 
\end{itemize}
Categories 8 and 10 corresponded to the no-movement sham trials and were not used in the analysis.

\subsubsection{MOTION DETECTION AND PERFORMANCE SUMMARY}
\label{MOTION_DETECTION_AND_PERFORMANCE_SUMMARY}
The normalized response count and Reaction Time (RT) per detection performance category were quantified for each axis and speed condition. The normalized response count was the adjusted count per response category, with respect to the given number of trials multiplied by participants; the total trial count per participant was 36, excluding sham trials. The total trial count per participant was adjusted to 36, such that the interpretation of results would be consistent with the experimental design. Participants had fewer total trials than 36 trials because trials that did not follow the experimental design were removed during the data standardization step mentioned in Section \ref{VERIFICATION_OF_SIMULATION_DATASET}. RT was the time that the participant used to find the correct axis and direction. The 95\% confidence interval per axis was calculated to determine which detection performance categories were significant. Detection performance categories above the lower confidence interval were evaluated further. Significant and corresponding detection performance categories were compared for the speed and axis.

\indent The Kolmogorov-Smirnov (KS) test was used to evaluate whether to use a parametric or non-parametric two-sample comparison test for within-axis and across-axis comparisons. All test evaluations resulted in non-parametric distributions; therefore, only non-parametric tests were used. Two non-parametric tests were used to evaluate comparisons: Wilcoxon signed-rank distribution test and Wilcoxon rank-sum distribution test \cite{Foundation_2013_python}. Uneven two-sample non-parametric test data vectors were compared using the Wilcoxon rank-sum test. However, the Wilcoxon signed-rank test required that equal length vectors be compared, thus shorter length vectors were padded with NaN values to preserve the equivalent number of samples with respect to the longer vector and the distribution of the shorter length vector. Statistical p-values are reported using the following standardized significance levels: the Bonferroni required value of 0.0167 for two test comparisons, 0.05 for single test comparisons, and 0.001 for strongly significant one or two test comparisons. A participant detection performance rank score was created to compare overall participant detection performance with perfect performance. The performance rank score was calculated per subject across trials, per experiment, where

\begin{equation}
Rank~score = 2 \cdot (IC~count) + (EC~count)
\label{eqn_rank_score}
\end{equation}

The rank score equation weights were arbitrarily chosen such that the equation formulation was most simplistic; RT was not considered in the rank score because rotational and translational experimental stimulation timings were different and thus non-comparable. IC performance was the desired behavior for the task so a weight of two was given to each IC trial. EC was also desired task behavior because participants were able to eventually find the correct axis and direction, however mistakes were made, thus a weight of one was given to each EC trial. NC and NR performance trials were not the desired task behavior so they were given no credit. Thus a rank score of 72 corresponded with perfect performance, where IC detection was performed for all 36 motion stimuli trials. Finally, the rank score was used to divide participants into three final categories in order to summarize performance with respect to each experiment. Mean and standard deviation of participants' rank score per experiment were calculated, such that participants were divided into best, average, and worst categories if their rank score was greater, within, and lower than one standard deviation from the experimental participant mean respectively.

\subsubsection{PHYSICAL DISORIENTATION}
Detection performance categories were related to only the SSQ disorientation sub-scale, not the combined SSQ score, because the task was related to disorientation with respect to motion detection \cite{Kennedy_1993_Simulator}, \cite{Bouchard_2007_SimulatorSickness}. Physical disorientation was monitored before and after the experiment using the SSQ disorientation sub-scale, such that the difference in before and after measures were attributed to the experienced task; SSQ disorientation difference equaled the disorientation score before the experiment minus the score after the experiment.

Negative SSQ disorientation difference meant that the task made the participant disoriented (e.g., they felt better before), and positive SSQ disorientation difference meant that the task rendered the participant less disoriented (e.g., they felt better after). Physical disorientation for accurate and non-accurate motion detection performers were compared, to quantify whether physical disorientation report could also be a marker for SD, like the perceptual joystick. Again, Wilcoxon signed-rank or rank-sum non-parametric distribution tests were used to evaluate comparisons, as the KS test only found non-parametric distributions. The mentioned statistical p-value reporting convention was used.


\subsection{RESULTS}
For both rotational and translational experiments, participants’ detection behavior was quantified using the detection performance categories in terms of count and initial RT with respect to stimulation speed and axis; no significant differences were found between positive and negative axial directions thus directional differences were not considered. As previously mentioned, human motion detection ability of self-motion along axis and axial directions are often reported in terms of motion detection threshold \cite{Valko_2012_Vestibular}, \cite{Hartmann_2014_Direction}, \cite{Karmali_2017_Multivariate}. A motion detection threshold is registered from a self-report that motion was felt along a specific axis and direction for a specific motion stimulus frequency. Results are typically displayed in terms of mean detection count across or per subject for many stimulus motion frequencies, where count results are grouped by successful and unsuccessful detection. We performed the same analysis presentation for our two sub and sup stimulus frequencies, displaying results in terms of count, mean count, and RT across participants, where count and RT results are grouped by detection performance categories.

\subsubsection{MOTION DETECTION PERFORMANCE}
Figure \ref{fig5} shows the normalized summed count (top row), normalized mean count (middle row), and mean RT (bottom row) per detection performance category, across participants for RO, PI, YA, LR, FB, and UD axes and sub \& sup speed conditions, for both rotation and translation. The top row shows the normalized summed count per detection performance category for each axis and speed condition. For rotation, the summed bars in the top row are equal to 648, which corresponds to the 18 participants multiplied by 36 trials. The top row represents the distribution of total trial responses per response category. Similarly, for translation, the summed bars in the top row are equal to 504 which corresponds to the 14 participants multiplied by 36 trials. The middle row shows the mean count of the same normalized count data across participants. The mean count represents the frequency of selecting a response category across participants. It was necessary to show both mean and total selection count because average axis selection count can not be clearly understood from the total axis selection count; total count gives information about overall participant response and average count gives information about participant tendencies. The bottom row displays the mean RT taken to detect correctly, thus only IC and EC response categories are shown. Bars without error bars indicate a single sample value, or several participants had the same count value. Single-sample bar values may exist due to data elimination during the rigorous standardization process. Wilcoxon signed-rank test and rank-sum tests were used to determine significance such that significant and slightly significant relationships were represented by (* within axis comparison of sub and sup, ** across axes comparison). Bonferroni correction: p < 0.0167 was used as the significance threshold. Detection performance categories above the lower confidence interval, denoted by the solid red line, were considered for statistical comparison across subjects for categories within (e.g.; sub vs. sup) and across axis (e.g.; RO sub vs. PI sub) conditions.

% -------------------- Figure 5 --------------------
\begin{figure*}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure5.eps}
\end{center}
\caption{Normalized summed count (top row), normalized mean count (middle row), and mean RT in seconds (bottom row) per detection performance category, axis, and speed for rotational and translational stimulation.}
\label{fig5}
\end{figure*}
% --------------------

\noindent \emph{DETECTION: SPEED COMPARISON (WITHIN AXIS CONDITION)}\\
There was a slightly significant sub versus sup count difference for the most counted detection performance category for the RO and PI axes, where sup speed resulted in a higher count than sub speed (RO count EC sup vs sub: KS: non-normal distribution, signed-rank: p < 0.001, rank-sum: p < 0.026, n=13; PI count IC sup vs sub: KS: non-normal distribution, signed-rank: p < 0.08, n=18). Slight significance is denoted in the top row of Figure 5, with a single star in purple and blue. There was a similar trend for the YA axis, where the most counted detection performance category, IC, had a higher sup count than sub count. This demonstrates that participants were more accurate at sup speed than at sub speed, regardless of the motion stimulation axis or direction. No significant differences between sub and sup speeds were found in the translational experiment. However, there was a trend for all axes where the most counted detection performance category had higher sup counts in comparison to sub counts. For example, the EC detection performance category for the LR and FB axes and the IC detection performance category for the UD axis had greater sup counts than sub counts. Translational motion sub and sup speed differences were less apparent than in rotational motion due to inner-ear stimulation differences. Reduced speed detection in translational motion are likely attributed to less semi-circular stimulation and delayed otholic signaling in comparison to rotational motion \cite{Angelaki_2008_Vestibular}. Therefore, we suspect that more data was needed for differences to become statistically significant.

\indent Regarding RT differences for the rotational experiment, some detection performance categories had significantly lower RTs for the sup than the sub speed condition. For RO and PI axes, the RT for the most counted detection performance category was significantly lower for sup speed in comparison to sub speed (RO RT EC sup vs sub: KS: non-normal distribution, signed-rank: p < 0.001, rank-sum: p < 0.001, n=66; PI RT IC sup vs sub: KS: non-normal distribution, signed-rank: p < 0.001, rank-sum: p < 0.001, n=65); significance is denoted in the bottom row of Figure 5 with a single star in purple and blue. In summary, we demonstrate that faster sup motion caused more accurate and faster motion detection than slower sub motion. This result has been reported in motion detection literature, thus confirming that the experiments were performed correctly and that the dataset accurately represented human response \cite{Valko_2012_Vestibular}, \cite{Hartmann_2014_Direction}.

\noindent \emph{DETECTION: AXES COMPARISON (ACROSS AXES PER SPEED CONDITION)}\\
The same speed condition and successful response categories denoted by IC and EC were compared across axes, in order to identify task difficulty with respect to the axis, and thus demonstrate that our experimental results were in alignment with psychophysical motion detection findings. Whole-body motion detection literature shows that RO and PI are easier to detect than YA and translational motion. According to a report RO and PI detection thresholds were statistically similar for novices and experts \cite{Hartmann_2014_Direction}. Additionally, RO was reported to be easier to detect than LR, UD, and YA in both non-vestibular and vestibular dysfunction participants \cite{Valko_2012_Vestibular}. Table \ref{table1} depicts significant differences in response count and mean RT for specific speed and axis categories. Category 1 and 2 represent categories with high count or fast RT and categories with low count or slow RT respectively; the first and second p-values correspond to the signed-rank and rank-sum test respectively. Listing significant differences allowed us to rank axis conditions, with respect to motion detection ease and difficulty, and then compare the ranked list with literature reports to confirm correctness of experimental stimuli. Table \ref{table1} shows that, in alignment with literature reports, we similarly found that RO, PI, and FB axial motions were easier to detect than YA, with dependence on speed when considering only correct responses. In particular, successful response category counts for both RO and PI at sup speed were significantly higher than those for YA, and for sub speed FB and PI had significantly higher counts than YA.

\begin{table}[h]
\caption{COUNT AND RT COMPARISONS FOR COMBINED IC \& EC RESPONSE.}
\label{table1}
\centering
\begin{tabular}{C{15pt}C{40pt}C{40pt}C{90pt}}
\hline
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Speed \& axis} & Significance\\
\hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Category 1} & \multicolumn{1}{c}{Category 2} & \multicolumn{1}{C{90pt}}{(KS: non-normal, signed-rank, rank-sum)}\\
\hline
Counts & sup RO & sup YA & p < 0.001, 0.0167, n=20\\
 & sup PI & sup YA & p < 0.001, 0.0167, n=20\\
 & sub FB & sub YA & p < 0.001, 0.0167, n=22\\
 & sub PI & sub YA & p < 0.001, 0.001, n=22\\
\hline
Mean RT & sup RO & sup YA & p < 0.001, 0.001, n=67\\
 & sub PI & sub YA & p < 0.001, 0.001, n=67\\
 & sub FB & sub YA & p < 0.001, 0.001, n=41\\
\hline
\end{tabular}
\end{table}

\indent Moreover, our results showed functional differences between the RO, LR, \& FB and PI, YA, \& UD tasks. During PI, participants mostly detected correctly (IC) and rarely when they did not detect correctly, they eventually or never corrected. Again in YA, participants often detected correctly (IC), but when they did not detect correctly they did not feel any motion (NR). Similarly in UD participants often detected correctly (IC), and when they did not detect correctly they often eventually corrected; IC was more prevalent when speed was fast. Whereas in RO, LR, and FB, participants could not initially detect the correct axis and/or direction, but they could eventually find the correct axis after several mistakes.

\indent Lastly, task difficulty for within rotational and translational stimulation appeared to be correlated with longer RT. As mentioned in Section \ref{EXPERIMENTAL_DESIGN}, participants were stimulated slower in the rotational task than in the translational task; thus, RT was different for the rotational and translation tasks and were not compared. The second portion in Table \ref{table1} labeled RT, shows the significant within experiment comparison across axes for only correct responses. For the rotational task at sup speed, RO and PI had faster RT than YA indicating that participants needed less time to detect motion for RO and PI. Similarly, for the translational task at sup speed, FB had significantly faster RT than UD.

\subsection{MOTION DETECTION PERFORMANCE RANK}
Including both rotational and translational tasks, the highest rank score was 55 and the lowest score was 11. On average, participants received a rank score of 37. Therefore, the best performer, regardless of rotation or translation, achieved $(55/72) \cdot 100=76.3\%$ accuracy for the task. The average performer was only able to achieve $(37/72) \cdot 100=51.38\%$ accuracy for the task. The same task accuracy statistic was calculated for sub and sup conditions individually, for both rotation and translation experiments, and similar results were found, as shown in Table \ref{table2}. Table \ref{table2} shows the experimental performance accuracy per speed condition using the performance rank measure. All percentages were calculated by dividing by 36 trials.

\begin{table}[h!]
\caption{DETECTION PERFORMANCE RANK PER SPEED CONDITION.}
\label{table2}
\centering
\begin{tabular}{C{50pt}|C{30pt}C{40pt}C{25pt}C{40pt}}
\hline
Rank & Rot sub & Trans sub & Rot sup & Trans sup \\
\hline
Best & 83.3\% & 75\% & 80.5\% & 77.7\% \\
Average & 47.2\% & 55.5\% & 55.5\% & 58.3\% \\
Worst & 11.1\% & 0\% & 30.5\% & 25\% \\
\hline
\end{tabular}
\end{table}

These rank statistics show that the detection task was challenging for the average person, regardless of the experimental conditions, but it was not impossible to perform with reasonable success. The participant distribution count for the rotational experiment was five best performers, 11 average performers, and two worst performers. Similarly, the participant distribution count for the translational task was as follows: two best performers, 11 average performers, and one worst performer. The rotational and translational participant distribution counts for best, average, and worst performance were similar, showing that both tasks were similarly challenging in terms of motion detection. Therefore, translational detection may not be more difficult than rotational detection in realistic environments.

\subsubsection{MOTION DETECTION PERFORMANCE AND PHYSICAL DISORIENTATION}
Twenty of the 31 participants did not feel any difference in terms of physical disorientation during the entire experiment. Considering the performance rank score mentioned in Section \ref{MOTION_DETECTION_AND_PERFORMANCE_SUMMARY}, approximately 1/3 of the average detectors, 1/3 of the best detectors, and 2/3 of the worst detectors experienced physical disorientation. The 2/3 worst detection ratio is reported for completeness; however this measure is disregarded because it is based on only three participants. Thus, 1/3 of the population felt physical disorientation regardless of performance.

\indent To investigate whether there was a relationship between physical disorientation and detection performance, the detection performance of the 12 participants who reported physical disorientation was evaluated; see Table \ref{table3} for a percentage of their summed trial performance per category per SSQ difference report. For instance, a participant who reported a before and after SSQ score of six and four respectively would have their trial performance category counts, of eight EC and six IC trials, associated with an SSQ disorientation difference score of negative two. Table \ref{table3} shows the motion detection response category per reported SSQ disorientation sub-scale difference for both the rotational and translational experiments. Performance category percentage values across SSQ scores sum to 100\%. Negative and positive SSQ values denote that the participant felt better before and after the task respectively. The bold percentages corresponding to negative SSQ values for categories EC and NC highlight that more negative physical disorientation was present in unsuccessful initial attempts to detect motion. Table \ref{table3} demonstrates that more negative physical disorientation was observed for unsuccessful initial detection response categories EC and NC, than for IC successful initial detection response or no response. The negative and positive SSQ disorientation differences per response category were summed respectively, to evaluate significance between IC negative and NC or EC negative. Physically disoriented best performers (IC) did not report significantly less physical disorientation than poor performers.

\begin{table}[h!]
\caption{SSQ DISORIENTATION SUB-SCALE PER MOTION DETECTION PERFORMANCE CATEGORY}
\label{table3}
\centering
\begin{tabular}{c|ccccccc}
\hline
& \multicolumn{7}{c}{\centering SSQ (\%)}\\
Category & -5 & -3 & -2 & -1 & 1 & 2 & 4\\
\hline
IC (1) & 5.4 & 8.4 & 17.8 & 23.6 & 24.5 & 8.9 & 11.5\\
EC (2,4,5) & 10.1 & 12.0 & 29.0 & 26.6 & 14.1 & 2.8 & 5.4\\
NC (3,6,7) & 10.3 & 2.7 & 38.5 & 26.3 & 5.2 & 11.5 & 5.5\\
NR (9) & 0 & 3.4 & 20.8 & 18.7 & 26.0 & 21.8 & 9.4\\
\hline
\end{tabular}
\end{table}

\indent In summary, no significant relationship between physical disorientation and motion detection was found. There was only a trend that EC and NC performers, who felt physical disorientation, felt better before the task than after. This implies that participants became fatigued while trying to perform the task, when detection was not easy for them. For IC performers who experienced physical disorientation, there was no trend in terms of feeling better before or after. Implying that participants who could detect easily, felt discomfort for other reasons not related to the experiment. There was a slight trend for NR performers that felt physical disorientation, such that they felt better after the task than before. Showing that participants who did not respond, became comfortable and relaxed in the dark experimental setting.

% ----------------------------------------

\section{PART II: SD PREDICTION USING ML, DL, AND CLUSTERING METHODS}
The goal of the Part II study was to identify the best numerical methods to predict SD, using the SD occurrence dataset that was experimentally created in Part I.

% --------------------------------------

\subsection{DATASET DESCRIPTION}
The rotation and translation SD datasets contained 19 columns, the columns contained time-series data per trial where scalar values per trial were repeated for each corresponding trial. Table \ref{table4} shows column attributes with respect to column number; highlighted attributes were used for SD prediction.
\begin{table}[h!]
\caption{ROTATION AND TRANSLATION SD DATASET ORGANIZATION}
\label{table4}
\centering
\begin{tabular}{l|c}
\hline
Column attribute & Column number\\
\hline
\textbf{Subject} & 0\\
\textbf{Trial} & 1\\
\textbf{Speed, Axis stimulus} & 2, 3\\
Data point count, \textbf{Time} & 4, 5\\
\textbf{Response Type (10 detection performance categories)} & 6\\
Commanded cabin position (RO/LR, PI/FB, YA/UD) & 7,8,9\\
Actual cabin position (RO/LR, PI/FB, YA/UD) & 10,11,12\\
\textbf{Joystick} (RO/LR, PI/FB, YA/UD) & 13,14,15\\
Vibrational noise (RO/LR, PI/FB, YA/UD) & 16,17,18\\
\hline
\end{tabular}
\end{table}
The subject, trial, speed, axis, time, response type, and joystick columns were pre-processed using the data pre-processing pipeline. Three ground-truth labels and feature-types were created; feature-types included time, frequency, and time \& frequency features and human movement science inspired features.

\subsection{ANALYSIS}
Three types of analysis were performed: evaluation of model architecture and feature usage, ground-truth label comparisons, and unsupervised label comparisons with respect to ground-truth labels. Model architecture and feature usage evaluations investigated which of the seven ML and DL model architectures had better prediction accuracy and/or ROC-AUC for different data use-cases, feature-typen and feature quantity using permutation importance \& human movement science driven selections. After identifying the best performing model architecture, data length usage was investigated such that the ideal data quantity that best predicted SD was found. In the second analysis, model architecture mean accuracy was evaluated across all feature conditions per ground-truth label; the ground-truth labels were ranked in order from highest to lowest mean accuracy for each model architecture. For the final and third analysis, unsupervised labels were created using K-means, Gaussian Mixture Model (GMM), and K-medoids clustering methods; six different feature matrix combinations were compared per method. The goal of creating feature matrix combinations was to ensure that the features were unique representations of joystick behavior. Feature matrix combinations comprised of: position, position/velocity, position/velocity/acceleration, position/two position Discrete Wavelet Transforms (DWTs), position/6 position DWTs, Principle Component Analysis (PCA) components of position/velocity/acceleration. 

\subsubsection{DATA PRE-PROCESSING: PIPELINE}
The data pre-processing pipeline prepared features and labels per data use-case such that they could be used for SD supervised and unsupervised classification analyses. There was uncertain about how to quantitatively define SD, therefore it was of interest to investigate several plausible quantitative definitions of SD, called SD ground-truth labels. Three ground-truth labels identifying SD occurrence were created, based on the identified performance categories IC, EC, NC, and NR in Part I. The three SD ground-truth labels were:
\begin{itemize}
\item Lenient: a binary label denoting SD for NC and NR performance categories and non-SD for IC and EC categories, implying that small occasional mistakes did not signify SD whereas successive errors signified SD, 
\item Strict: a binary label denoting SD for EC, NC, and NR performance categories and non-SD for the IC category, implying that small occasional mistakes and successive errors are likely to be SD and only non-SD occurred when performance was perfect,
\item Complex: a multi-category label depicting SD via NC and NR responses, mild-SD using EC responses, and non-SD using only IC responses.
\end{itemize}
The purpose of testing different labels was to understand how to best define SD from the intrinsic organization of the data; better predicting models using a certain label implies that the data is best structured for that label. We compare our data-driven definition of SD with the current functional definition of SD \cite{Newman_2007_SD}. SD identification label effectiveness was evaluated using model mean accuracy, where high mean accuracy signified that the label described the data well. Ground-truth label identification demonstrated that a numerically derived definition of SD was possible. Each ground-truth label was ranked from most to least appropriate based on highest to lowest model mean accuracy respectively.

Next, joystick signals were baseline shifted to zero and constant zero joystick response trials were removed. Feature-type selection was motivated by two factors: 
\begin{itemize}
\item investigation of time and/or frequency signal influences on model architecture,
\item exploitation of human movement science domain knowledge that humans regulate velocity and acceleration to perform position-based motions
\end{itemize}
HAR pose estimation typically uses position or image features, and HAR IMU accelerometer features are acceleration-only representations\cite{}. 


However, the human movement science domain has proven that the brain requires derivative information in order for the body to generate smooth position trajectories\cite{}. Thus, implying that position, velocity, and acceleration motion trajectories convey unique and important temporal information. 

It was of interest to understand whether the additional derivative information would improve human activity predictions. Thus, first and second derivatives of the joystick signal were calculated; a low pass third order filter with a cutoff frequency of 10Hz was applied to the second derivative. 

The position, velocity, and acceleration joystick time-series features are used in combination as opposed to position-only features, that are typically used in HAR. 

In section \ref{PART2_RESULTS_MODEL_ARCHITECTURE_USECASE_FEATURE_TYPES}, we compare model performance using only position trajectories in comparison to position, velocity, and acceleration trajectories, to determine whether the additional derivative information would result in better model performance.

Regarding the second motivating factor for feature selection and as mentioned in subsection \ref{ARTIFICIAL_INTELLIGENCE_METHODS_FOR_HAR}, it was of interest to compare the performance of certain model architectures using certain types of feature data (time, frequency, time \& frequency). Specific model architectures are designed to process certain types of feature data, such as sequential models are designed to identify trends in time-based features and CNN-based model are designed to identify spatial features. However, recent literature has shown that different model architectures can be combined with different types of feature data with similar or better performance \cite{}. In section \ref{PART2_RESULTS_MODEL_ARCHITECTURE_USECASE_FEATURE_TYPES}, we investigate all possible combinations of model architecture with respect to time, frequency, and time-frequency feature-types, and list the best to worst feature-types for each model architecture based on model performance. In addition, raw time-series time-based features are typically used in HAR literature \cite{}. Several studies have explored the usage of frequency-based and time-frequency features \cite{Nedorubova_2021_CWT_CNN_HumanActivity}. However there are few studies that compare the benefits or disadvantages of certain model architecture with respect to time, frequency, and time-frequency feature-type selection. In total, 9 and 27 time and frequency features were calculated for position-only and position, velocity, and acceleration constructions, respectively. The four feature-types were :
\begin{itemize}
\item time-only: joystick time-series signals in sequential order,
\item frequency-only: five frequency pattern sublevels of the DWT using the symlets 5 mother wavelet\cite{Nedorubova_2021_CWT_CNN_HumanActivity},
\item time \& frequency: flattened 2D spectrogram, formally called the short-time Fast Fourier transform (FFT),
\item time \& frequency: flattened 2D continuous wavelet transform (CWT) using the Mexican hat mother wavelet as reported in \cite{Nedorubova_2021_CWT_CNN_HumanActivity},
\end{itemize}

%\noindent \emph{LINEAR INTERPOLATION AND SCALING}\\
All features and labels were linearly interpolated or downsampled such that all trials were 400 data points, equivalent to 40s long. As previously mentioned in section \ref{EXPERIMENTAL_DESIGN}, data trial length was different for each trial because participants initially responded when they perceived motion; shortest and longest trials were approximately 32s and 50s respectively. Forty seconds was chosen as an appropriate length because the average trial length was approximately 45s and total trial length needed to be divisible by multiples of 10, 20, 40, 100, 200, 400 for the data length evaluation analysis. Finally, the features were scaled using standardization such that values were scaled appropriately \cite{Burkov_2019_ML}. Once the feature matrix and labels were created for each data use-case, class balance was calculated for each of the three ground-truth labels. Using random non-repeating selection classes with less label data were padded with existing class data, such each class had an equivalent representation of data samples.

% --------------------------------------

\subsubsection{MODEL ARCHITECTURE TYPES}
Eight unique types of ML \& DL models were compared using time-series features. These model architectures were of interest because they each organize the data in a distinctive manner such that temporal, or spatial, or feature space properties are exploited.\\

\noindent \emph{Support Vector Machine}\\
SVM is a ML method that distinguishes two or more classes by finding a bisecting line in feature space that separates the classes maximally. The feature vectors $x_i$ and class labels $y$ are known, such that the slope of the separable line $w$ and y-intercept $b$ are calculated using an iterative approach, where a cost function construction of $w$ is minimized\cite{Burkov_2019_ML, Anguita_2012_SVMHumanActivity}. The construction of SVM is useful for distinguishing classes for sequential features because all of the data is evaluated in feature space. Time-series data are likely to have connected areas and thus boundaries can be found around these areas to distinguish certain temporal patterns from other temporal patterns. Despite numerous confirmations in literature that DL methods outperform SVM using time-series features, SVM was selected as a comparison method to record/survey its performance with respect to DL models for joystick data \cite{Xiao_2003_DeepLearning}. In addition, it was of interest to investigate whether SVM could better predict frequency-only or time \& frequency features in comparison to time-series features; it is unclear whether SVM can predict similarly well as DL models for low frequency joystick data using frequency and/or time-frequency transformed features. The scikit-learn SVC model was used, where parameters C and gamma were automatically hyperparmeter tuned in an adaptive manner using accuracy, across batches of data points restricted to length of 70000; use-case data was assumed to be homogeneous. For example, C was initialized to the default value of 1 and gamma was initialized to a decimal value, the ratio of the number training features. If the current batch accuracy was lower than the previous batch accuracy, C and gamma would be increased and decreased by an incremental decimal value respectively; the incremental decimal value was 10 percent of the initialized values. Batch models were tested on randomly selected portions of test data, and the best predicting SVM batch model was selected to represent the data use-case. All data use-cases used the batch model hyperparameter tuning, however the following four data use-cases used default parameters: trans$\_$ax2$\_$all, trans$\_$ax1$\_$all, rot$\_$ax2$\_$all, trans$\_$all$\_$sup. Due to the fact that the majority of models were adaptively trained, the accuracy and ROC-AUC values are average to best prediction representations of SVM. Restriction of feature space to 70000 points was not only motivated by hyperparameter tuning, but used such that SVC could reliably compute the result without excess usage of RAM.\\


\noindent \emph{Long Short-Term Memory}\\
LSTM is a RNN DL method that uses an adjustable number of data points, called timesteps, across feature samples to make an output estimate $y$. In comparison to other RNN models, the LSTM is able to learn long-term temporal dependencies while avoiding the vanishing gradient problem, which often occurs using an RNN structure. An LSTM model consists of nodes or cells, where the number of cells are the number of timesteps, that are sequentially connected from left to right. The first cell to the left takes in the first timestep of features (X[batch,timesteps=0,features]) and outputs three values: the cell memory state $c$ which is a matrix containing the forward propagation values, the hidden activation state $a$ which is the cell memory state transformed by the tanh activation function multiplied by a constant, and the output estimate $y$ per timestep or batch. During the computation of each batch, the LSTM learns temporal information from timestep-to-timestep by passing $a$ and $c$ to the next LSTM cell on the right as an initialization; thus a prediction per batch depends upon all of the timesteps and predictions per timestep only depend upon previous timesteps \cite{Sarang_2021_Tensorflow2, Ng_2021_Deep_learning_specialization}. Similar to SVM, LSTM is able to predict sequential data well because it uses the learned associations of past data via the hidden activation and cell memory state matrices to make predictions per timestep. One Tensorflow LSTM model was used where the return state and return sequences were set to False. The hidden state size called $n\_a$ was tuned in advance and 40 was found to produce the highest prediction accuracy results across data use-cases, for rotational and translational data.\\


\noindent \emph{Neural Network, Multi-layer Perceptron}\\
A Neural Network (NN), also known as a MLP, estimates a probablistic output $\hat{y}$ by solving a linear/nonlinear optimization problem in a layered/nested manner such that at each layer, parameters called weights $W$ and biases $b$ are estimated from known individual inputs $x$ and outputs $y$. A NN with more than two non-output layers is referred to as a DL model. Specifically, known input features $X$ are passed to r nodes that each solve the optimization problem in three steps:
\begin{itemize}
\item Forward propagation: $Z = WX + b$, $A = f(Z)$ where $W$ and $b$ are initialized with strategically selected random values. The activation function $f$ truncates and bounds $Z$ matrix values to a desired range, such that cost function $J(w,b)$ is likely to decrease and the estimated output $\hat{y}$ returns appropriate binary or probablistic values. Relu and tanh are common activation functions used between non-output layers, and sigmoid and softmax are typical function used before the output layer.
\item Backward propagation: using the cost function $J(w,b)$ such that, four partial derivatives are computed $\frac{\partial J}{\partial A}, \frac{\partial J}{\partial Z}, \frac{\partial J}{\partial W}, \frac{\partial J}{\partial b}$,
\item Update parameters or compute $\hat{y}$: $W = W - \alpha\frac{\partial J}{\partial W}, b = b - \alpha\frac{\partial J}{\partial b}$ where $\alpha$ is known as the learning rate. The probablistic estimated output $\hat{y}$ is computed by transforming Z using a sigmoid or softmax function for desired binary or multi-class classes.
\end{itemize}
The number of layers and nodes per layer are chosen in a systematic search manner, called hyperparameter search, such that the output $\hat{y}$ is closest to $y$ \cite{Ng_2021_Deep_learning_specialization}. Concerning the performed analysis, two to eight layer MLP models with randomly selected nodes per layer were tested, such that the best performing MLP model was used for each data use case. Nodes per layer were selected as a function of the number of prediction classes, such that nodes decreased, increased, decreased then increased, or increased then decreased across layers. In general, 2 to 3 layer models with a decreasing node size per layer predicted best; Tensorflow Dense layers were employed for NN models.\\


\noindent \emph{Convolutional Neural Networks}\\
CNN is a DL feedforward neural network method that uses 2D spacial input information to estimate a probablistic output $\hat{y}$ using the NN framework of forward and backward propagation. Stacked 2D matrix or stacked 3D image inputs are manipulated value-by-value or pixel-by-pixel using convolution, pooling, and dropout to maximize spatial patterns before or after NN optimisation is performed \cite{Burkov_2019_ML, Ng_2021_Deep_learning_specialization}. There are known configurations of successive convolution, pooling, and dropout, called architectures. Well-known shorter architectures include Max Pooling CNN (MPCNN), Encoder-Decoder, whereas longer deep architectures include ResNet, Xception, and Inception. MPCNN architecture blocks combine repeating groups of 2D convolution with NN optimisation using relu and Max pooling. Whereas Encoder-Decoder architecture blocks combine repeating groups of 2D convolution with NN optimisation using leakyrelu and Dropout \cite{}. In the analysis, we tune the CNN representative model by performing both MPCNN and encoder-decoder architectures, and selecting the architecture with the highest accuracy for each data use-case and label. Feature signals were stacked on top of each other, and then reshaped into a 3D image of size 64 x 64. The encoder-decoder architecture consisted of: 64 filters 5x5 Conv2D with stride (2,2), LeakyReLU, Dropout 0.3, 128 filters 5x5 Conv2D with stride (2,2), Flatten, Dense layer output dimension with softmax or sigmoid activation for multi-class or binary prediction respectively. The following MPCNN architecture was used: 32 filters 5x5 Conv2D with stride (1,1), ReLU, MaxPooling2D pool size (2,2) with stride (2,2), 64 filters 5x5 Conv2D with stride (1,1), ReLU, MaxPooling2D pool size (2,2) with stride (2,2), Flatten, Dense layer 1000 with relu, Dense layer output dimension with softmax or sigmoid activation for multi-class or binary prediction respectively \cite{Nedorubova_2021_CWT_CNN_HumanActivity}. The same padding option was used for all models.\\


\noindent \emph{LSTM-CNN}\\
LSTM-CNN is a hybrid DL method that first performs LSTM to reduce inputs to binary patterns where similar sequential behavior was grouped. The reduced signal was then reshaped into a 3D image of size 64 x 64, such that CNN was used to find spatial trends in the binary behavioral image. It was hypothesized that spatial binary groups of SD and non-SD, like a QR code, maybe more efficient as inputs than the raw time-series or transformed time-series features. The LSTM-CNN was regulated in the same manner as the individual LSTM and CNN Tensorflow models. One Tensorflow LSTM model was used where return state and return sequences equaled False, and $n_a$ equaled 40. The same CNN model structures that were described for the individual CNN model were employed using the LSTM output.\\


\noindent \emph{Transformer Encoder}\\
The Transformer Encoder model consists of three main steps: positional encoding, multi-head attention, feedforward NN.
In the positional encoding step the feature data is transformed such that the features additionally contain positional information about each data point, via a summation. A unique positional encoding matrix is created from unique data point values that are mapped to shifted sinusoids. Next, Multi-Head Attention compares the positionally enhanced data points from each feature, called query denoted by $q$, with the other similarly enhanced feature data points, called keys denoted by $k$; data point values are called values denoted by $v$. The context similarity of data points are computed via Self-Attention. Attention learns the context of data points, meaning the similarity of certain positional data points with respect to others. The feedforward NN then learns how the context corresponds to the output $y$, thus allowing for reliable estimations for $y$ called $\hat{y}$. Multi-head attention and feedforward are typically repeated six times in a row, such that different combinations of query, keys, and values are selected, thus allowing for learning of different context representations with respect to the output $y$\cite{Ng_2021_Deep_learning_specialization, Sarang_2021_Tensorflow2}. 

Regarding the Transformer Encoder modeling analysis, the maximum number of unique encoded positions was set to the number of timesteps, the encoding/embedding dimension size was the number of features; input vocabulary size or unique data points was set to 10. The Standard Transformer Encoder modeling architecture was employed; Tensorflow functions Embedding, MultiHeadAttention, FullyConnected, LayerNormalization, and Dropout were used. The Transformer Encoder architecture was of interest as a spacial comparison method with the CNN architecture. In addition, due to the fact that the transformer architecture compares information in parallel, thus accounting for both data location and amplitude differences, it was of interest to evaluate which feature-type (time, frequency, or time-frequency) best accommodated the model structure for SD classification.\\


\noindent \emph{Subspace: Random Forest}\\
RF is an ensemble bagging method that combines the result of many weaker decision tree models into a single framework, by a process called voting where the mode prediction class is found per sample across all decision tree models. Briefly, the decision tree method systematically divides the feature space into two subspaces every decision criteria evaluation, such that the values in each subspace become more homogeneous. A rooted tree diagram with two subbranches at every decision criteria evaluation is used to represent the process of dividing the subspace into smaller subspaces, where a tree node and branch denote a subspace and subspace division respectively. Every decision criteria evaluation increases the depth of the tree by one. Successive divisions are made using an optimization-based decision criteria, methods include impurity or entropy, until at least one of several stopping criteria are satisfied. The goal of the decision criterion are to split the subspace at a location/boundary where the difference between certain neighboring points are the largest. The stopping criteria typically consists of the three situations:
\begin{itemize}
\item the subspace has less than a minimum number of data points (N\_min, min\_samples\_split), or the right and left subspaces have less than a minimum number of data points (min\_samples\_leaf),
\item the maximum depth has been achieved (max\_depth), 
\item the total decrease in impurity is less than a fixed threshold $\beta$, meaning the data points in the subspace have similar/homogeneous values.
\end{itemize}
RF is very effective at estimating $y$, called $\hat{y}$, if the weaker decision tree models are not correlated. Correlated models are undesired because they identically predict the same result, thus biasing the voting results with these strong predictors. Correlated models can be prevented by tuning the number of trees and correctly applying the decision criteria evaluation \cite{Burkov_2019_ML, Louppe_2012_RF}. Regarding the modeling analysis, default scikit-learn parameter selection was used for the RF model. Despite the fact that RF models are rarely evaluated in recent HAR literature, because they were proven to be less performant, it was of interest to evaluate their predictive ability with respect to feature-type (time, frequency, or time-frequency). Additionally, it was of interest to compare a reliable ML method, like RF, with recent DL methods for bench-marking.\\


\noindent \emph{Unsupervised Clustering}\\
K-means, K-medoids, and GMM unsupervised clustering methods were used to investigate SD. K-means is a recursive method that groups feature data $X$ into a specified number of $k$ clusters. $k$ cluster centroids are randomly initialized within the range of feature data, whereupon the Euclidean distance from each example in $X$ to each centroid is computed. Examples denotes a row of $X$, where the dimension of $X$ is timesteps by features. Each example in $X$ is assigned to the closest distance centroid, and finally the average feature vector of each centroid is calculated and becomes the new centroid. The process of computing the distance from examples to centroids and centroid re-computation is recursively repeated until the centroids remain constant. The K-medoids algorithm, also known as Partitioning Around Medoids (PAM), is identical to K-means with the exception of centroid selection and distance function. Selected centroids must be actual data points of $X$, and the sum of pairwise dissimilarities must be minimized. Minimizing pairwise dissimilarities indicates that swapping non-cluster points with cluster points must result in a lower cost score. The construction of K-medoids is beneficial for two main reasons: better characterization of feature data than K-means because centroid selections are actual data values, improved predictions for outliers due to the swapping method instead of Euclidean distance \cite{Kaufman_1990_PAM}. Unlike hard clustering methods, like K-means and K-medoids where each example is assigned to a unique cluster, GMM is a soft method that gives each example a membership score quantifying a probabilistic associated with each cluster. GMM initialization is similar to K-means, the steps for GMM include:
\begin{enumerate}
\item initialize mean/centroid $\mu_{j}$ and variance $\sigma^{2}_{j}$ of clusters randomly and hard assign each example to a cluster $j$ using the euclidean distance, such that an initial label is created.  
\item calculate the probability density function (pdf) per example  ($f(x|\mu_j, \sigma^{2}_j)$) denoting the probability of each example belonging to each cluster $j$.
\item compute the likelihood $b$ of each example per cluster, $b_{i}^{j} = \frac{f(x_{i}|\mu_{j}, \sigma^{2}_{j})\phi_{j}}{\sum_{c=0}^{k}f(x_{i}|\mu_{c}, \sigma^{2}_{c})\phi_{c}}$ where $i$ is the ith example, $j$ is the jth cluster, and $k$ is the total number of clusters. The likelihood indicates that an example was drawn from the Gaussian distribution of the cluster.
\item compute the membership score of each example per cluster, $\phi_{j} = \frac{1}{N}\sum_{i=1}^{N}b_{i}^{j}$ where N is the number of examples per cluster $j$.
\item update the label assignment such that examples are associated with clusters where the cluster membership score is highest.
\item update mean/centroid $\mu_{j}$ and variance $\sigma^{2}_{j}$ per cluster.
\item repeat steps 2 through 6 until cluster mean and standard deviation remain constant.
\end{enumerate}

K-means, K-medoids, and GMM were selected because they are reliable diverse data type methods that use unique example assignment strategies. Therefore, diverse types of joystick data like rotation, translation, and expert pilot trajectories can be reliably clustered. Additionally, comparing multiple unsupervised strategies improves the possibility of finding an unsupervised label similar to each ground-truth label. Unsupervised method labels were compared with SD ground-truth labels, using five different feature-type combinations as the feature matrix $X$. Feature matrix combinations consisted of: position, position/velocity, position/velocity/acceleration, position/two position DWTs, position/6 position DWTs, PCA components of position/velocity/acceleration. These five combinations were selected because they combine position trajectories with other unique representations of joystick behavior. Position combined with derivative representations were motivated by human movement science findings where position movements depend on velocity and acceleration information. Combinations of position and the DWT were used in order to create a feature space with both important temporal and frequency representations of joystick behavior. Finally, PCA was used on position and derivative representations in order to create a feature space with important temporal representations of joystick behavior. PCA is a dimensionality reduction technique where data is transformed into a new coordinate system where data is organized from highest to lowest data variance \cite{Burkov_2019_ML}. Scikit-learn was used to calculate K-means, GMM, and PCA methods; the scikit-learn-extra package was used to compute K-medoids.

% --------------------------------------

\subsubsection{PERFORMANCE METRICS}
\label{PERFORMANCE_METRICS}
% Average 5-fold cross validation test prediction accuracy and ROC-AUC measures were used to evaluate ML model performance.
Individual metric comparisons, of the three metrics, were evaluated using the same Wilcoxon signed-rank or rank-sum tests where p < 0.05 and p < 0.001 were considered significant and strongly significant respectively; only non-parametric tests were used because the KS test reported non-parametric distributions. Accuracy measured the true positive (TP) and true negative (TN) counts over the total number of samples; a value of 1 and 0 correspond to 100\% and 0\% correct prediction. Accuracy only gives information about how well the model approves data, but not about how well the model rejects data.
\begin{equation}
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\label{eqn_accuracy}
\end{equation}
where FP and FN correspond to false positive and negative counts, respectively. The accuracy metric in both TensorFlow and scikit-learn were used.

The familiar ROC- AUC measure was used to evaluate both classification acceptance and rejection performance. ROC-AUC is the area under the false positive rate (FPR), shown in equation (\ref{eqn_fpr}), versus the True Positive Rate (TPR), shown in equation (\ref{eqn_tpr}),
\begin{equation}
FPR = \frac{FP}{FP+TN}
\label{eqn_fpr}
\end{equation}

\begin{equation}
TPR = \frac{TP}{TP+FN}
\label{eqn_tpr}
\end{equation}
\noindent An ROC-AUC score of one indicates perfect prediction of all labeled classes, whereas a score of 0.5 or lower indicates that prediction of all labeled classes was poor with chance level performance or lower. ROC-AUC was needed in addition to accuracy to determine if FP values were balanced with TP values, ensuring that the SD model could accurately reject and accept the data \cite{Burkov_2019_ML}. AUC and roc auc score metrics in TensorFlow and scikit-learn were used respectively.

%\noindent \emph{PERMUTATION IMPORTANCE AND FEATURE-TYPE DEPENDENCY}\\
Feature importance was of interest because each feature contained distinct information about disorientation. It was of interest to understand which feature/s could convey the most informative information about the occurrence of perceptual disorientation. Feature importance was calculated such that each feature was shuffled individually and model accuracy was calculated for each shuffled feature. Unshuffled model prediction accuracy was subtracted with each of the shuffled feature prediction accuracy scores. The change in prediction accuracy for each shuffled feature was ranked, such that the feature with the largest change in prediction accuracy was considered the most important feature. The scikit-learn permutation importance (PI) function was used to calculate PI for SVM and RF models, PI was calculated manually for the remaining NN, LSTM, Transformer, CNN, and LSTM-CNN Tensorflow models.

A dependency score was constructed to evaluate each model architecture's dependence on three feature-types, the score was calculated by first obtaining the minimum number of used feature-types. For example there were three, 18, and six time, frequency, and time \& frequency features, therefore three was selected as the minimum number of feature-types and only the best three features from each group were used. A ratio was constructed such that the minimum number of feature-types, which was 3, was divided by the sum of the first three permutation important ordered features, per feature-type.
\begin{equation}
dependency~score = \dfrac{3}{\sum\limits_{rank=0}^{2}  PI~rank},
\label{dependency_score}
\end{equation}
where the dependency score was calculated for each feature-type and the PI rank corresponded to the first three features for each respective feature-type. A value of 1 signified that the permutation important feature-types changed model predictions during PI thus the model was strongly dependent on these features-types. A value closer to zero indicates that the feature-type did not strongly change model prediction during PI, and thus the model weakly depends on these features-types.

%\noindent \emph{RAND SCORE}\\
The rand score, also known as the rand index, is a metric that captures similarity of two labels using the same number of clusters, labels are compared regardless of the ordering of cluster assignment. The rand score is the ratio of the number of agreeing label pairs and the number of label pairs. The scikit-learn rand score function was used for all unsupervised label comparisons.

% --------------------------------------

\subsubsection{MODEL IMPLEMENTATION}
Python was used for all analyses, using numpy, pandas, scipy, pywt, tensorflow, scikit-learn, seaborn, plotly, and matplotlib (Python 3.9, Python Software Foundation, Fredericksburg, Virginia, USA). Modeling analysis was performed using jupyter-notebook with the PyPy3, Just-in-Time Compilation kernel for faster computational performance. Regarding Tensorflow models, batch size and epochs were set to 32 and 100 respectively, with early stopping; the Transformer model used a batch size of 64. He Uniform weight matrix initialization and Adam optimisation was used; default $\beta_1$, $\beta_2$, and learning rate values were used.

% --------------------------------------

\subsection{RESULTS}
\label{PART2_RESULTS}

\subsubsection{MODEL ARCHITECTURE, DATA USE-CASE, AND FEATURE-TYPES}
\label{PART2_RESULTS_MODEL_ARCHITECTURE_USECASE_FEATURE_TYPES}
Classification accuracy and ROC-AUC score on the test dataset was used to evaluate the predictive ability of each of the seven models. Figure \ref{fig6} shows the test prediction accuracy of the seven models for each data use-case and feature quantity, using the position/velocity/acceleration feature set. The model-type and data use-case are written on the left, and each feature quantity is depicted by a colored point for each respective model-type. Each model-type and use-case tested four different feature quantities, thus corresponding to four points per model on each line. The average model test accuracy from greatest to smallest was LSTM, RF, Trans, SVM, MLP, CNN, and LSTM-CNN with mean accuracy of 0.84, 0.82, 0.77, 0.67, 0.58, 0.45, and 0.44 respectively; the average for each model was computed across experiments, use-cases, labels, feature-types, and feature quantities.

% -------------------- Figure6 --------------------
\begin{figure*}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure6_2.eps}
\end{center}
\caption{Test prediction accuracy for the six model-types using the position/velocity/acceleration feature set for different: data use-cases, ground-truth labels (strict, lenient, complex), and feature quantity used. For visual ease, circles and crosses correspond to the rotation and translation experiment types, respectively. Blue, orange, green, red, purple, brown, and magenta correspond to the SVM, RF, MLP, LSTM, Transformer, CNN, and LSTM-CNN models. Regarding notation, ax0, ax1, ax2 indicate RO, PI, YA for the rotational task and LR, FB, UD for the translational task. Additionally, sup, sub, and all denote sup speed stimulation trials, sub speed stimulation trials, and all trials,
respectively.}
\label{fig6}
\end{figure*}
% --------------------

\noindent \emph{Model-type vs use-case type}\\
SD prediction accuracy was evaluated for use-case data in comparison to using all the data, to determine whether SD modeling was more effective by use-case. As previously mentioned, SD is currently evaluated per use-case, however it is unclear if this is the best strategy for predicting SD. Therefore it was of interest to demonstrate the most effective way to model SD. Two analyses were performed such that accuracy for speed or axes data use-case was compared with general data usage, regardless of model-type or ground-truth label. Regarding Accuracy and ROC-AUC comparison for speed data use-case with general data usage across model-type, statistical analysis showed non-significant differences for both Accuracy and ROC-AUC. Thus the three ground-truth labels did not distinguish speed use-case data better than general data. Similarly for axis data use-case compared with general data usage across model-type, statistical analysis showed non-significant differences for ROC-AUC. However, Accuracy was significantly higher for the translational lenient label use-case (mean accuracy: 0.84) than general data usage (mean accuracy: 0.79), regardless of model-type (translational lenient, axis vs all data: KS: non-normal distribution, signed-rank: p < 0.05, n=7). Next, we determined if certain model architectures could better distinguish use-case data than general data, regardless of the ground-truth label. Individually, model architectures did not have significantly better Accuracy or ROC-AUC performance for use-case data than general data. In summary, constructing a general model including all the use-cases will create a reliable SD predictive model. However for certain use-cases with appropriate label selection, SD prediction can be significantly improved by modeling with use-case data only. Due to the fact that use-case and general model performance was statistically similarly for the majority of use-cases, only general model performance was evaluated for the remaining analyses.


\noindent \emph{Model-type vs feature quantity}\\
% For a specific model architecture, how many features do we need to predict SD reliably? Or, does the model architecture influence the number of required features needed to predict SD?
The goal of this analysis was to investigate whether specific model architectures required a certain number of features to reliably predict SD. We evaluated the number of needed features per model-type using two methods: PI and comparison of accuracy for 'all', 'top3', 'top2', and 'top1' models. Models constructed with 27, three, two, and one feature/s were referred to as 'all', 'top3', 'top2', and 'top1' models, respectively. PI was calculated for each model-type, data use-case, and ground-truth label. Figure \ref{fig7} shows each model architecture, data use-case, and ground-truth label with respect to the three most important feature-types.
% -------------------- Figure 7 --------------------
\begin{figure*}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure7_2.eps}
\end{center}
\caption{Feature order importance based on permutation importance, for each model. Blue, orange and green circles indicate most important feature, second most important feature, and third most important feature.}
\label{fig7}
\end{figure*}
% --------------------
Regarding PI, Transformer, MLP, SVM and RF require at least 6, 4, 4, and 2 features in order to have reliable prediction of SD. LSTM, CNN, and LSTM-CNN had minuscule fluctuating differences from baseline measures when columns were individually permuted, therefore it is unclear how many features were required by these models; features were ordered from most important to least important but values with respect to baseline were too small for comparison. Therefore, we compare accuracy differences for usage of more to less features, for each model architecture. Concerning accuracy, 'top3' models reported the highest accuracy for all model-types. For the rotational task, the model-types from most to least accurate in terms of prediction, were Transformer, LSTM, RF, SVM, MLP, CNN, and LSTM-CNN where accuracy was 0.76, 0.74, 0.66, 0.55, 0.53, 0.5, and 0.47 respectively. Similarly for the translational task, the model-types from most to least accurate in terms of prediction, were Transformer, LSTM, RF, SVM, MLP, LSTM-CNN, and CNN where accuracy was 0.93, 0.9, 0.85, 0.71, 0.66, 0.46, and 0.44 respectively. This result shows that three features are likely to be sufficient for reliable prediction of SD. Moreover, Transformer, LSTM, and RF models are more accurate at predicting SD with three features than other model architectures.


\noindent \emph{Model-type vs feature-type via PI}\\
Next, feature-type was evaluated with respect to model-type. For each model, regardless of the ground-truth label, the most used feature-types were ranked. For both rotation and translation, certain model-types consistently used certain types of features. Feature-type with respect to model-type was further evaluated, by grouping the 27 features into three categories of time, frequency, and time \& frequency feature-types. Using the dependency score that was defined in subsection \ref{}, we quantify the amount in which each model-type uses time, frequency, and time \& frequency feature-types to accurately predict SD. Table \ref{table5} shows the time, frequency, and time \& frequency feature dependency score, for each model-type; rotational task results are shown above translational experiment results, and mean accuracy per model-type using all data are given.
\begin{table}[h!]
\caption{MODEL ARCHITECTURE DEPENDENCY ON TIME, FREQUENCY, AND TIME \& FREQUENCY FEATURES}
\label{table5}
\centering
\begin{tabular}{c|ccccc}
\hline
Model & feature-type & time & freq & tf & mean acc\\
\hline
SVM & freq & 0.11 & \textbf{0.5} & 0.26 & 0.55 \\
RF & freq & 0.16 & \textbf{0.7} & 0.07 & 0.66 \\
MLP & freq & 0.1 & \textbf{0.75} & 0.1 & 0.53 \\
LSTM & time & \textbf{1} & 0.25 & 0.04 & 0.74 \\
CNN & time & \textbf{1} & 0.25 & 0.04 & 0.49 \\
LSTM-CNN & time & \textbf{1} & 0.25 & 0.04 & 0.44 \\
Trans & tf & 0.09 & 0.31 & \textbf{0.69} & 0.76 \\
\hline
SVM & freq & 0.11 & \textbf{0.87} & 0.06 & 0.71 \\
RF & freq & 0.16 & \textbf{0.42} & 0.05 & 0.85 \\
MLP & freq & 0.2 & \textbf{0.7} & 0.06 & 0.63 \\
LSTM & time & \textbf{1} & 0.25 & 0.04 & 0.9 \\
CNN & time & \textbf{1} & 0.25 & 0.04 & 0.44 \\
LSTM-CNN & time & \textbf{1} & 0.25 & 0.04 & 0.46 \\
Trans & tf & 0.05 & 0.21 & \textbf{0.67} & 0.93 \\
\hline
\end{tabular}
\end{table}
SVM, RF, and MLP models depended on DWT frequency-only features the most, with a dependency score ranging from 0.5 to 0.75. LSTM, CNN, and LSTM-CNN models strongly depended on time-only features, with the maximum dependency score of 1. The Transformer architecture depended on time \& frequency features, the short-time FFT and CWT, with a dependency score of 0.66. It is likely that SVM, RF, and MLP model architectures intrinsically selected frequency-only features because repeated groupings in feature space caused by periodic wavelets were likely to be easier to distinguish with respect to the label for pure gradient descent dependent architectures. More detailed or finer resolution features like time domain features, are likely to facilitate more accurate predictions for LSTM, CNN, and LSTM-CNN because the gating structure of LSTM requires detailed data to distinguish long-term dependencies. Additionally, CNN smoothing methods like convolution and pooling are only effective on detailed data. Finally, the parallel and multi-representative Transformer structure thrives on sparse simplistic features with minute differences like the time \& frequency features. 



\noindent \emph{Model-type vs feature-type via human movement science knowledge}\\
Model performance for models composed of position, velocity, and acceleration features were compared with models composed of position-only features. As previously mentioned, human movement science has proven that derivative components of position trajectories assist in smooth position movements. Thus it was of interest to determine whether derivative components of joystick would improve prediction results. Accuracy was not statistically greater when position, velocity, and acceleration features were used, in comparison to position only features; here was slight significance for rotational Transformer and translation RF models. Additionally, ROC-AUC was statistically greater when position, velocity, and acceleration features are used, in comparison to position only features, for MLP rotation models (ROC-AUC mean (pos/vel/acc, pos): MLP (0.74, 0.56)). Significance for all comparisons were such that pos/vel/acc versus pos feature matrix results were compared using: KS: non-normal distribution, Bonferroni required value of signed-rank: p < 0.0167, sum-rank: p < 0.0167, n=12. In summary, including derivative position features are likely to improve predictions, however improvements are not likely to be strongly significant.


\noindent \emph{Quantity of data required}\\
We used the best performing model-type, LSTM, and investigated the ideal quantity of data required to reliably predict SD. For both rotation and translation experiments, using the LSTM model architecture, we show that the entire trial data of 40 seconds does not need to be used in order to obtain the best prediction of SD. Different data timesteps, including 1, 2, 4, 10, 20, 40 seconds, were tested for each ground-truth label and feature-quantity model, such that the ideal timestep length was selected based on highest obtained prediction accuracy per label and feature-quantity. For the rotational task, based on frequency of occurrence across different ground-truth labels and feature-quantity, 20 seconds (200 data points at a sampling frequency of 10Hz) was only needed to predict SD with a mean accuracy of 0.7. Similarly for the translational task, 4 seconds (40 data points at a sampling frequency of 10Hz) was needed to predict SD with a mean accuracy of 0.85. 


\subsubsection{GROUND-TRUTH LABEL COMPARISON}
Table \ref{table6} shows that the lenient ground-truth label returns the best predictive results, for top performing models including LSTM, RF, Transformer, and SVM and less performant models such as MLP.
\begin{table}[h!]
\caption{GROUND-TRUTH LABEL COMPARISON}
\label{table6}
\centering
\begin{tabular}{c|ccccccc}
\hline
& \multicolumn{7}{c}{\centering Model Accuracy}\\
Label & LSTM & Trans & RF & SVM & MLP & CNN & LCNN\\
\hline
\textbf{Rot L} & \textbf{0.8} & \textbf{0.8} & \textbf{0.9} & \textbf{0.7} & \textbf{0.6} & \textbf{0.5} & \textbf{0.5}\\
Rot S & 0.6 & 0.6 & 0.5 & 0.4 & 0.5 & 0.6 & 0.5\\
Rot C & 0.6 & 0.6 & 0.5 & 0.4 & 0.4 & 0.3 & 0.3\\
\hline
\textbf{Trans L} & \textbf{0.9} & \textbf{0.9} & \textbf{0.9} & \textbf{0.8} & \textbf{0.7} & \textbf{0.5} & \textbf{0.5}\\
Trans S & 0.7 & 0.7 & 0.7 & 0.6 & 0.6 & 0.5 & 0.5\\
Trans C & 0.8 & 0.7 & 0.8 & 0.6 & 0.5 & 0.3 & 0.3\\
\hline
\end{tabular}
\end{table}
The remaining least performant models such as CNN and LSTM-CNN show that a binary label is more appropriate for model SD than a three class multi-label model. Considering the two binary ground-truth labels, lenient and strict, the lenient label is hypothesized to produce better predictive results than the strict label because the lenient label allows for more samples to be labeled 'non-SD' than the strict label. The lenient label allows for initial mistakes to be made, with a decision criteria for labeling based on initial and eventual correctness, thus facilitating balanced class selection that allows for a better numerical representation of a 'non-SD' event in comparison to an SD event. The strict ground-truth label construction, where data was labeled as SD if a single mistake is made, is not unrealistic because pilots are trained to fly making zero to very little mistakes. Our participant population of novice compensatory trackers had difficulty performing the task of detecting motion, the criteria to make no initial mistakes adds an additional layer of difficulty to the task; as mentioned the best performer could only accomplish $71\%$ of the task correctly. Therefore, it is likely that our population generated very little 'non-SD' trial samples for the strict ground-truth label, resulting in a poor representation of the 'non-SD' class and thus poor classification results. Despite class balancing techniques, more data is likely to be needed for this ground-truth label construction. Regarding the complex multi-label, classification of initially, eventually, and not correct would likely improve if more data was available to characterize the three classes. It is known that multi-label classification requires more training data than binary classification \cite{Ng_2021_Deep_learning_specialization}.


\subsubsection{UNSUPERVISED CLUSTERING AND GROUND-TRUTH  LABEL COMPARISON}
K-means, GMM, and K-medoids unsupervised clustering methods were used with six different feature matrices, for each data use-case. Figure \ref{fig8} shows three heatmaps displaying the rand score for the three clustering methods; K-means, GMM, and K-medoids shown from top to bottom.
% -------------------- Figure 7 --------------------
\begin{figure*}[htp]
\begin{center}
\includegraphics[width=1.0\linewidth]{figures/figure8_1.eps}
\end{center}
\caption{Clustering labels compared with ground-truth labels via the rand score.}
\label{fig8}
\end{figure*}
% --------------------
Each unsupervised label was compared with the three ground-truth labels, to quantify how well each clustering method and feature matrix could replicate each experimental ground-truth label. Clustering methods that had highest to lowest rand scores, were K-medoids, K-means, and GMM. In particular, K-medoids achieved highest rand scores using position and velocity features while being compared to the the lenient label, for both rotational and translational tasks. K-means achieved highest rand scores using position and all DWT features with respect to the lenient label for rotation. However, for the translational task, K-means performed best using PCA features of position, velocity, and acceleration while being compared to the lenient label. GMM achieved highest rand scores using position and all DWT features with respect to the lenient label for rotation and translation. Mean rand scores across all data use-cases for K-medoids, K-means, and GMM were 0.77, 0.75, 0.7 for rotation and 0.82, 0.69, 0.65 for translation. For completeness, the lenient, strict, and then complex ground-truth labels were best to worst replicated using unsupervised clustering; the maximum achieved rand scores regardless of the clustering method were 0.77, 0.72, and 0.66 for lenient, strict, and complex respectively. For both rotational and translational tasks, the lenient label was best replicated using unsupervised clustering, where the maximum achieved rand scores regardless of the clustering method were 0.82 and 0.77 respectively. The strict label followed by the complex label, were replicable  using clustering with max rand scores of 0.72 and 0.66 for rotation and 0.73 and 0.73 for translation.


\section{DISCUSSION}
In this comprehensive study on SD, we show that it is possible to isolate, simulate, and recreate realistic aspects of a vestibular feedback dead-reckoning piloting task and create an SD occurrence dataset. Using the SD occurrence dataset, supervised classification methods were used to build and identify best predicting models, using joystick features and detection performance based labels. Non-supervised classification was used to determine the best clustering methods an joystick features that most accurately identified the occurrence of SD; detection performance labels were used as ground-truth objectives. During SD measurement and experimental quantification in Part I, we demonstrate how to create a minimalistic SD experimental dataset using a compensatory vestibular dead-reckoning task. A compensatory tracking vestibular dead-reckoning task was ideal because it is the most basic required flight task, the study of SD during the most basic flight task assures SD identification in its most basic form and the results can be generalized to all flight situations and/or SD use-cases. The generalized experimental design allowed for the collection of perceptual response joystick data during various basic scenarios of vestibular and proprioceptive stimulation; SD or non-SD states were apparent via the joystick response. A crucial data standardization step was used to verify that the simulator system correctly performed the experimental design, removing trials with delays and erroneous motion. The SD-targeted dataset captured known human motion detection trends, demonstrating that the real-time motion simulation environment was fidel despite the functional timing delays \cite{Stoffregen_2003_Nature}. To mitigate functional timing issues, where some events were executed incorrectly before other events, programming events could have been grouped into functional blocks or scripted codes, where similar tasks were executed in synchrony. Known motion detection trends include: a) accurate and faster response for sup speed stimulation in comparison to near below-threshold speed stimulation, b) PI, RO, FB, LR, UD, and YA were the least to most difficult axis tasks, c) longer reaction times corresponded with task difficulty for the respective rotational and translational experiments \cite{Valko_2012_Vestibular}, \cite{Hartmann_2014_Direction}, \cite{Karmali_2017_Multivariate}. Ranking of task difficulty per axis confirms literature reports that there is no sensory advantage for UD detection due to gravity, because the vestibular system compensates for gravity \cite{Valko_2012_Vestibular}. In addition to confirming known motion detection trends, functional differences in motion detection for RO, LR, \& FB and PI, YA, \& UD tasks were observed; where the most counted response for RO, LR, \& FB was EC in comparison to IC for PI, YA, \& UD. It is unclear why participants made more initial mistakes for RO, LR, \& FB than PI, YA, \& UD axes, however perhaps participants relied on more non-vestibular sensory cues (proprioception, tactile, auditory from the simulator motor) and/or had better natural upright posture during certain stimulus motions than others. Perhaps in PI, YA, and UD they relied more on clear vestibular cues because they self-generated less additional motion information from self-motion or joystick interaction, thus allowing participants to either initially detect correctly or not. It appears plausible that in RO, LR, and FB participants were more likely to generate additional and perhaps conflicting sensory information by naturally tilting or turning the head. It is likely that participants naturally adapted their posture, to be more or less upright, during certain motion stimuli in comparison to others. For example, a slightly left tilted head during pitch motion would more likely induce discomfort than during roll motion, thus encouraging participants to naturally sit upright during pitch stimuli and thus giving them an advantage to detect the motion more clearly. Such functional errors in RO, LR, and FB caused by natural postural behavior could easily escalate the occurrence of spatial disorientation. Statistical analysis showed that regardless of experimental conditions, the best performers achieved 76\% detection accuracy and average performers achieved 51\% accuracy. The task may have been difficult because participants were given the freedom to decide on which axis and in what direction the stimulation occurred, as is done in a real-life piloting situation. All participants had very little to no piloting experience, thus our results reflect human motion perception without the influence of piloting experience or training. Thus, the modeling results obtained from this dataset may not be representative of expert piloting behavior, because our novice participants’ responses have more variability than expert piloting responses. One-third of the participants experienced physical disorientation during the task; however, no significant relationship between physical disorientation and motion detection was found. There was a trend where participants who initially detected unsuccessfully felt worse after the experiment than participants who did initially detected successfully or did not try. More sample points regarding physical disorientation are needed during the experiment, instead of a sample before and after the task, in order to determine if physical disorientation is correlated with motion detection. We do not claim that questionnaire methods can not quantify SD, however before and after questionnaire samples may not produce enough data to find statistically significant correlations with other SD measures especially when population sample size is small. A physiological sampling measure that implies physical discomfort, with a sampling rate comparable to that of the joystick, such as EEG, NIRS, heart rate, or electrodermal activity, could provide more insight into correlations with physical and perceptual disorientation. Similarly, HAR IMU measures and even human pose camera measurements could be viable motion monitoring measures to capture physical discomfort. Trembling, body-position shifting, and postural changes are markers for physical discomfort, that may attribute to successive errors thus increasing the likelihood for SD to occur.

During SD model prediction in Part II, we demonstrate how to build a reliable predictive model for SD using joystick derived features and task performance labels (e.g., initially correct, eventually correct, never correct). Model architecture type, data use-case type, feature quantity, feature-type, and required data quantity were evaluated for joystick features to identify modeling specifications for effective SD prediction. Regarding model-type, LSTM, RF, Transformer, and SVM were found to produce the highest predictions for SD; average predictions across all conditions were 0.84, 0.82, 0.77, and 0.67 for models respectively. Confirming which model architectures best characterized SD behavior, contributes to both SD monitoring and HAR. The result that the LSTM model best predicted SD was in alignment with HAR literature, where often the LSTM model best predicts human behavior. LSTM was found to outperform SVM, for time-series human movement data. Additionally, the RF model predicted similarly well as the LSTM model; HAR literature rarely mentions Rf as a reliable model for HAR. It is likely that our low frequency joystick data was easier to characterize than typical high frequency accelerometer data, thus allowing for RF to be a viable model. Next concerning use-case data, models built using all data had similar accuracy and ROC-AUC as models built using use-case data. This result is useful and important for SD monitoring because currently SD is described by use-case, instead of in a general manner. Definitions and/or predictions of SD can be simplified by combining all the data into a single model, instead of building separate models for each use-case. 

Each model architecture type thrives when a certain number of key features are present. Permutation importance and accuracy prediction were used to investigate feature quantity by quantifying the number of important/dominant features per model-type. Transformer, SVM/MLP, and RF used 6, 4, and 2 important features. Additionally, models were ranked by prediction accuracy using a fixed number of features. Three features were identified as an ideal number of features where models-types generated highest prediction accuracy; Transformer, LSTM, and RF generated the highest accuracy predictions for both rotation and translation experiments where accuracy was 0.76, 0.74, and 0.66 and 0.93, 0.9, and 0.85 respectively. The two feature quantity results show that the top predicting model-types (Transformer, LSTM, and RF) only require roughly three key features to reliably predict SD. Feature quantity results with respect to model-type contribute to both SD monitoring and HAR fields. It was of interest to know which minimal features with respect to model architecture are needed to predict human activity. Computational resources can be saved using less features while obtaining similar results.

The effects of feature-type on model architecture was investigated in two ways: usage of time and/or frequency feature and usage of joystick derivative components as features.  Regarding time and/or frequency feature usage, feature ordering from most to least important dictated by permutation importance was used to construct a time, frequency, and time \& frequency dependency score for each model-type. The dependency score showed that model-types used specific feature-types consistently. LSTM, CNN, and LSTM-CNN models used time features. RF, SVM, and MLP used DWT frequency features, and Transformer depended heavily on CWT and short-time FFT time \& frequency features. Regarding human movement science motivations to use derivatives of the joystick as features, small non-significant improvements in prediction accuracy were observed when joystick derivatives were used as features in comparison to joystick-only features. Feature-type results with respect to model-type contribute to both SD monitoring and HAR fields. It is commonplace to test feature-type combinations with different model architectures, typically using a process of trial and error. However, our dependency score constructed from permutation importance provides insight about which feature attributes are exploitable by model architectures. Time spent on testing features and models could be reduced by having beforehand knowledge about which feature attributes are most appropriate for specific model architectures. Additionally these results shows that model architectures are not being optimally compared during bench-marking studies, because the same time domain feature-type are often used across all model-types even-though some models are in-adapted to using time features. Recent HAR literature often adopts better modeling strategies, such as LSTM and Transformer, however older modeling strategies like SVM and RF are able to provide competitive results if they are given feature transformations that are more suitable to their architecture.

The LSTM model-type was consistently the highest predicting model for model-type, data use-cases, and feature comparisons. Despite the fact that 40 seconds of trial data per prediction was available, it is possible that less trial data was necessary to similarly or more accurately predict SD. Additionally, during real-life SD detection it was of interest to know how often can one predict SD, meaning the data window required for SD prediction. The minimally required data length window for accurate SD prediction was identified by testing six data lengths, where lengths were 1, 2, 4, 10, 20, 40 seconds. For the rotational task, where adjustments to the joystick were more frequently made than in the translational task, 20 seconds of data was required such that SD prediction was most accurate. The translational often required only 4 seconds of data to generate the most accurate predictions. Identification of an ideal temporal window for SD prediction contributes to the field of SD monitoring. Real-time SD prediction monitoring could be improved by strategic windowing of past data.

The second analysis concerned determination of appropriate SD ground-truth labels. Three ground-truth labels constructed with respect to motion detection and compensation performance, demonstrated that certain characterizations/definitions of SD describe joystick feature data more than others. Lenient, strict, and complex ground-truth labels corresponded to the SD cases where IC \& EC responses were considered non-SD, IC responses were considered non-SD, and IC response were non-SD while EC responses were moderate SD, respectively. Prediction accuracy, regardless of other conditions, across model-type was more accurate for the lenient label in comparison to the other labels. Thus, the lenient label convention best characterizes the data, and reaffirms the existing functional definition, where SD occurrence is defined as 'involving successive failures and major performance errors' \cite{Newman_2007_SD}. These results contribute to the field of SD monitoring, assisting with defining SD from a numerical perspective. 

The third and final analysis employed unsupervised classification to quantify the error between ground-truth SD labels and unsupervised clustering method labels, via the rand score. In addition, ideal features for clustering methods were identified such that the error between ground-truth labels and unsupervised labels were minimum. The lenient ground-truth label received the highest rand-score across all clustering methods, in comparison to strict and complex ground-truth labels. K-mediods using position \& velocity or position, velocity, \& acceleration features replicated the three ground-truth labels better than other clustering methods and feature combinations. In particular, K-mediods with position \& velocity or position, velocity, \& acceleration features best replicated the lenient label with rand scores of 0.77 and 0.82 for the rotational and translational task respectively. This unsupervised classification analysis contributes to the field of SD monitoring. This result is significant for the field of SD monitoring because we provide a quantitative measure of error for labeling SD occurrence, when it is currently unknown how to identify SD with 100\% certainty. Additionally, we can not reasonably say how similar real-life joystick manipulations are to our experimental dataset, however we demonstrate the usage of clustering methods on two very different joystick trajectories and tasks (rotational and translational). Realistic data is likely to have less variance, than our novice participant dataset, however after scaling, the data is likely to be similar to our experimental dataset because human response movement is limited to a small frequency range. Therefore, these clustering methodologies are strongly likely to apply to real joystick data. Using K-mediods with position \& velocity or position, velocity, \& acceleration features, the occurrence of SD can be reliably predicted.

Several aspects of this two part study could have been improved. Regarding part I, the engineering of the motion simulation experiment could have been controlled better, using less real-time functionality such that required sequential events were executed in a desired order and not in the order of fastest execution. Less trials would have been removed during the data standardization step, if event order was sequentially guaranteed. In addition, data preprocessing during experimental data collection could have been integrated such that only necessary data for analysis was saved. For example, data was saved for the entire experiment however it would have been more efficient if data from the start and end of each trial was only saved. Due to the fact that the entire experiment was saved, an additional pre-processing step was needed to remove the unrequired data. In addition to the mentioned work limitations in Part I, class balancing and image preparation could have been performed differently in Part II. Class balancing using oversampling was performed, such that samples from each minority class were randomly selected to pad the respective class until samples were equivalent to the majority class. Oversampling produces datasets with less variability, therefore it would have been better to use techniques that numerically generate samples for the minority class, such as GANs or SMOTE using kmeans \cite{An_2021_Mgait}. Undersampling was not considered because there were already few class samples due to the rigorous data standardization process. Finally, LSTM-CNN and CNN prediction accuracy was lower than HAR literature reports because we used matrix representations of data, instead of equivalent figure rendered representations \cite{Nedorubova_2021_CWT_CNN_HumanActivity}. Specifically, matrix representations of CWT and short-time FFT features were used instead of mathplotlib figure rendered representations; figure representations display more detailed color variations of both CWT and short-time FFT information. This additional shading information that is generated by plotting the matrix may give better CNN accuracy. Initially, CWT and short-time FFT features were plotted using mathplotlib and saved as png images. The image files were opened and used as inputs to the CNN, average results were approximately 0.6-0.7 accuracy. Saving and opening thousands of image files was computationally expensive, therefore the data matrix representation was used instead of the image representations and average accuracy results decreased to approximately 0.45. Despite the lower accuracy values for CNN and LSTM-CNN, we used the matrix representations such that all models could be equally compared using the same numerical data.


\section{CONCLUSION}
This two-part study of SD demonstrated effective measurement and predictive methods for quantifying the occurrence of SD. This multi-disciplinary work contributes to many domains of study including aeronautic, psychology human movement science and motion detection, control theory human-in-the-loop, and HAR. In part I, human motion detection and compensatory control were measured for different orientation, speed, and axis stimuli. Translational and rotational motion detection orientations were investigated where fast (sup) and slow (sub) stimuli speeds were tested in combination with directional axis stimuli (RO/PI/YA, LR/FB/UD). Experimentation results showed that SD occurred less for faster detectable sup speeds than slower sub speeds. Additionally, SD occurred least to most for PI, RO, FB, LR, UD, and YA for both rotational and translational motion; speed and axis results were in alignment with human motion detection literature results. Vestibular deadreckoning is a difficult task however it is not impossible, because evaluation of global performance showed that best performers could correctly detect motion $76\%$ of the time and average performs could detect motion $51\%$ of the time. Therefore, the study of SD using vestibular deadreckoning is not unrealistic, and responses contain sufficient samples of SD and non-SD behavior. Physical disorientation questionnaire results did not significantly correlate with motion detection performance, however there was a trend that poor and good performers felt worst and similarly after the experiment respectively. Therefore, physical disorientation could be a potential feature for predicting SD, however questionnaire measurements are insufficient because they do not give enough quantitative samples. Other types of physical disorientation and/or discomfort measures with more measurement samples are needed. The creation of an SD dataset uniquely contributes to the field of SD monitoring, as mentioned in section \ref{INTRODUCTION} SD has been predominantly quantified by use-case using questionnaire reports. Recently, human and environmental measures related to SD have been used to quantify the occurrence of SD using statistical and/or modeling methods during specific use-case situations.  Our dataset not only quantifies SD statistically and by modeling, it purposely controls the context (e.g., vestibular stimulation without visual) such that SD responses are for a reduced case that is common to all flight use-cases. The statistical and modeling results are plausible for all flight use-cases, thus one can make estimates about response behavior and/or modeling for more complex SD situations using our simplistic task results. Using the SD dataset that was constructed in Part I, modeling methods for predicting SD were investigated during Part II. Using HAR ML and DL techniques, model parameter tuning selections for SD prediction were investigated, and ideal tuning parameters are reported and explained. The following key model construction parameters were tested: model architecture type, data use-case, feature quantity, feature-type, quantity of data required, ground-truth label type, and unsupervised clustering with respect to ground-truth labels. The LSTM model architecture had the highest mean prediction accuracy of 0.84 across experiments, data use-cases, ground-truth labels, feature-types, and feature quantities. The LSTM model required at least three features, and 4 \& 20 seconds of data for translational and rotational data respectively. Permutation importance with the dependency score showed that specific model architectures performed better with time, frequency, or time \& frequency feature-type; SVM, RF, and MLP models depended mostly on frequency features; LSTM, CNN, and LSTM-CNN models strongly depended on time features; the Transformer model depended on time & frequency features. The lenient ground-truth label characterized the feature data better than the strict and complex labels; the lenient label definition was in alignment with the current functional SD definition. Unsupervised clustering revealed that K-medoids using position and velocity features most accurately replicated all ground-truth labels, this result implies that the occurrence of SD can be approximately predicted 80\% of the time from general flight joystick data.


% References
% --------------------------------------------
\bibliography{bib}
% --------------------------------------------
% OR
% --------------------------------------------
% \begin{thebibliography}{10}

% \bibitem{Bles_2008_SD}
% Willem Bles.
% \newblock Spatial disorientation training demonstration and avoidance.
% \newblock 2008.

% \bibitem{Gibb_2010_Aviation}
% Randy Gibb, Rob Gray, and Lauren Scharff.
% \newblock {\em Aviation visual perception: Research, misperception and
  % mishaps}.
% \newblock Ashgate, 2010.

% \bibitem{Perdriel_1980_SD}
% Georges Perdriel and Alan~James Benson.
% \newblock Spatial disorientation in flight: Current problems.
% \newblock Technical report, Advisory Group for Aerospace Research and
  % Development Neuilly-sur-Seine (France), 1980.

% \bibitem{Gillingham_1993_Spatial}
% Kent~K Gillingham and Fred~H Previc.
% \newblock Spatial orientation in flight.
% \newblock Technical Report AL-TR-1993-0022, 1993.

% \bibitem{Previc_2004_Spatial}
% F.~H. Previc and W.~R. Ercoline.
% \newblock {\em Spatial Disorientation in Aviation}.
% \newblock American Institute of Aeronautics and Astronautics, Reston ,VA, 2004.

% \bibitem{Newman_2007_SD}
% David~G Newman and AFAIM FAICD.
% \newblock {\em An overview of spatial disorientation as a factor in aviation
  % accidents and incidents}.
% \newblock Number B2007/0063. Australian Transport Safety Bureau Canberra City,
  % Australia, 2007.

% \bibitem{Chaudhuri_2013_Wholebody}
% Shomesh~E Chaudhuri, Faisal Karmali, and Daniel~M Merfeld.
% \newblock Whole body motion-detection tasks can yield much lower thresholds
  % than direction-recognition tasks: implications for the role of vibration.
% \newblock {\em J Neurophysiol}, 110(12):2764--2772, 2013.

% \bibitem{Angelaki_2008_Vestibular}
% D.~E. Angelaki and K.~E. Cullen.
% \newblock Vestibular system: The many facets of a multimodal sense.
% \newblock {\em Annu Rev Neurosci}, 31(1):125--150, 2008.

% \bibitem{Melvill_1978_Vertical}
% J.~G. Melvill and L.~R. Young.
% \newblock Subjective detection of vertical acceleration: a velocity-dependent
  % response.
% \newblock {\em Acta Otolaryngol}, 85:45—53, 1978.

% \bibitem{Soyka_2011_Predicting}
% Florian Soyka, Paolo~Robuffo Giordano, Karl Beykirch, and Heinrich~H
  % B{\"u}lthoff.
% \newblock Predicting direction detection thresholds for arbitrary translational
  % acceleration profiles in the horizontal plane.
% \newblock {\em Exp Brain Res}, 209(1):95--107, 2011.

% \bibitem{Valko_2012_Vestibular}
% Yulia Valko, Richard~F Lewis, Adrian~J Priesol, and Daniel~M Merfeld.
% \newblock Vestibular labyrinth contributions to human whole-body motion
  % discrimination.
% \newblock {\em J Neurosci}, 32(39):13537--13542, 2012.

% \bibitem{Hartmann_2014_Direction}
% Matthias Hartmann, Katia Haller, Ivan Moser, Ernst-Joachim Hossner, and Fred~W
  % Mast.
% \newblock Direction detection thresholds of passive self-motion in artistic
  % gymnasts.
% \newblock {\em Exp Brain Res}, 232(4):1249--1258, 2014.

% \bibitem{BermudezRey_2016_Vestibular}
% Mar{\'\i}a~Carolina Berm{\'u}dez-Rey, Torin~K Clark, Wei Wang, Tania Leeder,
  % Yong Bian, and Daniel~M Merfeld.
% \newblock Vestibular perceptual thresholds increase above the age of 40.
% \newblock {\em Frontiers in Neurology}, 7:162, 2016.

% \bibitem{Karmali_2017_Multivariate}
% Faisal Karmali, Mar{\'\i}a~Carolina Berm{\'u}dez~Rey, Torin~K Clark, Wei Wang,
  % and Daniel~M Merfeld.
% \newblock Multivariate analyses of balance test performance, vestibular
  % thresholds, and age.
% \newblock {\em Frontiers in Neurology}, 8:578, 2017.

% \bibitem{Cheung_2000_Disorientation}
% Bob Cheung, Kevin Hofer, Chris~J Brooks, and Peter Gibbs.
% \newblock Underwater disorientation as induced by two helicopter ditching
  % devices.
% \newblock {\em Aviation, Space, and Environmental Medicine}, 71(9):879--888,
  % 2000.

% \bibitem{Sargent_2008_Disorientation}
% Jesse Sargent, Stephen Dopkins, John Philbeck, and Joeanna Arthur.
% \newblock Exploring the process of progressive disorientation.
% \newblock {\em Acta Psychol}, 129(2):234--242, 2008.

% \bibitem{Denquin_2021_LAF}
% Francois Denquin, Jamilah Foucher, Simon Pla, Jean-Christophe Sarrazin, and
  % Benoit~G Bardy.
% \newblock Optical and gravito-inertial contributions to the perception and
  % control of height in a simulated low-altitude flight context.
% \newblock {\em Ergonomics}, 64(10):1297--1309, 2021.

% \bibitem{Burkov_2019_ML}
% Andriy Burkov.
% \newblock {\em The Hundred-Page Machine Learning Book}.
% \newblock Andriy Burkov Canada, 2019.

% \bibitem{Kennedy_1993_Simulator}
% Robert~S Kennedy, Norman~E Lane, Kevin~S Berbaum, and Michael~G Lilienthal.
% \newblock Simulator sickness questionnaire: An enhanced method for quantifying
  % simulator sickness.
% \newblock {\em International Journal of Aviation Psychology}, 3(3):203--220,
  % 1993.

% \bibitem{Bouchard_2007_SimulatorSickness}
% St{\'e}phane Bouchard, Genevi{\`e}ve Robillard, and Patrice Renaud.
% \newblock Revising the factor structure of the simulator sickness
  % questionnaire.
% \newblock {\em Annual Review of CyberTherapy and Telemedicine}, 5:117--122,
  % 2007.

% \bibitem{Landrieu_2017_Timetocollision}
% Jer\'{e}mie Landrieu, Jamilah Abdur-Rahim, Jean-Christophe Sarrazin, and
  % Beno\^{i}t Bardy.
% \newblock Time-to-collision estimates during congruent visuo-vestibular
  % stimulations.
% \newblock In {\em Studies in Perception and Action XIV: Nineteenth
  % International Conference on Perception and Action (IPCA)}, pages 109--112.
  % Psychology Press, 2017.

% \bibitem{Bellmann_2011_DLR}
% Tobias Bellmann, Johann Heindl, Matthias Hellerer, Richard Kuchar, Karan
  % Sharma, and Gerd Hirzinger.
% \newblock The dlr robot motion simulator part i: Design and setup.
% \newblock In {\em 2011 IEEE International Conference on Robotics and
  % Automation}, pages 4694--4701. IEEE, 2011.

% \bibitem{Shadmehr_2004_Computational}
% Reza Shadmehr and Steven~P Wise.
% \newblock {\em The computational neurobiology of reaching and pointing: a
  % foundation for motor learning}.
% \newblock MIT press, 2004.

% \end{thebibliography}
% --------------------------------------------

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]{First A. Author} (M'76--SM'81--F'87) and all authors may include 
% biographies. Biographies are often not included in conference-related
% papers. This author became a Member (M) of IEEE in 1976, a Senior
% Member (SM) in 1981, and a Fellow (F) in 1987. The first paragraph may
% contain a place and/or date of birth (list place, then date). Next,
% the author's educational background is listed. The degrees should be
% listed with type of degree in what field, which institution, city,
% state, and country, and year the degree was earned. The author's major
% field of study should be lower-cased. 

% The second paragraph uses the pronoun of the person (he or she) and not the 
% author's last name. It lists military and work experience, including summer 
% and fellowship jobs. Job titles are capitalized. The current job must have a 
% location; previous positions may be listed 
% without one. Information concerning previous publications may be included. 
% Try not to list more than three books or published articles. The format for 
% listing publishers of a book within the biography is: title of book 
% (publisher name, year) similar to a reference. Current and previous research 
% interests end the paragraph. The third paragraph begins with the author's 
% title and last name (e.g., Dr.\ Smith, Prof.\ Jones, Mr.\ Kajor, Ms.\ Hunter). 
% List any memberships in professional societies other than the IEEE. Finally, 
% list any awards and work for IEEE committees and publications. If a 
% photograph is provided, it should be of good quality, and 
% professional-looking. Following are two examples of an author's biography.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a2.png}}]{Second B. Author} was born in Greenwich Village, New York, NY, USA in 
% 1977. He received the B.S. and M.S. degrees in aerospace engineering from 
% the University of Virginia, Charlottesville, in 2001 and the Ph.D. degree in 
% mechanical engineering from Drexel University, Philadelphia, PA, in 2008.

% From 2001 to 2004, he was a Research Assistant with the Princeton Plasma 
% Physics Laboratory. Since 2009, he has been an Assistant Professor with the 
% Mechanical Engineering Department, Texas A{\&}M University, College Station. 
% He is the author of three books, more than 150 articles, and more than 70 
% inventions. His research interests include high-pressure and high-density 
% nonthermal plasma discharge processes and applications, microscale plasma 
% discharges, discharges in liquids, spectroscopic diagnostics, plasma 
% propulsion, and innovation plasma applications. He is an Associate Editor of 
% the journal \emph{Earth, Moon, Planets}, and holds two patents. 

% Dr. Author was a recipient of the International Association of Geomagnetism 
% and Aeronomy Young Scientist Award for Excellence in 2008, and the IEEE 
% Electromagnetic Compatibility Society Best Symposium Paper Award in 2011. 
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a3.png}}]{Third C. Author, Jr.} (M'87) received the B.S. degree in mechanical 
% engineering from National Chung Cheng University, Chiayi, Taiwan, in 2004 
% and the M.S. degree in mechanical engineering from National Tsing Hua 
% University, Hsinchu, Taiwan, in 2006. He is currently pursuing the Ph.D. 
% degree in mechanical engineering at Texas A{\&}M University, College 
% Station, TX, USA.

% From 2008 to 2009, he was a Research Assistant with the Institute of 
% Physics, Academia Sinica, Tapei, Taiwan. His research interest includes the 
% development of surface processing and biological/medical treatment 
% techniques using nonthermal atmospheric pressure plasmas, fundamental study 
% of plasma sources, and fabrication of micro- or nanostructured surfaces. 

% Mr. Author's awards and honors include the Frew Fellowship (Australian 
% Academy of Science), the I. I. Rabi Prize (APS), the European Frequency and 
% Time Forum Award, the Carl Zeiss Research Award, the William F. Meggers 
% Award and the Adolph Lomb Medal (OSA).
% \end{IEEEbiography}

\EOD

\end{document}