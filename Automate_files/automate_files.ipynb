{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e15054b",
   "metadata": {},
   "source": [
    "# Two automated ways to organize files: folder organizer after a big project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954c610",
   "metadata": {},
   "source": [
    "Have you ever finished coding a project and had 50 or more files that you need to organize in a single folder.  It can be extremely time-connsuming checking which files are necessary for the project, which files are not referenced by other files, or checking if Git errors exist.  If you use Git commandline and do not commit the files correctly, Git will write comments in your code files so that you can correct the conflicting Git errors.  Sometime it can be difficult to identify which file/s is/are the problem from the git interface.\n",
    "\n",
    "<img src=\"toolbox.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83717960",
   "metadata": {},
   "source": [
    "## 1) Get a list of file names and folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc51732f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NAMES:  ['check_axes_assignmentPLOT', 'compare_condi', 'confidence_interval', 'create_labels_and_initial_feature', 'cut_initial_trials', 'datadriven_FRT_vs_expmatFRT', 'detectFB_LR_trials', 'detectUD_trials', 'detectUD_trials_cabindetect', 'detect_bad_trials_rot', 'detect_bad_trials_trans', 'detect_jumps_in_data', 'detect_jumps_in_index_vector', 'detect_jumps_in_index_vector_simple', 'detect_nonconsecutive_values', 'detect_sig_change_wrt_baseline', 'detect_vertically_short_FBLR', 'equalized_signal_len', 'explode_without_colnames1', 'explode_without_colnames2', 'filter_sig3axes', 'findall', 'find_signchange_w_window', 'freqresp_functions', 'freq_from_sig_freqresp', 'freq_from_sig_timecounting', 'full_sig_2_cell', 'generate_joy_move_sign', 'index_vector', 'interpretation_of_kstest', 'main_preprocessing_steps', 'make_a_properlist', 'make_new_vec_w_index_vec', 'my_dropna_python', 'normal_distribution_feature_data', 'numderiv', 'pad_data_2makeclasses_equivalent', 'plot_count2subplot', 'plot_count2subplot2', 'plot_TR2subplot', 'process_index_for_FBLR_trials_timedetect', 'process_index_for_trials', 'process_index_for_UD_trials_timedetect', 'saveSSQ', 'scale_feature_data', 'scikit_functions', 'scikit_functions_binaryclass', 'semi_automated_gen', 'size', 'standarization_cabanaORnexttr', 'standarization_check_if_joy_moved', 'standarization_fill_in_matrix', 'standarization_notebadtrials', 'standarization_plotting', 'two_sample_stats', 'value_counts_vector', 'vertshift_segments_of_data_wrt_prevsegment', 'window_3axesDATA']\n",
      "FOLDER NAMES:  []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "path = \"C:\\\\Users\"\n",
    "\n",
    "# Get file names \n",
    "files = os.listdir(path)\n",
    "all_fn = []\n",
    "for f in files:\n",
    "    fn = f.partition('.')[0]\n",
    "    \n",
    "    if fn[0:2] != '__':\n",
    "        all_fn.append(fn)\n",
    "print('FILE NAMES: ', all_fn)\n",
    "\n",
    "\n",
    "# Get folder names\n",
    "dir_names = []\n",
    "c = 0\n",
    "for root, directories, files in os.walk(path):\n",
    "    \n",
    "    if c == 0:\n",
    "        root_out = root\n",
    "    for name in directories:\n",
    "        dn = os.path.join(name)\n",
    "        if dn[0:2] != '__':\n",
    "            dir_names.append(dn)\n",
    "    c =+ 1\n",
    "print('FOLDER NAMES: ', dir_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df129818",
   "metadata": {},
   "source": [
    "## 2) Load subfunctions that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5352b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_properlist(vec):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    out = []\n",
    "    for i in range(len(vec)):\n",
    "        out = out + [np.ravel(vec[i])]\n",
    "    vecout = np.concatenate(out).ravel().tolist()\n",
    "    \n",
    "    return vecout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c1d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # -------------------------------------\n",
    "    # Creating word tokens (recognizing each word separately)\n",
    "    # -------------------------------------\n",
    "    # 1. Put the text into string format\n",
    "    Content = \"\"\n",
    "    for t in text:\n",
    "        Content = Content + t.replace(\"'\",'')\n",
    "\n",
    "    # 2. Tokenize first to get each character separate\n",
    "    tok = nltk.word_tokenize(Content)\n",
    "    #print('length of tok: ' + str(len(tok)))\n",
    "    \n",
    "    # 3. Remove undesireable words from MY OWN stopword list\n",
    "    word_tokens1 = remove_stopwords(tok)\n",
    "    \n",
    "    return word_tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b4aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(wordtokens):\n",
    "    \n",
    "    # Put words that are 4 characters long or more, like a name, location, etc that you do not want to process\n",
    "    list_to_remove = [\"\\n\"]\n",
    "    \n",
    "    # first let's do a marker method\n",
    "    marker_vec = np.zeros((len(wordtokens), 1))\n",
    "\n",
    "    # search for the remove tokens in tok, an put a 1 in the marker_vec\n",
    "    for i in range(len(wordtokens)):\n",
    "        for j in range(len(list_to_remove)):\n",
    "            if wordtokens[i] == list_to_remove[j]:\n",
    "                marker_vec[i] = 1\n",
    "\n",
    "    word_tokens0 = []\n",
    "    for i in range(len(marker_vec)):\n",
    "        if (marker_vec[i] == 0) & (len(wordtokens[i]) > 1): # this will remove tokens that are 3 characters or less \n",
    "            word_tokens0.append(wordtokens[i])\n",
    "    \n",
    "    return word_tokens0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3eb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count_uniquewords(word_tokens):\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Process word tokens\n",
    "    # -------------------------------------\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 1. Count word tokens and get a unique list of words : count how many times a word appears\n",
    "    # Get the document-term frequency array: you have to do this first because it links cvtext to vectorizer\n",
    "    X = vectorizer.fit_transform(word_tokens)\n",
    "    word_count0 = np.ravel(np.sum(X, axis=0)) # sum vertically\n",
    "    \n",
    "    # Get document-term frequency list : returns unique words in the document that are mentioned at least once\n",
    "    unique_words0 = np.ravel(vectorizer.get_feature_names())\n",
    "    # -------------------------------------\n",
    "    # 3. Remove undesireable words AGAIN and adjust the unique_words and word_count vectors\n",
    "    list_to_remove = [\"\"]\n",
    "    \n",
    "    # first let's do a marker method\n",
    "    marker_vec = np.zeros((len(unique_words0), 1))\n",
    "\n",
    "    # search for the remove tokens in tok, an put a 1 in the marker_vec\n",
    "    for i in range(len(unique_words0)):\n",
    "        for j in range(len(list_to_remove)):\n",
    "            if unique_words0[i] == list_to_remove[j]:\n",
    "                marker_vec[i] = 1\n",
    "    \n",
    "    unique_words = []\n",
    "    word_count = []\n",
    "    for i in range(len(marker_vec)):\n",
    "        if (marker_vec[i] == 0) & (len(unique_words0[i]) > 4):\n",
    "            unique_words.append(unique_words0[i])\n",
    "            word_count.append(word_count0[i])\n",
    "    \n",
    "    m = len(np.ravel(word_count))\n",
    "    # -------------------------------------\n",
    "    \n",
    "    # Matrix of unique words and how many times they appear\n",
    "    mat = np.concatenate([np.reshape(np.ravel(word_count), (m,1)), np.reshape(unique_words, (m,1))], axis=1)\n",
    "\n",
    "    print('There are ' + str(len(word_tokens)) + ' word tokens, but ' + str(len(unique_words)) + ' words are unique.')\n",
    "\n",
    "    sort_index = np.argsort(word_count)\n",
    "    \n",
    "    A = np.array(sort_index.T)\n",
    "\n",
    "    # But we want the index of unique_word_count sorted max to min\n",
    "    Ainvert = A[::-1]\n",
    "    \n",
    "    # Convert the array to a list : this is a list where each entry is a list\n",
    "    Ainv_list = []\n",
    "    for i in range(len(Ainvert)):\n",
    "        Ainv_list.append(Ainvert[i])\n",
    "        \n",
    "    # Top num_of_words counted words in document : cvkeywords\n",
    "    keywords = []\n",
    "    wc = []\n",
    "    p = np.ravel(word_count)\n",
    "    \n",
    "    top_words = len(Ainv_list)  # 20\n",
    "    for i in range(top_words):\n",
    "        keywords.append(unique_words[Ainv_list[i]])\n",
    "        wc.append(p[Ainv_list[i]])\n",
    "    \n",
    "    # Matrix of unique words and how many times they appear\n",
    "    mat_sort = np.concatenate([np.reshape(np.ravel(wc), (top_words,1)), np.reshape(np.ravel(keywords), (top_words,1))], axis=1)\n",
    "    print(mat_sort)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return wc, keywords, mat_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1b5ca",
   "metadata": {},
   "source": [
    "##  3) Figure out which files reference other files\n",
    "Open each file and read the first 50 lines of text to obtain python reference information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb51187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD_filelist:  ['detect_jumps_in_data']\n"
     ]
    }
   ],
   "source": [
    "num_of_lines2read = 50\n",
    "\n",
    "ref_filelist = []\n",
    "HEAD_filelist = []\n",
    "\n",
    "for i in all_fn:\n",
    "    #print('i: ', i)\n",
    "    filename = '%s\\\\%s.py' % (root_out,i)\n",
    "    #print(\"filename: \", filename)\n",
    "    with open(filename, encoding='utf8', errors=\"surrogateescape\") as cvinfo:\n",
    "        cvtext = cvinfo.readlines()  \n",
    "         \n",
    "    word_tokensCV = preprocessing(cvtext)\n",
    "    \n",
    "    # ------------------------------\n",
    "    # find files that should not be in the folder\n",
    "    # Physical files that are not referenced - are either main files OR not needed\n",
    "    # ------------------------------\n",
    "    # get referenced files\n",
    "    c = num_of_lines2read  # we only want to search the first several lines of document \n",
    "    d = 0 # flag for from to import\n",
    "    ref_filename = []\n",
    "    for j in word_tokensCV[0:c]:\n",
    "        if j == 'from' and d == 0:\n",
    "            #desireable text started : reference to handmade functions\n",
    "            d = 1\n",
    "        elif j != 'import' and d == 1:\n",
    "            ref_filename.append(j)\n",
    "        elif j == 'import' and d == 1:\n",
    "            #stop desirable text\n",
    "            d = 0\n",
    "        elif j == 'HEAD':\n",
    "            HEAD_filelist.append(i)\n",
    "    \n",
    "    ref_filelist.append(ref_filename)\n",
    "\n",
    "print('HEAD_filelist: ', HEAD_filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c8bb5",
   "metadata": {},
   "source": [
    "### Solution 1: \n",
    "Look at the HEAD_filelist to know which files need to be corrected using git commandline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffac3e",
   "metadata": {},
   "source": [
    "## 4) Tokenize the reference list\n",
    "Parce the reference list more so that we can compare this list to the file name list, to determine which files in the folder have been referenced.  If the file has been referenced, it is a necessary file for the project.  If the file has not been referenced, it is a MAIN file or it is an UNNEEDED file that you need to manually decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e3aca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 285 word tokens, but 62 words are unique.\n",
      "[['75' 'subfunctions']\n",
      " ['21' 'make_a_properlist']\n",
      " ['14' 'plotly']\n",
      " ['14' 'subplots']\n",
      " ['9' 'scipy']\n",
      " ['8' 'statistics']\n",
      " ['7' 'findall']\n",
      " ['6' 'detect_sig_change_wrt_baseline']\n",
      " ['3' 'signal']\n",
      " ['3' 'detect_jumps_in_data']\n",
      " ['3' 'itertools']\n",
      " ['3' 'numderiv']\n",
      " ['3' 'preprocessing']\n",
      " ['3' 'joystick']\n",
      " ['3' 'standarization_check_if_joy_moved']\n",
      " ['3' 'compare_condi']\n",
      " ['3' 'two_sample_stats']\n",
      " ['3' 'vector']\n",
      " ['2' 'detect_jumps_in_index_vector']\n",
      " ['2' 'confidence_interval']\n",
      " ['2' 'collections']\n",
      " ['2' 'first']\n",
      " ['2' 'cabin']\n",
      " ['2' 'full_sig_2_cell']\n",
      " ['2' 'index']\n",
      " ['2' 'indices']\n",
      " ['2' 'alone']\n",
      " ['2' 'interpretation_of_kstest']\n",
      " ['2' 'almost']\n",
      " ['2' 'semi_automated_gen']\n",
      " ['2' 'vertshift_segments_of_data_wrt_prevsegment']\n",
      " ['2' 'standarization_plotting']\n",
      " ['2' 'standarization_fill_in_matrix']\n",
      " ['2' 'sklearn']\n",
      " ['1' 'mlxtend']\n",
      " ['1' 'textwrap']\n",
      " ['1' 'certain']\n",
      " ['1' 'check_axes_assignmentplot']\n",
      " ['1' 'stimuli']\n",
      " ['1' 'constant']\n",
      " ['1' 'correct']\n",
      " ['1' 'cut_initial_trials']\n",
      " ['1' 'delay']\n",
      " ['1' 'detect_bad_trials_rot']\n",
      " ['1' 'detect_bad_trials_trans']\n",
      " ['1' 'detect_nonconsecutive_values']\n",
      " ['1' 'movement']\n",
      " ['1' 'return']\n",
      " ['1' 'detect_vertically_short_fblr']\n",
      " ['1' 'directly']\n",
      " ['1' 'find_signchange_w_window']\n",
      " ['1' 'process_index_for_ud_trials_timedetect']\n",
      " ['1' 'process_index_for_trials']\n",
      " ['1' 'freqresp_functions']\n",
      " ['1' 'position']\n",
      " ['1' 'given']\n",
      " ['1' 'number']\n",
      " ['1' 'moves']\n",
      " ['1' 'initial']\n",
      " ['1' 'interpolate']\n",
      " ['1' 'window_3axesdata']\n",
      " ['1' 'without']]\n"
     ]
    }
   ],
   "source": [
    "out = make_a_properlist(ref_filelist) # make arrays of strings a list of strings \n",
    "out2 = [i.replace(\".\",' ') for i in out]\n",
    "out3 = [i.split() for i in out2]  # tokenize strings with spaces in between them\n",
    "out4 = make_a_properlist(out3)\n",
    "#print(out4)\n",
    "\n",
    "wc, keywords, mat_sort = get_word_count_uniquewords(out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "773f75eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-referenced files : 31\n",
      "['create_labels_and_initial_feature', 'datadriven_frt_vs_expmatfrt', 'detectfb_lr_trials', 'detectud_trials', 'detectud_trials_cabindetect', 'detect_jumps_in_index_vector_simple', 'equalized_signal_len', 'explode_without_colnames1', 'explode_without_colnames2', 'filter_sig3axes', 'freq_from_sig_freqresp', 'freq_from_sig_timecounting', 'generate_joy_move_sign', 'index_vector', 'main_preprocessing_steps', 'make_new_vec_w_index_vec', 'my_dropna_python', 'normal_distribution_feature_data', 'pad_data_2makeclasses_equivalent', 'plot_count2subplot', 'plot_count2subplot2', 'plot_tr2subplot', 'process_index_for_fblr_trials_timedetect', 'savessq', 'scale_feature_data', 'scikit_functions', 'scikit_functions_binaryclass', 'size', 'standarization_cabanaornexttr', 'standarization_notebadtrials', 'value_counts_vector']\n"
     ]
    }
   ],
   "source": [
    "all_fn_lower = [i.lower() for i in all_fn]\n",
    "\n",
    "reff = {}  # initialization\n",
    "for i in all_fn_lower:\n",
    "    reff[i] = 0\n",
    "\n",
    "for i in all_fn_lower:\n",
    "    if i in keywords:\n",
    "        reff[i] = reff[i] + 1\n",
    "\n",
    "# List the non-referenced files\n",
    "non_reff_files = []\n",
    "for x in range(len(reff)):\n",
    "    if reff[all_fn_lower[x]] == 0:\n",
    "        non_reff_files.append(all_fn_lower[x])\n",
    "print('Number of non-referenced files :', len(non_reff_files))\n",
    "print(non_reff_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad0b7c",
   "metadata": {},
   "source": [
    "### Solution 2:\n",
    "So there are 31 files that are not referenced by other files in the common folder.  Thus, these files are either main functions or they have nothing to do with the project and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31aaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
