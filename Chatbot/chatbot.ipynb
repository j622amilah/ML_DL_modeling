{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc985cc",
   "metadata": {},
   "source": [
    "# Chatbot : webscraping & human interaction\n",
    "\n",
    "As someone in Data Science with experience in human-in-the-loop and real-time human measures, designing and programming a human interaction chatbot sounded interesting.  Without following a known chatbot structure, I tried to be creative and come up with my own logical structure.  \n",
    "\n",
    "I looked at a few medium blogs, and I tried to first make a \"Self-learning Retrieval Based chatbot\" over a Rule-based chatbot or Self-learning Generative chatbot.\n",
    "\n",
    "As you can see in the image, I thought of using an architecture similar to query (q), key (k), value (v) that is used in Question & Answer deep learning transformers for the \"searching Knowledge logic\".  In this practice session, I only did the \"When\" architecture; I looked for sentences related to information that would contain \"when\" information like numbers, dates, months, and time.  I think this simplistic architecture could work extremely well, with a Q&A transformer architecture analyzing the found sentences instead of cos-sine similarity.\n",
    "\n",
    "Check out github for the supporting python subfunctions : https://github.com/j622amilah/Chatbot!\n",
    "\n",
    "<img src=\"chatbot.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147c4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a24878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Personal python functions\n",
    "import sys\n",
    "sys.path.insert(1, 'C:\\\\Users\\\\jamilah\\\\Documents\\\\Subfunctions_python')\n",
    "\n",
    "from findall import *\n",
    "from dictionary.sort_dict_by_value import *\n",
    "from string_text_processing.text_url_2_senANDwordtokens import *\n",
    "from string_text_processing.preprocessing import *\n",
    "from string_text_processing.get_word_count_uniquewords import *\n",
    "from string_text_processing.remove_chars_from_wordtoken import *\n",
    "from string_text_processing.get_cossine_similarity import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f20c3",
   "metadata": {},
   "source": [
    "# Get Knowlege base for chatbot by Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a347707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 209 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://en.wikipedia.org/wiki/Chatbot\"\n",
    "name_txtfile = 'chatbot_wikipedia'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9000b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 82 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://en.wikipedia.org/wiki/Q%26A_software\"\n",
    "name_txtfile = 'QampA_wikipedia'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7d8b495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1493 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://preply.com/en/blog/22-useful-english-greetings-for-every-day/\"\n",
    "name_txtfile = 'how2sayHello'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23599999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1591 sentences\n"
     ]
    }
   ],
   "source": [
    "inputurl = \"https://regardsurlefrancais.com/2017/09/19/les-salutations-en-francais/\"\n",
    "name_txtfile = 'commentDitBonjour'\n",
    "sen, word_tok = text_url_2_senANDwordtokens(inputurl, name_txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b3ff0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'QampA',\n",
       " 'software',\n",
       " 'is',\n",
       " 'software>online',\n",
       " 'that',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'answer',\n",
       " 'questions',\n",
       " 'asked',\n",
       " 'by',\n",
       " 'users',\n",
       " 'QampA',\n",
       " 'stands',\n",
       " 'for',\n",
       " 'question',\n",
       " 'and']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bcbae",
   "metadata": {},
   "source": [
    "# Retrieval Based chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3d0b9",
   "metadata": {},
   "source": [
    "## Logic and Parser Rules\n",
    "\n",
    "Give the chatbot text, and it will behave based on the text.\n",
    "If you want it to say 'greetings', give it text on 'greetings' and 'manners'.\n",
    "\n",
    "Need to keep the texts separate maybe, based on text A it would say AAA, and based on text B it would say BBB.  Then decide if AAA or BBB is better, based on similarity maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b34f73",
   "metadata": {},
   "source": [
    "## The chatbot tells us what it knows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d0379c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2570 word tokens, but 651 words are unique.\n",
      "[['128' 'chatbot']\n",
      " ['34' 'health']\n",
      " ['30' 'servic']\n",
      " ...\n",
      " ['1' 'classif']\n",
      " ['1' 'classifi']\n",
      " ['1' 'impact']]\n",
      "Hello, I am a chatbot and I can talk about the following topics :  ['chatbot', 'health', 'servic', 'custom', 'convers', 'compani', 'human', 'provid', 'gener', 'inform']\n"
     ]
    }
   ],
   "source": [
    "# Get the theme of the knowledge base\n",
    "word_tokens2 = preprocessing(word_tok)\n",
    "\n",
    "list_to_remove = ['b', \"their\", \"based\", \"which\", 'would', 'https']\n",
    "wc, keywords, mat_sort = get_word_count_uniquewords(word_tokens2, list_to_remove)\n",
    "\n",
    "print(\"Hello, I am a chatbot and I can talk about the following topics : \", keywords[0:10])\n",
    "#print(\"Top 10 wc : \", wc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca8022",
   "metadata": {},
   "source": [
    "## Let's ask the chatbot a question about the topics it knows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d5719aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a question to the chatbot: when were chatbots first made\n",
      "when were chatbots first made\n"
     ]
    }
   ],
   "source": [
    "# Get user input about the topics it can talk about\n",
    "inp = input(\"Type a question to the chatbot: \")\n",
    " \n",
    "# prints inp\n",
    "print(inp)\n",
    "inp = 'when were chatbots first made'\n",
    "#inp = 'when did chatbots become popular'\n",
    "#inp = 'when was qampa first made'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08b9b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_wt0:  ['when', 'were', 'chatbots', 'first', 'made']\n",
      "inp_wt:  ['when', 'were', 'chatbot', 'first', 'made']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['when']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which sentences are relate to the main themes and {who, what, when, where, why}\n",
    "\n",
    "# who = name, he, she, Mr, Mrs, Ms\n",
    "# what = keywords\n",
    "# when = number!  ---> search for months, dates, time, year, when was\n",
    "# where = location\n",
    "# why = because, due to the fact\n",
    "\n",
    "# verbs ?? = action\n",
    "\n",
    "# word token input\n",
    "inp_wt0 = inp.split()\n",
    "print('inp_wt0: ', inp_wt0)\n",
    "\n",
    "# Stem the input text\n",
    "ps = PorterStemmer()\n",
    "inp_wt = []\n",
    "for w in inp_wt0:\n",
    "    inp_wt.append(ps.stem(w))\n",
    "print('inp_wt: ', inp_wt)\n",
    "\n",
    "# Simple framework for setting up a Q&A transformer model: Retrival of key sentences in the knowledge data source\n",
    "q_type = []\n",
    "for i in inp_wt:\n",
    "    if i in ['who', 'name of']:\n",
    "        q_type.append('who')\n",
    "        # Need a function that detects names\n",
    "        list_to_search = ['he', 'she', 'they', 'Mr', 'Mrs', 'Ms']\n",
    "        # Search knowledge for sentences that contain names, pronouns (he, she, they, Mr, Mrs, Ms)\n",
    "    elif i in ['what']:\n",
    "        q_type.append('what')\n",
    "        # Search knowledge for sentences that contain knowledge keywords similar to user keywords\n",
    "    elif i in ['when']:\n",
    "        q_type.append('when')  # ONLY DOING 'when' for the moment\n",
    "    elif i in ['where']:\n",
    "        q_type.append('where')\n",
    "    elif i in ['why']:\n",
    "        q_type.append('why')\n",
    "q_type        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81747cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sen_with_nums:  [1, 3, 5, 26, 30, 31, 35, 51, 53, 62, 63, 64, 65, 66, 68, 74, 75, 76, 77, 79, 80, 82, 83, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 130, 131, 134, 143, 177, 180, 208]\n",
      "sen_with_timetext:  [63, 65, 66, 68, 80, 83, 118, 119, 120, 122, 208]\n",
      "relv_sen_unique:  [  1   3   5  26  30  31  35  51  53  62  63  64  65  66  68  74  75  76\n",
      "  77  79  80  82  83 109 110 112 113 114 115 116 117 118 119 120 122 130\n",
      " 131 134 143 177 180 208]\n"
     ]
    }
   ],
   "source": [
    "# Based on the Question Type: find the relavent sentences in the Knowledge Source that could Answer the Question Type\n",
    "for i in q_type:\n",
    "    \n",
    "    if i == 'when':\n",
    "        # -----------------------------------------\n",
    "        # What to do if \"when\" is in the input:\n",
    "        # -----------------------------------------\n",
    "        out = []\n",
    "        for i in word_tok:\n",
    "            # Search to see if there are '.' at the end of the word\n",
    "            i = remove_chars_from_wordtoken(i, '.', '')\n",
    "            i = remove_chars_from_wordtoken(i, '%', '')\n",
    "            i = remove_chars_from_wordtoken(i, '$', '')\n",
    "\n",
    "            # First determine if a number is a ratio\n",
    "            i = remove_chars_from_wordtoken(i, '/', ' divided by ')\n",
    "\n",
    "            # Second determine if a number is present\n",
    "            if i.upper() == i.lower() and i != '':\n",
    "                # is a number\n",
    "                out.append(int(i))\n",
    "\n",
    "        imp_nums = np.unique(out)\n",
    "        imp_nums_str = [str(i) for i in imp_nums]\n",
    "        \n",
    "        # Search knowledge for sentences that contain numbers and text referring to time \n",
    "        # -----------------------------------------\n",
    "        # 1) Get the sentences that contain numbers\n",
    "        sen_with_nums = []\n",
    "        for ind, i in enumerate(sen):\n",
    "            # Returns the unique values in 1st array NOT in 2nd array\n",
    "            vals_from_long_not_in_short = np.setdiff1d(imp_nums_str, i)\n",
    "            out = np.setdiff1d(imp_nums_str, vals_from_long_not_in_short)\n",
    "            if any(out):\n",
    "                sen_with_nums.append(ind)\n",
    "        # -----------------------------------------\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # 2) Get the sentences \n",
    "        list_to_search = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December', 'year', 'month', 'week', 'day', 'hour', 'minute', 'second', 'ms', 'millisecond']\n",
    "        sen_with_timetext = []\n",
    "        for ind, i in enumerate(sen):\n",
    "            # Returns the unique values in 1st array NOT in 2nd array\n",
    "            vals_from_long_not_in_short = np.setdiff1d(list_to_search, i)\n",
    "            out = np.setdiff1d(list_to_search, vals_from_long_not_in_short)\n",
    "            if any(out):\n",
    "                sen_with_timetext.append(ind)\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        \n",
    "print('sen_with_nums: ', sen_with_nums)\n",
    "print('sen_with_timetext: ', sen_with_timetext)\n",
    "\n",
    "# Combine 'relevant sentence' lists\n",
    "sen_with_nums += sen_with_timetext\n",
    "relv_sen_unique = np.unique(sen_with_nums)\n",
    "print('relv_sen_unique: ', relv_sen_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13b42241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [40.0, 0.6324555320336759, 59],\n",
       " 115: [40.0, 0.6324555320336759, 29],\n",
       " 114: [40.0, 0.6324555320336759, 28],\n",
       " 119: [40.0, 0.6324555320336759, 20],\n",
       " 118: [40.0, 0.6324555320336759, 19],\n",
       " 63: [40.0, 0.6324555320336759, 17],\n",
       " 65: [40.0, 0.6324555320336759, 17],\n",
       " 3: [20.0, 0.4472135954999579, 83],\n",
       " 130: [20.0, 0.4472135954999579, 63],\n",
       " 5: [20.0, 0.4472135954999579, 34],\n",
       " 30: [20.0, 0.4472135954999579, 31],\n",
       " 35: [20.0, 0.4472135954999579, 31],\n",
       " 134: [20.0, 0.4472135954999579, 29],\n",
       " 143: [20.0, 0.4472135954999579, 29],\n",
       " 131: [20.0, 0.4472135954999579, 25],\n",
       " 117: [20.0, 0.4472135954999579, 24],\n",
       " 116: [20.0, 0.4472135954999579, 23],\n",
       " 122: [20.0, 0.4472135954999579, 16],\n",
       " 120: [20.0, 0.4472135954999579, 15],\n",
       " 26: [0.0, nan, 11],\n",
       " 31: [0.0, nan, 10],\n",
       " 51: [0.0, nan, 26],\n",
       " 53: [0.0, nan, 26],\n",
       " 62: [0.0, nan, 11],\n",
       " 64: [0.0, nan, 12],\n",
       " 66: [0.0, nan, 14],\n",
       " 68: [0.0, nan, 15],\n",
       " 74: [0.0, nan, 8],\n",
       " 75: [0.0, nan, 15],\n",
       " 76: [0.0, nan, 9],\n",
       " 77: [0.0, nan, 15],\n",
       " 79: [0.0, nan, 18],\n",
       " 80: [0.0, nan, 27],\n",
       " 82: [0.0, nan, 18],\n",
       " 83: [0.0, nan, 27],\n",
       " 109: [0.0, nan, 18],\n",
       " 110: [0.0, nan, 19],\n",
       " 112: [0.0, nan, 18],\n",
       " 113: [0.0, nan, 19],\n",
       " 177: [0.0, nan, 15],\n",
       " 180: [0.0, nan, 15],\n",
       " 208: [0.0, nan, 18]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for common words in the input with each of 'relevant sentences'\n",
    "cos_sim_all = get_cossine_similarity(inp_wt, relv_sen_unique, sen)\n",
    "cos_sim_all = make_a_properlist(cos_sim_all)\n",
    "\n",
    "# 1) Get a Percentage of how many words of 'input' are used in sen\n",
    "dict_val = {}\n",
    "for ind, rels in enumerate(relv_sen_unique):\n",
    "    tot = np.zeros((len(inp_wt),1))\n",
    "    for i in range(len(inp_wt)):\n",
    "        for j in range(len(sen[rels])):\n",
    "            if inp_wt[i] == sen[rels][j]:\n",
    "                tot[i] = 1\n",
    "                break\n",
    "\n",
    "    per = sum(np.ravel(tot))/len(inp_wt) * 100\n",
    "    \n",
    "    # Percentage and cos-sine similarity\n",
    "    dict_val[rels] = [per, cos_sim_all[ind], len(sen[rels])] \n",
    "\n",
    "# Sort by max\n",
    "dict_val_sorted = sort_dict_by_value(dict_val, reverse = True)\n",
    "dict_val_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab77d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_val ['1994']\n",
      "chatbot_output :  ['Designed', 'to', 'convincingly', 'simulate', 'the', 'way', 'a', 'human', 'would', 'behave', 'as', 'a', 'conversational', 'partner', 'chatbot', 'systems', 'typically', 'require', 'continuous', 'tuning', 'and', 'testing', 'and', 'many', 'in', 'production', 'remain', 'unable', 'to', 'adequately', 'converse', 'while', 'none', 'of', 'them', 'can', 'pass', 'the', 'standard', 'test>Turing', 'The', 'term', 'ChatterBot', 'was', 'originally', 'coined', 'by', 'Loren', 'Mauldin>Michael', 'creator', 'of', 'the', 'first', 'in', '1994', 'to', 'describe', 'these', 'conversational']\n"
     ]
    }
   ],
   "source": [
    "# So the Longest sentence WITH the Highest similarity appears to be the best response!!\n",
    "chatbot_output = sen[list(dict_val_sorted.keys())[0]]\n",
    "\n",
    "\n",
    "vals_from_long_not_in_short = np.setdiff1d(imp_nums_str, chatbot_output)\n",
    "out = np.setdiff1d(imp_nums_str, vals_from_long_not_in_short)\n",
    "if any(out):\n",
    "    print('number_val', out)\n",
    "\n",
    "print('chatbot_output : ', chatbot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1940a",
   "metadata": {},
   "source": [
    "I asked some simple questions, so I think I got Luckly!\n",
    "\n",
    "I think it could be improved with a Q&A transformer step with the 'relavant sentences'!  And, I was a little lazy about separating the sentences perfectly using the webscraped words.  If the sentences were perfectly separated, it would be interesting to see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ffcff216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
