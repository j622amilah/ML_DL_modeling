{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compare your CV to a job post\n",
    "Determine if your CV has the correct keywords needed to get the job that you want.  This project was inspired by https://towardsdatascience.com/ai-is-working-against-your-job-application-bec65d496d22 .  It is possible that your CV may not be even looked at by a real person for the first selection process, the resumes could be choosen by a computer program that is looking for key text related to the available position.  In this case, it is important to have both the correct keywords for the computer algorithm and have your CV look aesthetically pleasing to a human.  ^_^'\n",
    "\n",
    "This notebook reads in a .pdf or .txt CV, and a job post.  And compares the word, giving a similarity score for all the words and/or the most frequent words used in each document.\n",
    "\n",
    "<img src=\"neutral_CV.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib   # Needed for reading in both your CV and jobpost\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords') # You only have to do this once\n",
    "#nltk.download('punkt')       # only run one time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np      # linear algebra numpy library\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "def size(vec):\n",
    "    rvec = []\n",
    "    cvec = []\n",
    "    coutr = 0\n",
    "    \n",
    "    for r in vec:\n",
    "        rvec = rvec + [int(1)]\n",
    "\n",
    "        coutr += 1\n",
    "        coutc = 0\n",
    "    \n",
    "        for col in r:\n",
    "            coutc += 1\n",
    "        \n",
    "        cvec = cvec + [coutc]\n",
    "    \n",
    "    row = coutr\n",
    "    col = max(cvec)\n",
    "    result = namedtuple('result', 'row00 col00')\n",
    "    return result(row00=row, col00=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(wordtokens):\n",
    "    \n",
    "    # Put words that are 4 characters long or more, like a name, location, etc that you do not want to process\n",
    "    list_to_remove = [\"gmail\", \"gmail.com\", \"https\"]\n",
    "    \n",
    "    # first let's do a marker method\n",
    "    marker_vec = np.zeros((len(wordtokens), 1))\n",
    "\n",
    "    # search for the remove tokens in tok, an put a 1 in the marker_vec\n",
    "    for i in range(len(wordtokens)):\n",
    "        for j in range(len(list_to_remove)):\n",
    "            if wordtokens[i] == list_to_remove[j]:\n",
    "                marker_vec[i] = 1\n",
    "\n",
    "    word_tokens0 = []\n",
    "    for i in range(len(marker_vec)):\n",
    "        if (marker_vec[i] == 0) & (len(wordtokens[i]) > 4): # this will remove tokens that are 3 characters or less \n",
    "            word_tokens0.append(wordtokens[i])\n",
    "            \n",
    "    # 4. Removing stopwords using sklearn              \n",
    "    stop_words = set(stopwords.words('english')) # does not remove \"and\" or \"or\"\n",
    "    word_tokens1 = []\n",
    "    for w in word_tokens0: \n",
    "        if w not in stop_words: \n",
    "            word_tokens1.append(w)\n",
    "    \n",
    "    return word_tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # -------------------------------------\n",
    "    # Creating word tokens (recognizing each word separately)\n",
    "    # -------------------------------------\n",
    "    # 1. Put the text into string format\n",
    "    Content = \"\"\n",
    "    for t in text:\n",
    "        Content = Content + t.lower().replace(\"'\",'')\n",
    "\n",
    "    # 2. Tokenize first to get each character separate\n",
    "    tok = nltk.word_tokenize(Content)\n",
    "    print('length of tok: ' + str(len(tok)))\n",
    "    \n",
    "    # 3. Remove undesireable words from MY OWN stopword list\n",
    "    word_tokens1 = remove_stopwords(tok)\n",
    "    \n",
    "    # 5. Combining word stems \n",
    "    ps = PorterStemmer()\n",
    "    word_tokens2 = []\n",
    "    for w in word_tokens1:\n",
    "        word_tokens2.append(ps.stem(w))\n",
    "    \n",
    "    return word_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count_uniquewords(word_tokens):\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Process word tokens\n",
    "    # -------------------------------------\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 1. Count word tokens and get a unique list of words : count how many times a word appears\n",
    "    # Get the document-term frequency array: you have to do this first because it links cvtext to vectorizer\n",
    "    X = vectorizer.fit_transform(word_tokens)\n",
    "    word_count0 = np.ravel(np.sum(X, axis=0)) # sum vertically\n",
    "    \n",
    "    # Get document-term frequency list : returns unique words in the document that are mentioned at least once\n",
    "    unique_words0 = np.ravel(vectorizer.get_feature_names())\n",
    "    # -------------------------------------\n",
    "    # 3. Remove undesireable words AGAIN and adjust the unique_words and word_count vectors\n",
    "    list_to_remove = [\"sallyjon\", \"zendesk\", \"rosevil\", \"sacramento\"]\n",
    "    \n",
    "    # first let's do a marker method\n",
    "    marker_vec = np.zeros((len(unique_words0), 1))\n",
    "\n",
    "    # search for the remove tokens in tok, an put a 1 in the marker_vec\n",
    "    for i in range(len(unique_words0)):\n",
    "        for j in range(len(list_to_remove)):\n",
    "            if unique_words0[i] == list_to_remove[j]:\n",
    "                marker_vec[i] = 1\n",
    "    \n",
    "    unique_words = []\n",
    "    word_count = []\n",
    "    for i in range(len(marker_vec)):\n",
    "        if (marker_vec[i] == 0) & (len(unique_words0[i]) > 4):\n",
    "            unique_words.append(unique_words0[i])\n",
    "            word_count.append(word_count0[i])\n",
    "    \n",
    "    m = len(np.ravel(word_count))\n",
    "    # -------------------------------------\n",
    "    \n",
    "    # Matrix of unique words and how many times they appear\n",
    "    mat = np.concatenate([np.reshape(np.ravel(word_count), (m,1)), np.reshape(unique_words, (m,1))], axis=1)\n",
    "    #print(mat)\n",
    "\n",
    "    print('There are ' + str(len(word_tokens)) + ' word tokens, but ' + str(len(unique_words)) + ' words are unique.')\n",
    "\n",
    "    # 2. (Option) sort the unique_words by the word_count such that most frequent words are 1st\n",
    "    # Gives the index of unique_word_count sorted from min to max\n",
    "    sort_index = np.argsort(word_count)\n",
    "    \n",
    "    # Convert from matrix to array, so we can manipulate the entries\n",
    "    # Puts the response vector in an proper array vector\n",
    "    A = np.array(sort_index.T)\n",
    "\n",
    "    # But we want the index of unique_word_count sorted max to min\n",
    "    Ainvert = A[::-1]\n",
    "    \n",
    "    # Convert the array to a list : this is a list where each entry is a list\n",
    "    Ainv_list = []\n",
    "    for i in range(len(Ainvert)):\n",
    "        Ainv_list.append(Ainvert[i])\n",
    "        \n",
    "    # Top num_of_words counted words in document : cvkeywords\n",
    "    keywords = []\n",
    "    wc = []\n",
    "    p = np.ravel(word_count)\n",
    "    \n",
    "    #print('Ainv_list' + str(Ainv_list))\n",
    "    \n",
    "    top_words = len(Ainv_list)  # 20\n",
    "    for i in range(top_words):\n",
    "        keywords.append(unique_words[Ainv_list[i]])\n",
    "        wc.append(p[Ainv_list[i]])\n",
    "    \n",
    "    # Matrix of unique words and how many times they appear\n",
    "    mat_sort = np.concatenate([np.reshape(np.ravel(wc), (top_words,1)), np.reshape(np.ravel(keywords), (top_words,1))], axis=1)\n",
    "    print(mat_sort)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    return wc, keywords, mat_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CV file : C:\\Users\\HP EliteBook\\Documents\\Sourceforge_PROJECTS\\Resume_compare_python\\CV.txt\n",
      "length of tok: 368\n",
      "There are 193 word tokens, but 98 words are unique.\n",
      "[['19' 'custom']\n",
      " ['11' 'servic']\n",
      " ['5' 'experi']\n",
      " ['3' 'account']\n",
      " ['3' 'manag']\n",
      " ['3' 'employe']\n",
      " ['3' 'softwar']\n",
      " ['2' 'month']\n",
      " ['2' 'conduct']\n",
      " ['2' 'product']\n",
      " ['2' 'direct']\n",
      " ['2' 'complaint']\n",
      " ['2' 'inform']\n",
      " ['2' 'posit']\n",
      " ['2' 'return']\n",
      " ['2' 'satisfact']\n",
      " ['2' 'septemb']\n",
      " ['2' 'maintain']\n",
      " ['2' 'skill']\n",
      " ['2' 'excel']\n",
      " ['2' 'specialist']\n",
      " ['2' 'kayako']\n",
      " ['2' 'averag']\n",
      " ['2' 'friendli']\n",
      " ['1' 'futur']\n",
      " ['1' 'higher']\n",
      " ['1' 'desir']\n",
      " ['1' 'handl']\n",
      " ['1' 'daili']\n",
      " ['1' 'field']\n",
      " ['1' 'highli']\n",
      " ['1' 'disposit']\n",
      " ['1' 'employ']\n",
      " ['1' 'discount']\n",
      " ['1' 'compani']\n",
      " ['1' 'comput']\n",
      " ['1' 'comprehens']\n",
      " ['1' 'addit']\n",
      " ['1' 'administr']\n",
      " ['1' 'almost']\n",
      " ['1' 'america']\n",
      " ['1' 'answer']\n",
      " ['1' 'assist']\n",
      " ['1' 'attain']\n",
      " ['1' 'attitud']\n",
      " ['1' 'award']\n",
      " ['1' 'bachelor']\n",
      " ['1' 'basic']\n",
      " ['1' 'bookstor']\n",
      " ['1' 'california']\n",
      " ['1' 'cancel']\n",
      " ['1' 'close']\n",
      " ['1' 'commun']\n",
      " ['1' 'complic']\n",
      " ['1' 'includ']\n",
      " ['1' 'wealth']\n",
      " ['1' 'insur']\n",
      " ['1' 'store']\n",
      " ['1' 'resolv']\n",
      " ['1' 'respond']\n",
      " ['1' 'rocki']\n",
      " ['1' 'salli']\n",
      " ['1' 'scholarship']\n",
      " ['1' 'scienc']\n",
      " ['1' 'special']\n",
      " ['1' 'state']\n",
      " ['1' 'support']\n",
      " ['1' 'januari']\n",
      " ['1' 'system']\n",
      " ['1' 'telephon']\n",
      " ['1' 'train']\n",
      " ['1' 'troubleshoot']\n",
      " ['1' 'typist']\n",
      " ['1' 'unhappi']\n",
      " ['1' 'univers']\n",
      " ['1' 'unruli']\n",
      " ['1' 'research']\n",
      " ['1' 'repres']\n",
      " ['1' 'recipi']\n",
      " ['1' 'receiv']\n",
      " ['1' 'knowledg']\n",
      " ['1' 'leader']\n",
      " ['1' 'upper']\n",
      " ['1' 'major']\n",
      " ['1' 'master']\n",
      " ['1' 'memor']\n",
      " ['1' 'microsoft']\n",
      " ['1' 'octob']\n",
      " ['1' 'offic']\n",
      " ['1' 'organ']\n",
      " ['1' 'polici']\n",
      " ['1' 'power']\n",
      " ['1' 'present']\n",
      " ['1' 'profession']\n",
      " ['1' 'progress']\n",
      " ['1' 'provid']\n",
      " ['1' 'queri']\n",
      " ['1' '95661']]\n",
      "Reading job file : C:\\Users\\HP EliteBook\\Documents\\Sourceforge_PROJECTS\\Resume_compare_python\\job_post.txt\n",
      "length of tok: 382\n",
      "There are 149 word tokens, but 91 words are unique.\n",
      "[['15' 'custom']\n",
      " ['10' 'servic']\n",
      " ['4' 'problem']\n",
      " ['4' 'inform']\n",
      " ['3' 'phone']\n",
      " ['3' 'peopl']\n",
      " ['3' 'skill']\n",
      " ['2' 'becom']\n",
      " ['2' 'cancel']\n",
      " ['2' 'think']\n",
      " ['2' 'commun']\n",
      " ['2' 'product']\n",
      " ['2' 'provid']\n",
      " ['2' 'listen']\n",
      " ['2' 'quickli']\n",
      " ['2' 'answer']\n",
      " ['2' 'requir']\n",
      " ['2' 'polici']\n",
      " ['2' 'repres']\n",
      " ['1' 'employ']\n",
      " ['1' 'essenti']\n",
      " ['1' 'eventu']\n",
      " ['1' 'youll']\n",
      " ['1' 'evolv']\n",
      " ['1' 'expect']\n",
      " ['1' 'email']\n",
      " ['1' 'fulfil']\n",
      " ['1' 'greet']\n",
      " ['1' 'guarante']\n",
      " ['1' 'identifi']\n",
      " ['1' 'includ']\n",
      " ['1' 'intent']\n",
      " ['1' 'experi']\n",
      " ['1' 'customer']\n",
      " ['1' 'diploma']\n",
      " ['1' 'complain']\n",
      " ['1' 'across']\n",
      " ['1' 'angri']\n",
      " ['1' 'associ']\n",
      " ['1' 'bachelor']\n",
      " ['1' 'center']\n",
      " ['1' 'compani']\n",
      " ['1' 'composur']\n",
      " ['1' 'descript']\n",
      " ['1' 'constantli']\n",
      " ['1' 'convers']\n",
      " ['1' 'could']\n",
      " ['1' 'crucial']\n",
      " ['1' 'degre']\n",
      " ['1' 'demeanor']\n",
      " ['1' 'learn']\n",
      " ['1' 'minimum']\n",
      " ['1' 'localwis']\n",
      " ['1' 'second']\n",
      " ['1' 'regard']\n",
      " ['1' 'request']\n",
      " ['1' 'resolv']\n",
      " ['1' 'respons']\n",
      " ['1' 'satisfact']\n",
      " ['1' 'school']\n",
      " ['1' 'script']\n",
      " ['1' 'social']\n",
      " ['1' 'question']\n",
      " ['1' 'solut']\n",
      " ['1' 'solver']\n",
      " ['1' 'stress']\n",
      " ['1' 'thank']\n",
      " ['1' 'thorough']\n",
      " ['1' 'understand']\n",
      " ['1' 'unhappi']\n",
      " ['1' 'realli']\n",
      " ['1' 'process']\n",
      " ['1' 'localwise']\n",
      " ['1' 'order']\n",
      " ['1' 'locat']\n",
      " ['1' 'manner']\n",
      " ['1' 'media']\n",
      " ['1' 'memor']\n",
      " ['1' 'volum']\n",
      " ['1' 'natur']\n",
      " ['1' 'offer']\n",
      " ['1' 'overview']\n",
      " ['1' 'pressur']\n",
      " ['1' 'patienc']\n",
      " ['1' 'perhap']\n",
      " ['1' 'person']\n",
      " ['1' 'persuad']\n",
      " ['1' 'place']\n",
      " ['1' 'pleasant']\n",
      " ['1' 'point']\n",
      " ['1' 'account']]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Analysis with a single words\n",
    "# -------------------------------------\n",
    "\n",
    "# -------------------------------------\n",
    "# Load CV document\n",
    "# -------------------------------------\n",
    "# Load the job description\n",
    "file_loc = \"C:\\\\Users\\\\HP EliteBook\\\\Documents\\\\Sourceforge_PROJECTS\\\\Resume_compare_python\\\\CV.txt\"\n",
    "print('Reading CV file : ' + file_loc)\n",
    "\n",
    "with open(file_loc, encoding='utf8', errors=\"surrogateescape\") as cvinfo:\n",
    "    cvtext = cvinfo.readlines()   #  the raw text with \\n for returns following lines\n",
    "\n",
    "# -------------------------------------\n",
    "# Creating word tokens (recognizing each word separately)\n",
    "# -------------------------------------\n",
    "word_tokensCV = preprocessing(cvtext)\n",
    "\n",
    "wc_CV, keywords_CV, mat_sort_CV = get_word_count_uniquewords(word_tokensCV)\n",
    "\n",
    "# -------------------------------------\n",
    "# Load Job post document\n",
    "# -------------------------------------\n",
    "# Load the job description\n",
    "file_loc = \"C:\\\\Users\\\\HP EliteBook\\\\Documents\\\\Sourceforge_PROJECTS\\\\Resume_compare_python\\\\job_post.txt\"\n",
    "print('Reading job file : ' + file_loc)\n",
    "\n",
    "with open(file_loc, encoding='utf8', errors=\"surrogateescape\") as job:\n",
    "    jobtext = job.readlines()   #  the raw text with \\n for returns following lines\n",
    "\n",
    "# -------------------------------------\n",
    "# Creating word tokens (recognizing each word separately)\n",
    "# -------------------------------------\n",
    "word_tokensJOB = preprocessing(jobtext)\n",
    "\n",
    "wc_JOB, keywords_JOB, mat_sort_JOB = get_word_count_uniquewords(word_tokensJOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of common words between CV and JOB: 23.076923076923077%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Construct a joint document-term frequency matrix of (CV vs job_post) words\n",
    "# -------------------------------------\n",
    "# 1) Get a Percentage of how many common words of JOB are used in CV\n",
    "# You do not care if keywords are repeated a lot, because in a CV you do not repeat words often.  A measure to see\n",
    "# if you are just using the correct words to match with the job you want.\n",
    "tot = np.zeros((len(keywords_JOB),1))\n",
    "for i in range(len(keywords_JOB)):\n",
    "    for j in range(len(keywords_CV)):\n",
    "        if keywords_JOB[i] == keywords_CV[j]:\n",
    "            tot[i] = 1\n",
    "            break\n",
    "\n",
    "per = sum(np.ravel(tot))/len(keywords_JOB) * 100\n",
    "print('Percentage of common words between CV and JOB: ' + str(per) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['custom' '15' 'custom' '19']\n",
      " ['servic' '10' 'servic' '11']\n",
      " ['problem' '4' '' '0']\n",
      " ['inform' '4' 'inform' '2']\n",
      " ['phone' '3' '' '0']\n",
      " ['peopl' '3' '' '0']\n",
      " ['skill' '3' 'skill' '2']\n",
      " ['becom' '2' '' '0']\n",
      " ['cancel' '2' 'cancel' '1']\n",
      " ['think' '2' '' '0']\n",
      " ['commun' '2' 'commun' '1']\n",
      " ['product' '2' 'product' '2']\n",
      " ['provid' '2' 'provid' '1']\n",
      " ['listen' '2' '' '0']\n",
      " ['quickli' '2' '' '0']\n",
      " ['answer' '2' 'answer' '1']\n",
      " ['requir' '2' '' '0']\n",
      " ['polici' '2' 'polici' '1']\n",
      " ['repres' '2' 'repres' '1']\n",
      " ['employ' '1' 'employ' '1']\n",
      " ['essenti' '1' '' '0']\n",
      " ['eventu' '1' '' '0']\n",
      " ['youll' '1' '' '0']\n",
      " ['evolv' '1' '' '0']\n",
      " ['expect' '1' '' '0']\n",
      " ['email' '1' '' '0']\n",
      " ['fulfil' '1' '' '0']\n",
      " ['greet' '1' '' '0']\n",
      " ['guarante' '1' '' '0']\n",
      " ['identifi' '1' '' '0']\n",
      " ['includ' '1' 'includ' '1']\n",
      " ['intent' '1' '' '0']\n",
      " ['experi' '1' 'experi' '5']\n",
      " ['customer' '1' '' '0']\n",
      " ['diploma' '1' '' '0']\n",
      " ['complain' '1' '' '0']\n",
      " ['across' '1' '' '0']\n",
      " ['angri' '1' '' '0']\n",
      " ['associ' '1' '' '0']\n",
      " ['bachelor' '1' 'bachelor' '1']\n",
      " ['center' '1' '' '0']\n",
      " ['compani' '1' 'compani' '1']\n",
      " ['composur' '1' '' '0']\n",
      " ['descript' '1' '' '0']\n",
      " ['constantli' '1' '' '0']\n",
      " ['convers' '1' '' '0']\n",
      " ['could' '1' '' '0']\n",
      " ['crucial' '1' '' '0']\n",
      " ['degre' '1' '' '0']\n",
      " ['demeanor' '1' '' '0']\n",
      " ['learn' '1' '' '0']\n",
      " ['minimum' '1' '' '0']\n",
      " ['localwis' '1' '' '0']\n",
      " ['second' '1' '' '0']\n",
      " ['regard' '1' '' '0']\n",
      " ['request' '1' '' '0']\n",
      " ['resolv' '1' 'resolv' '1']\n",
      " ['respons' '1' '' '0']\n",
      " ['satisfact' '1' 'satisfact' '2']\n",
      " ['school' '1' '' '0']\n",
      " ['script' '1' '' '0']\n",
      " ['social' '1' '' '0']\n",
      " ['question' '1' '' '0']\n",
      " ['solut' '1' '' '0']\n",
      " ['solver' '1' '' '0']\n",
      " ['stress' '1' '' '0']\n",
      " ['thank' '1' '' '0']\n",
      " ['thorough' '1' '' '0']\n",
      " ['understand' '1' '' '0']\n",
      " ['unhappi' '1' 'unhappi' '1']\n",
      " ['realli' '1' '' '0']\n",
      " ['process' '1' '' '0']\n",
      " ['localwise' '1' '' '0']\n",
      " ['order' '1' '' '0']\n",
      " ['locat' '1' '' '0']\n",
      " ['manner' '1' '' '0']\n",
      " ['media' '1' '' '0']\n",
      " ['memor' '1' 'memor' '1']\n",
      " ['volum' '1' '' '0']\n",
      " ['natur' '1' '' '0']\n",
      " ['offer' '1' '' '0']\n",
      " ['overview' '1' '' '0']\n",
      " ['pressur' '1' '' '0']\n",
      " ['patienc' '1' '' '0']\n",
      " ['perhap' '1' '' '0']\n",
      " ['person' '1' '' '0']\n",
      " ['persuad' '1' '' '0']\n",
      " ['place' '1' '' '0']\n",
      " ['pleasant' '1' '' '0']\n",
      " ['point' '1' '' '0']\n",
      " ['account' '1' 'account' '3']]\n",
      "Of the words that are in common with your CV and the JOB post, cos_sim with respect to 1 tells how similarly common words were frequently used within both documents.\n",
      "cos_sim : 0.8433517491000196\n"
     ]
    }
   ],
   "source": [
    "# 2) Similarity of common word usage (similarity)\n",
    "# Of the words that are common, how often are common words in CV and JOB are used?  \n",
    "# Use similarity measure to see how CV is worded similarly as JOB.  \n",
    "# But this does not tell you how to improve your CV.\n",
    "\n",
    "# filler is a number that is NOT in the index list and greater than the length of JOB and CV\n",
    "filler = np.maximum(len(keywords_CV), len(keywords_JOB)) + 100\n",
    "\n",
    "index_CV = filler*np.ones((len(wc_CV),1))\n",
    "for i in range(len(wc_JOB)):\n",
    "    for j in range(len(wc_CV)):\n",
    "        if keywords_JOB[i] == keywords_CV[j]:\n",
    "            index_CV[i] = j\n",
    "\n",
    "# Reshuffle index_CV\n",
    "index_CV = np.ravel(index_CV)\n",
    "# print('index_CV : ' + str(index_CV)) \n",
    "\n",
    "keywords_CV_orderofJOB = []\n",
    "wc_CV_orderofJOB = []\n",
    "for i in range(len(index_CV)):\n",
    "    if index_CV[i] == filler:\n",
    "        # keep same word and set word count to zero (because the word is not the same)\n",
    "        keywords_CV_orderofJOB = keywords_CV_orderofJOB + ['']\n",
    "        wc_CV_orderofJOB = wc_CV_orderofJOB + [0]\n",
    "    else:\n",
    "        keywords_CV_orderofJOB = keywords_CV_orderofJOB + [keywords_CV[int(index_CV[i])]]\n",
    "        wc_CV_orderofJOB = wc_CV_orderofJOB + [wc_CV[int(index_CV[i])]]\n",
    "\n",
    "# print('length of keywords_CV_orderofJOB : ' + str(len(keywords_CV_orderofJOB)))\n",
    "# print('length of keywords_JOB : ' + str(len(keywords_JOB))) \n",
    "\n",
    "minlen = np.minimum(len(keywords_CV_orderofJOB), len(keywords_JOB))\n",
    "# print('minlen : ' + str(minlen))\n",
    "\n",
    "a = wc_JOB[0:minlen]\n",
    "b = wc_CV_orderofJOB[0:minlen]\n",
    "\n",
    "a0 = np.reshape(np.ravel(keywords_JOB[0:minlen]), (minlen,1))\n",
    "a1 = np.reshape(np.ravel(a), (minlen,1))\n",
    "a2 = np.reshape(np.ravel(keywords_CV_orderofJOB[0:minlen]), (minlen,1))\n",
    "a3 = np.reshape(np.ravel(b), (minlen,1))\n",
    "mat_sort = np.concatenate([a0, a1, a2, a3], axis=1)\n",
    "print(mat_sort)\n",
    "\n",
    "# Cosine similarity of similarly aligned words using the frequency count of words\n",
    "cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "print('Of the words that are in common with your CV and the JOB post, cos_sim with respect to 1 tells how similarly common words were frequently used within both documents.')\n",
    "print('cos_sim : ' + str(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have these words on YOUR CV, but they are mentioned in the JOB post frequently :\n",
      "[word, word count percentage in JOB]\n",
      "[['problem' '26.666666666666668']\n",
      " ['phone' '20.0']\n",
      " ['peopl' '20.0']\n",
      " ['becom' '13.333333333333334']\n",
      " ['think' '13.333333333333334']\n",
      " ['listen' '13.333333333333334']\n",
      " ['quickli' '13.333333333333334']\n",
      " ['requir' '13.333333333333334']]\n",
      "You might want to add these words to your CV and your CV will be more similar to the JOB post!\n"
     ]
    }
   ],
   "source": [
    "# 3) Non-common frequently used words in Job are how you improve your CV similarity \n",
    "# MISSING KEYWORDS :\n",
    "# the non-common CV and job words that are frequently used in job, are the words you should put on your CV and \n",
    "# the number of times they are repeated in JOB is how many times you might want to put them on your CV.\n",
    "\n",
    "# Decide the number of top words in JOB to consider\n",
    "topwords = 20\n",
    "\n",
    "# Look in wc_CV_orderofJOB for words that are zero\n",
    "missing_word = []\n",
    "missing_word_count = []\n",
    "for i in range(topwords):\n",
    "    if wc_CV_orderofJOB[i] == 0:\n",
    "        missing_word = missing_word + [np.ravel(keywords_JOB[i])]\n",
    "        missing_word_count = missing_word_count + [np.ravel(wc_JOB[i])]\n",
    "\n",
    "r = len(missing_word)\n",
    "\n",
    "a4 = np.reshape(np.ravel(missing_word), (r,1))\n",
    "a5 = np.reshape((np.ravel(missing_word_count)/wc_JOB[0])*100, (r,1))\n",
    "miss_sort = np.concatenate([a4, a5], axis=1)\n",
    "print('You do not have these words on YOUR CV, but they are mentioned in the JOB post frequently :')\n",
    "print('[word, word count percentage in JOB]')\n",
    "print(miss_sort)\n",
    "print('You might want to add these words to your CV and your CV will be more similar to the JOB post!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Sally might want to put on her resume that she \"addressed PPROBLEMS by talking with PEOPLE on the PHONE\" or that she is able to \"THINK QUICKLY and is a good LISTENER\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
