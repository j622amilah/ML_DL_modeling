{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d7ebeb",
   "metadata": {},
   "source": [
    "# Databricks: platform for DevOps\n",
    "\n",
    "Lately I have been practicing spark, python,  SQL, and R on the community Databricks platform (https://community.cloud.databricks.com). Databricks has several kernels with a simple jupyter-notebook cell interface, such that you can manipulate large quantities of data to quickly exploite the best features of different programming languages. For example, if you need to manipulate a large database you can preprocess the data quickly using SQL, then save the necessary data to a 'fast retrival' table called a Delta Table. Then you can do more complicated manipulations with the 'fast table', like machine learning or deep learning modeling in python AND/OR statistical analysis in R.\n",
    "\n",
    "<img src=\"main_splash.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Some basic commands to get started on this platform are: 0) loading data, 1) saving data to a Delta Table, 2) standard pre-processing in SQL, 3) conversion of SQL table to python and R.\n",
    "Databricks even has an option for publishing the notebook online: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6791636491620955/2798567329490024/4779345166745182/latest.html. The published notebooks are only available for 6 month, so I also put the notebook on Git. Below are highlights from the notbook. Enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d40c61",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805336c",
   "metadata": {},
   "source": [
    "### Load from csv: create a pyspark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "# Example 1: File location and type\n",
    "file_location = \"/FileStore/tables/bikeshare-1.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"True\"  #\"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_pyspark = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4092dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Example 2: Below, I add onto the command and perform an initial pre-processing \n",
    "# of data directly with the loading command.\n",
    "file_location = \"/mnt/training/healthcare/tracker/moocs/daily-metrics.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"True\"  #\"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_pyspark = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location) \\\n",
    "  .select(\n",
    "        \"device_id\",\n",
    "        \"dte\",\n",
    "        col(\"resting_heartrate\").cast(DoubleType()),\n",
    "        col(\"active_heartrate\").cast(DoubleType()),\n",
    "        col(\"bmi\").cast(DoubleType()),\n",
    "        col(\"vo2\").cast(DoubleType()),\n",
    "        col(\"workout_minutes\").cast(DoubleType()),\n",
    "        \"lifestyle\",\n",
    "        col(\"steps\").cast(IntegerType())\n",
    "      )\n",
    "display(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbd63c",
   "metadata": {},
   "source": [
    "### Save pyspark Dataframe As Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df_pyspark.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/ht_daily_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS delta_ht_daily_metrics\n",
    "USING DELTA LOCATION '/FileStore/tables/ht_daily_metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab63b0e",
   "metadata": {},
   "source": [
    "### Read pyspark Delta Table in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM delta_ht_daily_metrics;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3baf3",
   "metadata": {},
   "source": [
    "### My standard preprocessing Framework for small SQL Dataframes\n",
    "\n",
    "I find that strategies for making an SQL query are rarely discussed. It can sometimes be troublesome to manipulate scaler information with DataFrame columns, even with the \"WITH\" command and making Temporary Tables. Below, I developed a standardized organization for adding scalar information, like the max, min, etc, directly into the table such that all needed information for calculating simple equations like t-statistic/z-score are possible.\n",
    "\n",
    "In step 1, I make a column of zeros that will serve as a way to store scalar values. In step 2, I save all scalar values to a column in the DataFrame. And, then in step 3, I add comparison scalar column values information to the DataFrame. \n",
    "\n",
    "Adding repeating column information to a DataFrame does seem computaionally wasteful, but for small datasets this seems like the most convient method of manipulation both column and scalar information. There are vector manipulation functions in SQL like TRANSFORM and FILTER, so maybe an alternative to this framework could be to save vectors and scalars with the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de995456",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a DataFrame of columns with both scalar and column information for Analysis\n",
    "-- How to preprocess in SQL :[0] always add certain manipulation columns (row_num, zero_column to add scalar values to), [1] add certain manipulation scalars (min/max/frequency count of some variables).\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW ht_daily_metrics2 AS\n",
    "WITH temptab AS\n",
    "(\n",
    "-- Step 0: Store column transformations HERE\n",
    "SELECT *, \n",
    "CAST(workout_minutes AS INT) AS wm_int,\n",
    "IF(WEEKDAY(dte) > 4, 2, 5) AS normval,\n",
    "IF(WEEKDAY(dte) > 4, \"weekend\", \"weekday\") AS wday,\n",
    "CASE WHEN lifestyle = 'Cardio Enthusiast' THEN 1 WHEN lifestyle = 'Athlete' THEN 2 WHEN lifestyle = 'Weight Trainer' THEN 3 WHEN lifestyle = 'Sedentary' THEN 4 END AS lifestyle_num,\n",
    "ROW_NUMBER() OVER(ORDER BY device_id) AS num_row FROM delta_ht_daily_metrics\n",
    ")\n",
    "-- Step 1: Create zero_col to do scalar-column transformations\n",
    "SELECT *, (num_row*0) AS zero_col FROM temptab;\n",
    "\n",
    "\n",
    "-- Step 2: Store scalar and existing column transformations HERE\n",
    "CREATE OR REPLACE TEMPORARY VIEW ht_daily_metrics3 AS\n",
    "WITH temptab AS\n",
    "(\n",
    "-- Store scalars into columns HERE\n",
    "SELECT *,\n",
    "zero_col+(SELECT AVG(resting_heartrate) FROM ht_daily_metrics2) AS samp1_mean,\n",
    "zero_col+(SELECT AVG(active_heartrate) FROM ht_daily_metrics2) AS samp2_mean,\n",
    "zero_col+(SELECT STD(resting_heartrate) FROM ht_daily_metrics2) AS samp1_std,\n",
    "zero_col+(SELECT STD(active_heartrate) FROM ht_daily_metrics2) AS samp2_std,\n",
    "zero_col+(SELECT COUNT(*) FROM ht_daily_metrics2) AS samp1_2_len\n",
    "FROM ht_daily_metrics2\n",
    ")\n",
    "-- Step 3: Store comparisons of scalars columns HERE\n",
    "SELECT *,\n",
    "zero_col+(SELECT IF(MIN(samp1_mean) > MIN(samp2_mean), MIN(samp2_mean), MIN(samp1_mean)) FROM temptab) AS mean_comp_min\n",
    "FROM temptab;\n",
    "\n",
    "\n",
    "SELECT * FROM ht_daily_metrics3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6cf32",
   "metadata": {},
   "source": [
    "### Create SQL functions for t-statistic/z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION t_OR_Z_statistic_twosample( samp1 FLOAT, samp2 FLOAT, samp1_mean FLOAT, samp2_mean FLOAT, samp1_std FLOAT, samp2_std FLOAT, samp1_len FLOAT, samp2_len FLOAT)\n",
    "RETURNS FLOAT\n",
    "RETURN ((samp1 - samp2) - (samp1_mean - samp2_mean))/SQRT( ((samp1_std*samp1_std)/samp1_len) + ((samp2_std*samp2_std)/samp2_len));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Calculate the pdf (the normal distribution OR the probability density function)\n",
    "CREATE OR REPLACE FUNCTION normal_pdf( t_OR_Z FLOAT, t_OR_Z_mean FLOAT, t_OR_Z_std FLOAT)\n",
    "RETURNS FLOAT\n",
    "RETURN (1/(SQRT(2*PI())*t_OR_Z_std))*EXP(-((t_OR_Z - t_OR_Z_mean)*(t_OR_Z - t_OR_Z_mean))/(2*t_OR_Z_std*t_OR_Z_std));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW ht_daily_metrics5 AS\n",
    "WITH temp_tab AS\n",
    "(\n",
    "SELECT *,\n",
    "zero_col+(SELECT AVG(t_OR_Z_statistic_2samp) FROM ht_daily_metrics4) AS t_OR_Z_statistic_2samp_mean,\n",
    "zero_col+(SELECT STD(t_OR_Z_statistic_2samp) FROM ht_daily_metrics4) AS t_OR_Z_statistic_2samp_std\n",
    "FROM ht_daily_metrics4\n",
    ")\n",
    "SELECT *,\n",
    "normal_pdf( t_OR_Z_statistic_2samp, t_OR_Z_statistic_2samp_mean, t_OR_Z_statistic_2samp_std) AS t_OR_Z_statistic_2samp_pdf\n",
    "FROM temp_tab;\n",
    "\n",
    "SELECT * FROM ht_daily_metrics5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Run the function in a query just below\n",
    "SELECT t_OR_Z_critical_twosample(samp1_mean, samp2_mean, samp1_std, samp2_std, samp1_2_len, samp1_2_len) AS t_OR_Z_statistic_2samp FROM ht_daily_metrics5\n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4dd167",
   "metadata": {},
   "source": [
    "### Convert SQL Table to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df_pandas = spark.sql(\"SELECT * FROM ht_daily_metrics3\").toPandas()\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from scipy.stats import ttest_ind\n",
    "resting_hr = spark.sql(\"SELECT resting_heartrate FROM ht_daily_metrics3\").toPandas()\n",
    "active_hr = spark.sql(\"SELECT active_heartrate FROM ht_daily_metrics3\").toPandas()\n",
    "\n",
    "ttest_ind(resting_hr, active_hr, equal_var = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef5c7a",
   "metadata": {},
   "source": [
    "### Convert pandas DataFrame to numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df_numpy = df_pandas.to_numpy()\n",
    "df_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d206a",
   "metadata": {},
   "source": [
    "### Load SQL Table in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc61e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "library(SparkR)\n",
    "library(sparklyr)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "# After you load sparklyr, you must call sparklyr::spark_connect to connect to the cluster, specifying the databricks connection method.\n",
    "sc <- spark_connect(method = \"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "R_ht_daily_metrics3 <- collect(sdf_sql(sc, \"SELECT * FROM ht_daily_metrics3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "print(R_ht_daily_metrics3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "# Selecting columns in a DataFrame requires library(dplyr)\n",
    "a_hr <- R_ht_daily_metrics3 %>% select(active_heartrate)\n",
    "r_hr <- R_ht_daily_metrics3 %>% select(resting_heartrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "install.packages(\"ggpubr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%r\n",
    "t.test(a_hr, r_hr, alternative = \"two.sided\", var.equal = FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
